{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Project 1\n",
    "Wojciech Kutak\n",
    "\n",
    "---\n",
    "\n",
    "### Cross Attention Network for few-shot learning problem\n",
    "#### 1. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "DATA_PATH = os.path.join(os.getcwd(), \"..\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsampler = None):\n",
    "        super().__init__()\n",
    "        self.downsampler = downsampler\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsampler is not None:\n",
    "            identity = self.downsampler(identity)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet_32x32(nn.Module):\n",
    "    def __init__(self, block: nn.Module = ResidualBlock):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 64, (3, 3), stride=1, padding=1)\n",
    "        self.bnorm = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.res_layer1 = self._residual_layer(block, 64, 128, 2)\n",
    "\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.res_layer2 = self._residual_layer(block, 128, 256, 2)\n",
    "\n",
    "        self.max_pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.res_layer3 = self._residual_layer(block, 256, 512, 2)\n",
    "\n",
    "        self.output_shape = (512, 3, 3)\n",
    "\n",
    "\n",
    "    def _residual_layer(self, block, in_channels, out_channels, blocks_num, stride=1):\n",
    "        \"\"\"Creates a residual layer consisting out of residual blocks\"\"\"\n",
    "        downsampler = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsampler = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(in_channels, out_channels, stride, downsampler))\n",
    "        for _ in range(blocks_num - 1):\n",
    "            layers.append(block(out_channels, out_channels, stride))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bnorm(self.conv(x)))\n",
    "\n",
    "        x = self.res_layer1(self.max_pool1(x))\n",
    "        x = self.res_layer2(self.max_pool2(x))\n",
    "        x = self.res_layer3(self.max_pool3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet_32x32                             [30, 512, 3, 3]           --\n",
       "├─Conv2d: 1-1                            [30, 64, 32, 32]          1,792\n",
       "├─BatchNorm2d: 1-2                       [30, 64, 32, 32]          128\n",
       "├─ReLU: 1-3                              [30, 64, 32, 32]          --\n",
       "├─MaxPool2d: 1-4                         [30, 64, 15, 15]          --\n",
       "├─Sequential: 1-5                        [30, 128, 15, 15]         --\n",
       "│    └─ResidualBlock: 2-1                [30, 128, 15, 15]         --\n",
       "│    │    └─Sequential: 3-1              [30, 128, 15, 15]         8,448\n",
       "│    │    └─Conv2d: 3-2                  [30, 128, 15, 15]         73,856\n",
       "│    │    └─BatchNorm2d: 3-3             [30, 128, 15, 15]         256\n",
       "│    │    └─ReLU: 3-4                    [30, 128, 15, 15]         --\n",
       "│    │    └─Conv2d: 3-5                  [30, 128, 15, 15]         147,584\n",
       "│    │    └─BatchNorm2d: 3-6             [30, 128, 15, 15]         256\n",
       "│    │    └─ReLU: 3-7                    [30, 128, 15, 15]         --\n",
       "│    └─ResidualBlock: 2-2                [30, 128, 15, 15]         --\n",
       "│    │    └─Conv2d: 3-8                  [30, 128, 15, 15]         147,584\n",
       "│    │    └─BatchNorm2d: 3-9             [30, 128, 15, 15]         256\n",
       "│    │    └─ReLU: 3-10                   [30, 128, 15, 15]         --\n",
       "│    │    └─Conv2d: 3-11                 [30, 128, 15, 15]         147,584\n",
       "│    │    └─BatchNorm2d: 3-12            [30, 128, 15, 15]         256\n",
       "│    │    └─ReLU: 3-13                   [30, 128, 15, 15]         --\n",
       "├─MaxPool2d: 1-6                         [30, 128, 7, 7]           --\n",
       "├─Sequential: 1-7                        [30, 256, 7, 7]           --\n",
       "│    └─ResidualBlock: 2-3                [30, 256, 7, 7]           --\n",
       "│    │    └─Sequential: 3-14             [30, 256, 7, 7]           33,280\n",
       "│    │    └─Conv2d: 3-15                 [30, 256, 7, 7]           295,168\n",
       "│    │    └─BatchNorm2d: 3-16            [30, 256, 7, 7]           512\n",
       "│    │    └─ReLU: 3-17                   [30, 256, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-18                 [30, 256, 7, 7]           590,080\n",
       "│    │    └─BatchNorm2d: 3-19            [30, 256, 7, 7]           512\n",
       "│    │    └─ReLU: 3-20                   [30, 256, 7, 7]           --\n",
       "│    └─ResidualBlock: 2-4                [30, 256, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-21                 [30, 256, 7, 7]           590,080\n",
       "│    │    └─BatchNorm2d: 3-22            [30, 256, 7, 7]           512\n",
       "│    │    └─ReLU: 3-23                   [30, 256, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-24                 [30, 256, 7, 7]           590,080\n",
       "│    │    └─BatchNorm2d: 3-25            [30, 256, 7, 7]           512\n",
       "│    │    └─ReLU: 3-26                   [30, 256, 7, 7]           --\n",
       "├─MaxPool2d: 1-8                         [30, 256, 3, 3]           --\n",
       "├─Sequential: 1-9                        [30, 512, 3, 3]           --\n",
       "│    └─ResidualBlock: 2-5                [30, 512, 3, 3]           --\n",
       "│    │    └─Sequential: 3-27             [30, 512, 3, 3]           132,096\n",
       "│    │    └─Conv2d: 3-28                 [30, 512, 3, 3]           1,180,160\n",
       "│    │    └─BatchNorm2d: 3-29            [30, 512, 3, 3]           1,024\n",
       "│    │    └─ReLU: 3-30                   [30, 512, 3, 3]           --\n",
       "│    │    └─Conv2d: 3-31                 [30, 512, 3, 3]           2,359,808\n",
       "│    │    └─BatchNorm2d: 3-32            [30, 512, 3, 3]           1,024\n",
       "│    │    └─ReLU: 3-33                   [30, 512, 3, 3]           --\n",
       "│    └─ResidualBlock: 2-6                [30, 512, 3, 3]           --\n",
       "│    │    └─Conv2d: 3-34                 [30, 512, 3, 3]           2,359,808\n",
       "│    │    └─BatchNorm2d: 3-35            [30, 512, 3, 3]           1,024\n",
       "│    │    └─ReLU: 3-36                   [30, 512, 3, 3]           --\n",
       "│    │    └─Conv2d: 3-37                 [30, 512, 3, 3]           2,359,808\n",
       "│    │    └─BatchNorm2d: 3-38            [30, 512, 3, 3]           1,024\n",
       "│    │    └─ReLU: 3-39                   [30, 512, 3, 3]           --\n",
       "==========================================================================================\n",
       "Total params: 11,024,512\n",
       "Trainable params: 11,024,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 8.95\n",
       "==========================================================================================\n",
       "Input size (MB): 0.37\n",
       "Forward/backward pass size (MB): 141.74\n",
       "Params size (MB): 44.10\n",
       "Estimated Total Size (MB): 186.21\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(ResNet_32x32(), (30, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionLayer(nn.Module):\n",
    "    def __init__(self, m: int, bottleneck_size: int):\n",
    "        super(FusionLayer, self).__init__()\n",
    "\n",
    "        self.temperature = 1.0\n",
    "        self.m = m\n",
    "        self.bottleneck_size = bottleneck_size\n",
    "        self.spatial_gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv1 = nn.Conv2d(self.m, self.bottleneck_size, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(self.bottleneck_size, self.m, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, R: torch.Tensor):\n",
    "        # print(\"FusionLayer forward\")\n",
    "        # print(\"R shape:\", R.shape)\n",
    "        b, M, m, h, w = R.shape\n",
    "        w: torch.Tensor = self.spatial_gap(R)\n",
    "        # print(\"spatial w:\", w.shape)\n",
    "        w = w.view(b * M, *w.shape[2:])\n",
    "        # print(\"spatial w after:\", w.shape)\n",
    "\n",
    "        # w = w.unsqueeze(-2)\n",
    "        # Meta learner\n",
    "        w = self.conv1(w)\n",
    "        # print(\"conv1 w:\", w.shape)\n",
    "        w = self.relu(w)\n",
    "        # print(\"relu w:\",w.shape)\n",
    "        w = self.conv2(w)\n",
    "        # print(\"conv2 w:\",w.shape)\n",
    "\n",
    "        # w = w.squeeze((-2, -1))\n",
    "        # print(w.shape)\n",
    "\n",
    "        # Convolution operation\n",
    "        # w is now a vector of average class\n",
    "        A = self.attention(w, R)\n",
    "        # print(\"Attention:\", A.shape)\n",
    "\n",
    "\n",
    "        return A\n",
    "\n",
    "\n",
    "    def attention(self, weights: torch.Tensor, R: torch.Tensor) -> torch.Tensor:\n",
    "        weights_t = weights.transpose(-3, -1).squeeze(1)\n",
    "\n",
    "        b, M, m, h, w = R.shape\n",
    "        R = R.view(b * M, m, h*w)\n",
    "        # print(\"weights:\", weights.shape)\n",
    "        # print(\"weights_t:\", weights_t.shape)\n",
    "        # print(\"R:\", R.shape)\n",
    "\n",
    "        R_mean = torch.bmm(weights_t, R) / self.temperature\n",
    "        # print(\"R_mean:\", R_mean.shape)\n",
    "        R_mean = R_mean.view(b, M, 1, m)\n",
    "        A = F.softmax(R_mean, dim=-1)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FusionLayer forward\n",
      "R shape: torch.Size([3, 5, 25, 5, 5])\n",
      "spatial w: torch.Size([3, 5, 25, 1, 1])\n",
      "spatial w after: torch.Size([15, 25, 1, 1])\n",
      "conv1 w: torch.Size([15, 15, 1, 1])\n",
      "relu w: torch.Size([15, 15, 1, 1])\n",
      "conv2 w: torch.Size([15, 25, 1, 1])\n",
      "weights: torch.Size([15, 25, 1, 1])\n",
      "weights_t: torch.Size([15, 1, 25])\n",
      "R: torch.Size([15, 25, 25])\n",
      "R_mean: torch.Size([15, 1, 25])\n",
      "Attention: torch.Size([3, 5, 1, 25])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FusionLayer                              [3, 5, 1, 25]             --\n",
       "├─AdaptiveAvgPool2d: 1-1                 [3, 5, 25, 1, 1]          --\n",
       "├─Conv2d: 1-2                            [15, 15, 1, 1]            390\n",
       "├─ReLU: 1-3                              [15, 15, 1, 1]            --\n",
       "├─Conv2d: 1-4                            [15, 25, 1, 1]            400\n",
       "==========================================================================================\n",
       "Total params: 790\n",
       "Trainable params: 790\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.04\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width, height = 5, 5\n",
    "m = width*height\n",
    "M = 5\n",
    "batch_size = 3\n",
    "summary(FusionLayer(m=m, bottleneck_size=15), (batch_size, M, m, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  2.,   4.],\n",
       "          [  3.,   6.],\n",
       "          [  4.,   8.]],\n",
       "\n",
       "         [[ 10.,  20.],\n",
       "          [ 11.,  22.],\n",
       "          [112., 224.]]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 2.0]).view(1, 2)\n",
    "b = torch.tensor([[2.0, 3.0, 4.0], [10.0, 11.0, 112.0]]).view(1, 2, 3, 1)\n",
    "a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionModule(nn.Module):\n",
    "    def __init__(self, input_shape: tuple[int,int,int]):\n",
    "        super(CrossAttentionModule, self).__init__()\n",
    "\n",
    "        # print(\"input shape\", *input_shape)\n",
    "        c, h, w = input_shape\n",
    "        self.fusion_layer = FusionLayer(m=h*w, bottleneck_size=int(h*w*2/3))\n",
    "\n",
    "\n",
    "    def forward(self, P: torch.Tensor, Q: torch.Tensor):\n",
    "    # def forward(self, S: torch.Tensor):\n",
    "        # P, Q = S[0], S[1, :, 0, :, :, :]\n",
    "        # print(\"CrossAttentionModule forward\")\n",
    "        # print(P.shape, Q.shape)\n",
    "        # assert P.shape == Q.shape\n",
    "        b, M, c, h, w = P.shape\n",
    "        assert (b, c, h, w) == Q.shape\n",
    "        m = h*w\n",
    "        # Change representation from tensor c*h*w to c*m (2 dims)\n",
    "        P = P.view(b, M, c, m)\n",
    "        Q = Q.view(b, 1, c, m)\n",
    "\n",
    "        P_norm = F.normalize(P, p=2, dim=1)\n",
    "        Q_norm = F.normalize(Q, p=2, dim=1)\n",
    "        # print(\"P norm:\", P_norm.shape)\n",
    "        P_norm_t = P_norm.transpose(-2, -1)\n",
    "        # print(\"P norm t:\", P_norm_t.shape)\n",
    "        # print(\"Q norm:\", Q_norm.shape)\n",
    "        R_q = torch.matmul(P_norm_t, Q_norm)\n",
    "        R_p = R_q.transpose(-2, -1).view(b, M, m, h, w)\n",
    "        R_q = R_q.view(b, M, m, h, w)\n",
    "        # print(\"R_q\", R_q.shape)\n",
    "        # print(\"R_p\", R_p.shape)\n",
    "\n",
    "\n",
    "\n",
    "        A_p: torch.Tensor = self.fusion_layer(R_p)\n",
    "        # print()\n",
    "        A_q: torch.Tensor = self.fusion_layer(R_q)\n",
    "        # print()\n",
    "\n",
    "        # print(\"P:\", P.shape, \"A_p:\", A_p.shape)\n",
    "        P_feat = torch.mul(P, A_p) + P\n",
    "        # print(\"P_feat:\", P_feat.shape)\n",
    "\n",
    "        # print(\"Q:\", Q.shape, \"A_q:\", A_q.shape)\n",
    "        Q = Q.expand(b, M, c, m)\n",
    "        # print(\"Q exp:\", Q.shape, \"A_q:\", A_q.shape)\n",
    "        Q_feat = torch.mul(Q, A_q) + Q\n",
    "        # print(\"Q_feat:\", Q_feat.shape)\n",
    "        return P_feat, Q_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1],\n",
      "         [2, 3]],\n",
      "\n",
      "        [[4, 5],\n",
      "         [6, 7]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0, 1],\n",
       "          [2, 3]],\n",
       " \n",
       "         [[4, 5],\n",
       "          [6, 7]]]),\n",
       " tensor([[[0, 1],\n",
       "          [2, 3]],\n",
       " \n",
       "         [[4, 5],\n",
       "          [6, 7]]]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(8).reshape(2, 2, 2)\n",
    "print(a)\n",
    "a2 = a.expand(2, 2, 2, 2).transpose(0, 1)\n",
    "a2[:, 0, :, :], a2[:, 1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossAttentionModule forward\n",
      "torch.Size([3, 5, 512, 5, 5]) torch.Size([3, 512, 5, 5])\n",
      "P norm: torch.Size([3, 5, 512, 25])\n",
      "P norm t: torch.Size([3, 5, 25, 512])\n",
      "Q norm: torch.Size([3, 1, 512, 25])\n",
      "R_q torch.Size([3, 5, 25, 5, 5])\n",
      "R_p torch.Size([3, 5, 25, 5, 5])\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([3, 5, 25, 5, 5])\n",
      "spatial w: torch.Size([3, 5, 25, 1, 1])\n",
      "spatial w after: torch.Size([15, 25, 1, 1])\n",
      "conv1 w: torch.Size([15, 15, 1, 1])\n",
      "relu w: torch.Size([15, 15, 1, 1])\n",
      "conv2 w: torch.Size([15, 25, 1, 1])\n",
      "weights: torch.Size([15, 25, 1, 1])\n",
      "weights_t: torch.Size([15, 1, 25])\n",
      "R: torch.Size([15, 25, 25])\n",
      "R_mean: torch.Size([15, 1, 25])\n",
      "Attention: torch.Size([3, 5, 1, 25])\n",
      "\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([3, 5, 25, 5, 5])\n",
      "spatial w: torch.Size([3, 5, 25, 1, 1])\n",
      "spatial w after: torch.Size([15, 25, 1, 1])\n",
      "conv1 w: torch.Size([15, 15, 1, 1])\n",
      "relu w: torch.Size([15, 15, 1, 1])\n",
      "conv2 w: torch.Size([15, 25, 1, 1])\n",
      "weights: torch.Size([15, 25, 1, 1])\n",
      "weights_t: torch.Size([15, 1, 25])\n",
      "R: torch.Size([15, 25, 25])\n",
      "R_mean: torch.Size([15, 1, 25])\n",
      "Attention: torch.Size([3, 5, 1, 25])\n",
      "\n",
      "P: torch.Size([3, 5, 512, 25]) A_p: torch.Size([3, 5, 1, 25])\n",
      "P_feat: torch.Size([3, 5, 512, 25])\n",
      "Q: torch.Size([3, 1, 512, 25]) A_q: torch.Size([3, 5, 1, 25])\n",
      "Q exp: torch.Size([3, 5, 512, 25]) A_q: torch.Size([3, 5, 1, 25])\n",
      "Q_feat: torch.Size([3, 5, 512, 25])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CrossAttentionModule                     [3, 5, 512, 25]           --\n",
       "├─FusionLayer: 1-1                       [3, 5, 1, 25]             --\n",
       "│    └─AdaptiveAvgPool2d: 2-1            [3, 5, 25, 1, 1]          --\n",
       "│    └─Conv2d: 2-2                       [15, 15, 1, 1]            390\n",
       "│    └─ReLU: 2-3                         [15, 15, 1, 1]            --\n",
       "│    └─Conv2d: 2-4                       [15, 25, 1, 1]            400\n",
       "├─FusionLayer: 1-2                       [3, 5, 1, 25]             (recursive)\n",
       "│    └─AdaptiveAvgPool2d: 2-5            [3, 5, 25, 1, 1]          --\n",
       "│    └─Conv2d: 2-6                       [15, 15, 1, 1]            (recursive)\n",
       "│    └─ReLU: 2-7                         [15, 15, 1, 1]            --\n",
       "│    └─Conv2d: 2-8                       [15, 25, 1, 1]            (recursive)\n",
       "==========================================================================================\n",
       "Total params: 790\n",
       "Trainable params: 790\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.02\n",
       "==========================================================================================\n",
       "Input size (MB): 1.54\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 1.55\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 3\n",
    "m = 5\n",
    "\n",
    "summary(CrossAttentionModule((512, 5, 5)), (2, b, m, 512, 5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  4.],\n",
       "         [ 9., 16.]]),\n",
       " tensor([[ 7., 10.],\n",
       "         [15., 22.]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "B = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "torch.mul(A, B), torch.matmul(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7., 22.],\n",
       "        [21., 44.]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1.0, 2.0], [3., 4.]])\n",
    "B = torch.tensor([[7., 11.]])\n",
    "A*B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionNetwork(nn.Module):\n",
    "    def __init__(self, cam: CrossAttentionModule = None):\n",
    "        super(CrossAttentionNetwork, self).__init__()\n",
    "\n",
    "        self.embedding = ResNet_32x32()\n",
    "        self.cam = cam if cam is not None else CrossAttentionModule(self.embedding.output_shape)\n",
    "\n",
    "\n",
    "    def forward(self, support: torch.Tensor, query: torch.Tensor):\n",
    "    # def forward(self, X: torch.Tensor):\n",
    "\n",
    "        # support = X[0]\n",
    "        # query = X[1, :, 0, 0, :, :]\n",
    "        b, M, K, c, h, w = support.shape\n",
    "        # print(\"Support: \", support.shape)\n",
    "        # print(\"query: \", query.shape)\n",
    "\n",
    "        # Shapes of support and query tensors should be\n",
    "        # - support.shape = (b, M, K, c, h, w),\n",
    "        # - query.shape = (b, c, h, w),\n",
    "        # where b - batch size, M - number of classes, K - number of shots (examples per class),\n",
    "        # c - channels, h - height and w - width.\n",
    "\n",
    "\n",
    "        # Class feature P is defined as an average of embedded samples from one class\n",
    "        P = self.embedding(support.view(-1, c, h, w))\n",
    "        embedding_shape = P.shape[1:]\n",
    "        P = P.view(b, M, K, *embedding_shape)\n",
    "        P = torch.mean(P, dim=2)\n",
    "\n",
    "        Q = self.embedding(query)\n",
    "        P_feature, Q_feature = self.cam(P, Q)\n",
    "\n",
    "        # print(\"P_feature:\", P_feature.shape)\n",
    "        # print(\"Q_feature:\", Q_feature.shape)\n",
    "\n",
    "        return P_feature, Q_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape 512 3 3\n",
      "Support:  torch.Size([32, 5, 5, 3, 32, 32])\n",
      "query:  torch.Size([32, 3, 32, 32])\n",
      "CrossAttentionModule forward\n",
      "torch.Size([32, 5, 512, 3, 3]) torch.Size([32, 512, 3, 3])\n",
      "P norm: torch.Size([32, 5, 512, 9])\n",
      "P norm t: torch.Size([32, 5, 9, 512])\n",
      "Q norm: torch.Size([32, 1, 512, 9])\n",
      "R_q torch.Size([32, 5, 9, 3, 3])\n",
      "R_p torch.Size([32, 5, 9, 3, 3])\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([32, 5, 9, 3, 3])\n",
      "spatial w: torch.Size([32, 5, 9, 1, 1])\n",
      "spatial w after: torch.Size([160, 9, 1, 1])\n",
      "conv1 w: torch.Size([160, 6, 1, 1])\n",
      "relu w: torch.Size([160, 6, 1, 1])\n",
      "conv2 w: torch.Size([160, 9, 1, 1])\n",
      "weights: torch.Size([160, 9, 1, 1])\n",
      "weights_t: torch.Size([160, 1, 9])\n",
      "R: torch.Size([160, 9, 9])\n",
      "R_mean: torch.Size([160, 1, 9])\n",
      "Attention: torch.Size([32, 5, 1, 9])\n",
      "\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([32, 5, 9, 3, 3])\n",
      "spatial w: torch.Size([32, 5, 9, 1, 1])\n",
      "spatial w after: torch.Size([160, 9, 1, 1])\n",
      "conv1 w: torch.Size([160, 6, 1, 1])\n",
      "relu w: torch.Size([160, 6, 1, 1])\n",
      "conv2 w: torch.Size([160, 9, 1, 1])\n",
      "weights: torch.Size([160, 9, 1, 1])\n",
      "weights_t: torch.Size([160, 1, 9])\n",
      "R: torch.Size([160, 9, 9])\n",
      "R_mean: torch.Size([160, 1, 9])\n",
      "Attention: torch.Size([32, 5, 1, 9])\n",
      "\n",
      "P: torch.Size([32, 5, 512, 9]) A_p: torch.Size([32, 5, 1, 9])\n",
      "P_feat: torch.Size([32, 5, 512, 9])\n",
      "Q: torch.Size([32, 1, 512, 9]) A_q: torch.Size([32, 5, 1, 9])\n",
      "Q exp: torch.Size([32, 5, 512, 9]) A_q: torch.Size([32, 5, 1, 9])\n",
      "Q_feat: torch.Size([32, 5, 512, 9])\n",
      "P_feature: torch.Size([32, 5, 512, 9])\n",
      "Q_feature: torch.Size([32, 5, 512, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "CrossAttentionNetwork                         [32, 5, 512, 9]           --\n",
       "├─ResNet_32x32: 1-1                           [800, 512, 3, 3]          --\n",
       "│    └─Conv2d: 2-1                            [800, 64, 32, 32]         1,792\n",
       "│    └─BatchNorm2d: 2-2                       [800, 64, 32, 32]         128\n",
       "│    └─ReLU: 2-3                              [800, 64, 32, 32]         --\n",
       "│    └─MaxPool2d: 2-4                         [800, 64, 15, 15]         --\n",
       "│    └─Sequential: 2-5                        [800, 128, 15, 15]        --\n",
       "│    │    └─ResidualBlock: 3-1                [800, 128, 15, 15]        230,400\n",
       "│    │    └─ResidualBlock: 3-2                [800, 128, 15, 15]        295,680\n",
       "│    └─MaxPool2d: 2-6                         [800, 128, 7, 7]          --\n",
       "│    └─Sequential: 2-7                        [800, 256, 7, 7]          --\n",
       "│    │    └─ResidualBlock: 3-3                [800, 256, 7, 7]          919,552\n",
       "│    │    └─ResidualBlock: 3-4                [800, 256, 7, 7]          1,181,184\n",
       "│    └─MaxPool2d: 2-8                         [800, 256, 3, 3]          --\n",
       "│    └─Sequential: 2-9                        [800, 512, 3, 3]          --\n",
       "│    │    └─ResidualBlock: 3-5                [800, 512, 3, 3]          3,674,112\n",
       "│    │    └─ResidualBlock: 3-6                [800, 512, 3, 3]          4,721,664\n",
       "├─ResNet_32x32: 1-2                           [32, 512, 3, 3]           (recursive)\n",
       "│    └─Conv2d: 2-10                           [32, 64, 32, 32]          (recursive)\n",
       "│    └─BatchNorm2d: 2-11                      [32, 64, 32, 32]          (recursive)\n",
       "│    └─ReLU: 2-12                             [32, 64, 32, 32]          --\n",
       "│    └─MaxPool2d: 2-13                        [32, 64, 15, 15]          --\n",
       "│    └─Sequential: 2-14                       [32, 128, 15, 15]         (recursive)\n",
       "│    │    └─ResidualBlock: 3-7                [32, 128, 15, 15]         (recursive)\n",
       "│    │    └─ResidualBlock: 3-8                [32, 128, 15, 15]         (recursive)\n",
       "│    └─MaxPool2d: 2-15                        [32, 128, 7, 7]           --\n",
       "│    └─Sequential: 2-16                       [32, 256, 7, 7]           (recursive)\n",
       "│    │    └─ResidualBlock: 3-9                [32, 256, 7, 7]           (recursive)\n",
       "│    │    └─ResidualBlock: 3-10               [32, 256, 7, 7]           (recursive)\n",
       "│    └─MaxPool2d: 2-17                        [32, 256, 3, 3]           --\n",
       "│    └─Sequential: 2-18                       [32, 512, 3, 3]           (recursive)\n",
       "│    │    └─ResidualBlock: 3-11               [32, 512, 3, 3]           (recursive)\n",
       "│    │    └─ResidualBlock: 3-12               [32, 512, 3, 3]           (recursive)\n",
       "├─CrossAttentionModule: 1-3                   [32, 5, 512, 9]           --\n",
       "│    └─FusionLayer: 2-19                      [32, 5, 1, 9]             --\n",
       "│    │    └─AdaptiveAvgPool2d: 3-13           [32, 5, 9, 1, 1]          --\n",
       "│    │    └─Conv2d: 3-14                      [160, 6, 1, 1]            60\n",
       "│    │    └─ReLU: 3-15                        [160, 6, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-16                      [160, 9, 1, 1]            63\n",
       "│    └─FusionLayer: 2-20                      [32, 5, 1, 9]             (recursive)\n",
       "│    │    └─AdaptiveAvgPool2d: 3-17           [32, 5, 9, 1, 1]          --\n",
       "│    │    └─Conv2d: 3-18                      [160, 6, 1, 1]            (recursive)\n",
       "│    │    └─ReLU: 3-19                        [160, 6, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-20                      [160, 9, 1, 1]            (recursive)\n",
       "===============================================================================================\n",
       "Total params: 11,024,635\n",
       "Trainable params: 11,024,635\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 248.14\n",
       "===============================================================================================\n",
       "Input size (MB): 19.66\n",
       "Forward/backward pass size (MB): 3931.02\n",
       "Params size (MB): 44.10\n",
       "Estimated Total Size (MB): 3994.78\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(CrossAttentionNetwork(), (2, 32, 5, 5, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CANLoss(nn.Module):\n",
    "    def __init__(self, can: CrossAttentionNetwork, lamb: float = 0.5, n_classes: int = 5):\n",
    "        super(CANLoss, self).__init__()\n",
    "\n",
    "        self.can = can\n",
    "        self.lamb = lamb\n",
    "        self.classifier = nn.LazyConv1d(out_channels=n_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def cosine_dist(self, P_gap: torch.Tensor, Q_features: torch.Tensor) -> torch.Tensor:\n",
    "        # print(\"\\ncosine_dist\")\n",
    "        # print(\"Q_features\",  Q_features.shape)\n",
    "        # Q_features: (b, c, m)\n",
    "        b, c, m = Q_features.shape\n",
    "        # print(\"P_gap\",  P_gap.shape)\n",
    "        # P_gap: (b, c)\n",
    "        P_gap_exp = P_gap.expand(m, b, c).transpose(0, 1).transpose(1, 2)\n",
    "        # print(\"P_gap_exp\", P_gap_exp.shape)\n",
    "        cos_dist = F.cosine_similarity(P_gap_exp, Q_features, dim=-2)\n",
    "        # print(\"cos_dist\", cos_dist.shape)\n",
    "        return cos_dist\n",
    "\n",
    "\n",
    "    def L1_loss(self, P_sig: torch.Tensor, Q_sig: torch.Tensor, y_true: torch.Tensor):\n",
    "        # P_features: (b, c, m)\n",
    "        # Q_features: (b, c, m)\n",
    "        # print(\"l1 loss\")\n",
    "        # print(\"P_features:\", P_sig.shape)\n",
    "        # print(\"Q_features:\", Q_sig.shape)\n",
    "\n",
    "\n",
    "        P_gap = torch.mean(P_sig, dim=-1)\n",
    "        # P_gap: (b, c)\n",
    "        distances = self.cosine_dist(P_gap, Q_sig)\n",
    "        # distances: (b, m)\n",
    "        # print(\"Distances:\", distances.shape)\n",
    "\n",
    "        likelihoods = torch.log(F.softmax(-distances, dim=-1))\n",
    "        # print(\"likelihoods\", likelihoods.shape)\n",
    "        l1 = torch.sum(likelihoods)\n",
    "        # print(\"l1\", l1)\n",
    "        return l1\n",
    "\n",
    "\n",
    "    def L2_loss(self, Q_sig: torch.Tensor, y_true: torch.Tensor):\n",
    "        # print(\"l2 loss\")\n",
    "        # print(\"Q_sig:\", Q_sig.shape)\n",
    "        # print()\n",
    "\n",
    "        Z: torch.Tensor = self.classifier(Q_sig)\n",
    "        # print(\"Z\", Z.shape)\n",
    "        Z_significant = []\n",
    "        for y, z in zip(y_true, Z):\n",
    "            Z_significant.append(z[y])\n",
    "        Z_significant = torch.concat(Z_significant)\n",
    "        # print(\"Z_significant:\", Z_significant.shape)\n",
    "\n",
    "        y_pred = F.softmax(Z_significant, dim=-1)\n",
    "        # print(\"y_pred\", y_pred.shape)\n",
    "        l2 = torch.sum(y_pred)\n",
    "        # print(\"l2:\", l2.shape)\n",
    "        return l2\n",
    "\n",
    "\n",
    "    def forward(self, support: torch.Tensor, query: torch.Tensor, y_true: torch.Tensor):\n",
    "        P_features, Q_features = self.can(support, query)\n",
    "        Q_significant = []\n",
    "        P_significant = []\n",
    "        for y, q, p in zip(y_true, Q_features, P_features):\n",
    "            Q_significant.append(q[y])\n",
    "            P_significant.append(p[y])\n",
    "        Q_significant = torch.concatenate(Q_significant, dim=0)\n",
    "        P_significant = torch.concatenate(P_significant, dim=0)\n",
    "        l1 = self.L1_loss(P_significant, Q_significant, y_true)\n",
    "        l2 = self.L2_loss(Q_significant, y_true)\n",
    "        loss = self.lamb*l1 + l2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape 512 3 3\n",
      "Support:  torch.Size([32, 5, 5, 3, 32, 32])\n",
      "query:  torch.Size([32, 3, 32, 32])\n",
      "CrossAttentionModule forward\n",
      "torch.Size([32, 5, 512, 3, 3]) torch.Size([32, 512, 3, 3])\n",
      "P norm: torch.Size([32, 5, 512, 9])\n",
      "P norm t: torch.Size([32, 5, 9, 512])\n",
      "Q norm: torch.Size([32, 1, 512, 9])\n",
      "R_q torch.Size([32, 5, 9, 3, 3])\n",
      "R_p torch.Size([32, 5, 9, 3, 3])\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([32, 5, 9, 3, 3])\n",
      "spatial w: torch.Size([32, 5, 9, 1, 1])\n",
      "spatial w after: torch.Size([160, 9, 1, 1])\n",
      "conv1 w: torch.Size([160, 6, 1, 1])\n",
      "relu w: torch.Size([160, 6, 1, 1])\n",
      "conv2 w: torch.Size([160, 9, 1, 1])\n",
      "weights: torch.Size([160, 9, 1, 1])\n",
      "weights_t: torch.Size([160, 1, 9])\n",
      "R: torch.Size([160, 9, 9])\n",
      "R_mean: torch.Size([160, 1, 9])\n",
      "Attention: torch.Size([32, 5, 1, 9])\n",
      "\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([32, 5, 9, 3, 3])\n",
      "spatial w: torch.Size([32, 5, 9, 1, 1])\n",
      "spatial w after: torch.Size([160, 9, 1, 1])\n",
      "conv1 w: torch.Size([160, 6, 1, 1])\n",
      "relu w: torch.Size([160, 6, 1, 1])\n",
      "conv2 w: torch.Size([160, 9, 1, 1])\n",
      "weights: torch.Size([160, 9, 1, 1])\n",
      "weights_t: torch.Size([160, 1, 9])\n",
      "R: torch.Size([160, 9, 9])\n",
      "R_mean: torch.Size([160, 1, 9])\n",
      "Attention: torch.Size([32, 5, 1, 9])\n",
      "\n",
      "P: torch.Size([32, 5, 512, 9]) A_p: torch.Size([32, 5, 1, 9])\n",
      "P_feat: torch.Size([32, 5, 512, 9])\n",
      "Q: torch.Size([32, 1, 512, 9]) A_q: torch.Size([32, 5, 1, 9])\n",
      "Q exp: torch.Size([32, 5, 512, 9]) A_q: torch.Size([32, 5, 1, 9])\n",
      "Q_feat: torch.Size([32, 5, 512, 9])\n",
      "P_feature: torch.Size([32, 5, 512, 9])\n",
      "Q_feature: torch.Size([32, 5, 512, 9])\n",
      "l1 loss\n",
      "P_features: torch.Size([32, 512, 9])\n",
      "Q_features: torch.Size([32, 512, 9])\n",
      "\n",
      "cosine_dist\n",
      "Q_features torch.Size([32, 512, 9])\n",
      "P_gap torch.Size([32, 512])\n",
      "P_gap_exp torch.Size([32, 512, 9])\n",
      "cos_dist torch.Size([32, 9])\n",
      "Distances: torch.Size([32, 9])\n",
      "likelihoods torch.Size([32, 9])\n",
      "l1 tensor(-632.9370, grad_fn=<SumBackward0>)\n",
      "l2 loss\n",
      "Q_sig: torch.Size([32, 512, 9])\n",
      "\n",
      "Z torch.Size([32, 5, 9])\n",
      "Z_significant: torch.Size([32, 9])\n",
      "y_pred torch.Size([32, 9])\n",
      "l2: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "b = 32\n",
    "M = 5\n",
    "K = 5\n",
    "c = 3\n",
    "height, width = 32, 32\n",
    "can = CrossAttentionNetwork()\n",
    "criterion = CANLoss(can, n_classes=M)\n",
    "\n",
    "support = torch.ones(b, M, K, c, height, width)\n",
    "query = torch.ones(b, c, height, width)\n",
    "y_true = torch.randint(0, 5, (b, 1))\n",
    "loss = criterion(support, query, y_true)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1],\n",
       "          [ 2,  3],\n",
       "          [ 4,  5],\n",
       "          [ 6,  7],\n",
       "          [ 8,  9]],\n",
       " \n",
       "         [[10, 11],\n",
       "          [12, 13],\n",
       "          [14, 15],\n",
       "          [16, 17],\n",
       "          [18, 19]],\n",
       " \n",
       "         [[20, 21],\n",
       "          [22, 23],\n",
       "          [24, 25],\n",
       "          [26, 27],\n",
       "          [28, 29]]]),\n",
       " torch.Size([2, 5, 2]),\n",
       " torch.Size([2]))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(30).view(3, 5, 2)\n",
    "idx = torch.tensor([0, 1])\n",
    "a, a[idx].shape, idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 32\n",
    "M = 5\n",
    "K = 5\n",
    "c = 3\n",
    "height, width = 32, 32\n",
    "can = CrossAttentionNetwork().cuda()\n",
    "criterion = CANLoss(can, n_classes=M).cuda()\n",
    "\n",
    "support = torch.ones(b, M, K, c, height, width).cuda()\n",
    "query = torch.ones(b, c, height, width).cuda()\n",
    "y_true = torch.randint(0, 5, (b, 1)).cuda()\n",
    "loss = criterion(support, query, y_true)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.RandomState(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def get_device():\n",
    "    return \"cuda:0\" if torch.cuda.device_count() > 0 else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "CINIC_MEAN = [0.47889522, 0.47227842, 0.43047404]\n",
    "CINIC_STD = [0.24205776, 0.23828046, 0.25874835]\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "        path: str,\n",
    "        batch_size: int,\n",
    "        shuffle: bool,\n",
    "        use_augmentations: bool,\n",
    "        subset_size: int = 600,\n",
    ") -> DataLoader:\n",
    "    augmentations = ([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomErasing()\n",
    "    ] if use_augmentations else [])\n",
    "    transform = transforms.Compose([*augmentations,\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=CINIC_MEAN,std=CINIC_STD)])\n",
    "\n",
    "    ds = torchvision.datasets.ImageFolder(path, transform=transform)\n",
    "    indices = torch.randperm(len(ds))[:subset_size]\n",
    "    subset = Subset(ds, indices)\n",
    "    loader = DataLoader(subset, batch_size=batch_size, shuffle=shuffle, num_workers=2, pin_memory=True)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_cinic(\n",
    "        data_path: str,\n",
    "        batch_size: int = 256\n",
    ") -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    train_path = os.path.join(data_path, \"train\")\n",
    "    valid_path = os.path.join(data_path, \"valid\")\n",
    "    test_path = os.path.join(data_path, \"test\")\n",
    "\n",
    "    cinic_train = get_dataset(train_path, batch_size, True, True)\n",
    "    cinic_validation = get_dataset(valid_path, batch_size, False, False)\n",
    "    cinic_test = get_dataset(test_path, batch_size, False, False)\n",
    "\n",
    "    return cinic_train, cinic_validation, cinic_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "get_cinic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

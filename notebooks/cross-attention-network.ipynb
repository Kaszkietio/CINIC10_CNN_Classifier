{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Project 1\n",
    "Wojciech Kutak\n",
    "\n",
    "---\n",
    "\n",
    "### Cross Attention Network for few-shot learning problem\n",
    "#### 1. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "DATA_PATH = os.path.join(\"..\", \"data\")\n",
    "FEW_SHOT_PATH = os.path.join(\"..\", \"data_few_shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsampler = None):\n",
    "        super().__init__()\n",
    "        self.downsampler = downsampler\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsampler is not None:\n",
    "            identity = self.downsampler(identity)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet_32x32(nn.Module):\n",
    "    def __init__(self, block: nn.Module = ResidualBlock):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 64, (3, 3), stride=1, padding=1)\n",
    "        self.bnorm = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.res_layer1 = self._residual_layer(block, 64, 128, 2)\n",
    "\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.res_layer2 = self._residual_layer(block, 128, 256, 2)\n",
    "\n",
    "        self.max_pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.res_layer3 = self._residual_layer(block, 256, 512, 2)\n",
    "\n",
    "        self.output_shape = (512, 3, 3)\n",
    "\n",
    "\n",
    "    def _residual_layer(self, block, in_channels, out_channels, blocks_num, stride=1):\n",
    "        \"\"\"Creates a residual layer consisting out of residual blocks\"\"\"\n",
    "        downsampler = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsampler = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(in_channels, out_channels, stride, downsampler))\n",
    "        for _ in range(blocks_num - 1):\n",
    "            layers.append(block(out_channels, out_channels, stride))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bnorm(self.conv(x)))\n",
    "\n",
    "        x = self.res_layer1(self.max_pool1(x))\n",
    "        x = self.res_layer2(self.max_pool2(x))\n",
    "        x = self.res_layer3(self.max_pool3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet_32x32                             [30, 512, 3, 3]           --\n",
       "├─Conv2d: 1-1                            [30, 64, 32, 32]          1,792\n",
       "├─BatchNorm2d: 1-2                       [30, 64, 32, 32]          128\n",
       "├─ReLU: 1-3                              [30, 64, 32, 32]          --\n",
       "├─MaxPool2d: 1-4                         [30, 64, 15, 15]          --\n",
       "├─Sequential: 1-5                        [30, 128, 15, 15]         --\n",
       "│    └─ResidualBlock: 2-1                [30, 128, 15, 15]         --\n",
       "│    │    └─Sequential: 3-1              [30, 128, 15, 15]         8,448\n",
       "│    │    └─Conv2d: 3-2                  [30, 128, 15, 15]         73,856\n",
       "│    │    └─BatchNorm2d: 3-3             [30, 128, 15, 15]         256\n",
       "│    │    └─ReLU: 3-4                    [30, 128, 15, 15]         --\n",
       "│    │    └─Conv2d: 3-5                  [30, 128, 15, 15]         147,584\n",
       "│    │    └─BatchNorm2d: 3-6             [30, 128, 15, 15]         256\n",
       "│    │    └─ReLU: 3-7                    [30, 128, 15, 15]         --\n",
       "│    └─ResidualBlock: 2-2                [30, 128, 15, 15]         --\n",
       "│    │    └─Conv2d: 3-8                  [30, 128, 15, 15]         147,584\n",
       "│    │    └─BatchNorm2d: 3-9             [30, 128, 15, 15]         256\n",
       "│    │    └─ReLU: 3-10                   [30, 128, 15, 15]         --\n",
       "│    │    └─Conv2d: 3-11                 [30, 128, 15, 15]         147,584\n",
       "│    │    └─BatchNorm2d: 3-12            [30, 128, 15, 15]         256\n",
       "│    │    └─ReLU: 3-13                   [30, 128, 15, 15]         --\n",
       "├─MaxPool2d: 1-6                         [30, 128, 7, 7]           --\n",
       "├─Sequential: 1-7                        [30, 256, 7, 7]           --\n",
       "│    └─ResidualBlock: 2-3                [30, 256, 7, 7]           --\n",
       "│    │    └─Sequential: 3-14             [30, 256, 7, 7]           33,280\n",
       "│    │    └─Conv2d: 3-15                 [30, 256, 7, 7]           295,168\n",
       "│    │    └─BatchNorm2d: 3-16            [30, 256, 7, 7]           512\n",
       "│    │    └─ReLU: 3-17                   [30, 256, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-18                 [30, 256, 7, 7]           590,080\n",
       "│    │    └─BatchNorm2d: 3-19            [30, 256, 7, 7]           512\n",
       "│    │    └─ReLU: 3-20                   [30, 256, 7, 7]           --\n",
       "│    └─ResidualBlock: 2-4                [30, 256, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-21                 [30, 256, 7, 7]           590,080\n",
       "│    │    └─BatchNorm2d: 3-22            [30, 256, 7, 7]           512\n",
       "│    │    └─ReLU: 3-23                   [30, 256, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-24                 [30, 256, 7, 7]           590,080\n",
       "│    │    └─BatchNorm2d: 3-25            [30, 256, 7, 7]           512\n",
       "│    │    └─ReLU: 3-26                   [30, 256, 7, 7]           --\n",
       "├─MaxPool2d: 1-8                         [30, 256, 3, 3]           --\n",
       "├─Sequential: 1-9                        [30, 512, 3, 3]           --\n",
       "│    └─ResidualBlock: 2-5                [30, 512, 3, 3]           --\n",
       "│    │    └─Sequential: 3-27             [30, 512, 3, 3]           132,096\n",
       "│    │    └─Conv2d: 3-28                 [30, 512, 3, 3]           1,180,160\n",
       "│    │    └─BatchNorm2d: 3-29            [30, 512, 3, 3]           1,024\n",
       "│    │    └─ReLU: 3-30                   [30, 512, 3, 3]           --\n",
       "│    │    └─Conv2d: 3-31                 [30, 512, 3, 3]           2,359,808\n",
       "│    │    └─BatchNorm2d: 3-32            [30, 512, 3, 3]           1,024\n",
       "│    │    └─ReLU: 3-33                   [30, 512, 3, 3]           --\n",
       "│    └─ResidualBlock: 2-6                [30, 512, 3, 3]           --\n",
       "│    │    └─Conv2d: 3-34                 [30, 512, 3, 3]           2,359,808\n",
       "│    │    └─BatchNorm2d: 3-35            [30, 512, 3, 3]           1,024\n",
       "│    │    └─ReLU: 3-36                   [30, 512, 3, 3]           --\n",
       "│    │    └─Conv2d: 3-37                 [30, 512, 3, 3]           2,359,808\n",
       "│    │    └─BatchNorm2d: 3-38            [30, 512, 3, 3]           1,024\n",
       "│    │    └─ReLU: 3-39                   [30, 512, 3, 3]           --\n",
       "==========================================================================================\n",
       "Total params: 11,024,512\n",
       "Trainable params: 11,024,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 8.95\n",
       "==========================================================================================\n",
       "Input size (MB): 0.37\n",
       "Forward/backward pass size (MB): 141.74\n",
       "Params size (MB): 44.10\n",
       "Estimated Total Size (MB): 186.21\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(ResNet_32x32(), (30, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionLayer(nn.Module):\n",
    "    def __init__(self, m: int, bottleneck_size: int):\n",
    "        super(FusionLayer, self).__init__()\n",
    "\n",
    "        self.temperature = 1.0\n",
    "        self.m = m\n",
    "        self.bottleneck_size = bottleneck_size\n",
    "        self.spatial_gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv1 = nn.Conv2d(self.m, self.bottleneck_size, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(self.bottleneck_size, self.m, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, R: torch.Tensor):\n",
    "        # print(\"FusionLayer forward\")\n",
    "        # print(\"R shape:\", R.shape)\n",
    "        b, M, m, h, w = R.shape\n",
    "        w: torch.Tensor = self.spatial_gap(R)\n",
    "        # print(\"spatial w:\", w.shape)\n",
    "        w = w.view(b * M, *w.shape[2:])\n",
    "        # print(\"spatial w after:\", w.shape)\n",
    "\n",
    "        # w = w.unsqueeze(-2)\n",
    "        # Meta learner\n",
    "        w = self.conv1(w)\n",
    "        # print(\"conv1 w:\", w.shape)\n",
    "        w = self.relu(w)\n",
    "        # print(\"relu w:\",w.shape)\n",
    "        w = self.conv2(w)\n",
    "        # print(\"conv2 w:\",w.shape)\n",
    "\n",
    "        # w = w.squeeze((-2, -1))\n",
    "        # print(w.shape)\n",
    "\n",
    "        # Convolution operation\n",
    "        # w is now a vector of average class\n",
    "        A = self.attention(w, R)\n",
    "        # print(\"Attention:\", A.shape)\n",
    "\n",
    "\n",
    "        return A\n",
    "\n",
    "\n",
    "    def attention(self, weights: torch.Tensor, R: torch.Tensor) -> torch.Tensor:\n",
    "        weights_t = weights.transpose(-3, -1).squeeze(1)\n",
    "\n",
    "        b, M, m, h, w = R.shape\n",
    "        R = R.view(b * M, m, h*w)\n",
    "        # print(\"weights:\", weights.shape)\n",
    "        # print(\"weights_t:\", weights_t.shape)\n",
    "        # print(\"R:\", R.shape)\n",
    "\n",
    "        R_mean = torch.bmm(weights_t, R) / self.temperature\n",
    "        # print(\"R_mean:\", R_mean.shape)\n",
    "        R_mean = R_mean.view(b, M, 1, m)\n",
    "        A = F.softmax(R_mean, dim=-1)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FusionLayer forward\n",
      "R shape: torch.Size([3, 5, 25, 5, 5])\n",
      "spatial w: torch.Size([3, 5, 25, 1, 1])\n",
      "spatial w after: torch.Size([15, 25, 1, 1])\n",
      "conv1 w: torch.Size([15, 15, 1, 1])\n",
      "relu w: torch.Size([15, 15, 1, 1])\n",
      "conv2 w: torch.Size([15, 25, 1, 1])\n",
      "weights: torch.Size([15, 25, 1, 1])\n",
      "weights_t: torch.Size([15, 1, 25])\n",
      "R: torch.Size([15, 25, 25])\n",
      "R_mean: torch.Size([15, 1, 25])\n",
      "Attention: torch.Size([3, 5, 1, 25])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FusionLayer                              [3, 5, 1, 25]             --\n",
       "├─AdaptiveAvgPool2d: 1-1                 [3, 5, 25, 1, 1]          --\n",
       "├─Conv2d: 1-2                            [15, 15, 1, 1]            390\n",
       "├─ReLU: 1-3                              [15, 15, 1, 1]            --\n",
       "├─Conv2d: 1-4                            [15, 25, 1, 1]            400\n",
       "==========================================================================================\n",
       "Total params: 790\n",
       "Trainable params: 790\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.04\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width, height = 5, 5\n",
    "m = width*height\n",
    "M = 5\n",
    "batch_size = 3\n",
    "summary(FusionLayer(m=m, bottleneck_size=15), (batch_size, M, m, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  2.,   4.],\n",
       "          [  3.,   6.],\n",
       "          [  4.,   8.]],\n",
       "\n",
       "         [[ 10.,  20.],\n",
       "          [ 11.,  22.],\n",
       "          [112., 224.]]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 2.0]).view(1, 2)\n",
    "b = torch.tensor([[2.0, 3.0, 4.0], [10.0, 11.0, 112.0]]).view(1, 2, 3, 1)\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionModule(nn.Module):\n",
    "    def __init__(self, input_shape: tuple[int,int,int]):\n",
    "        super(CrossAttentionModule, self).__init__()\n",
    "\n",
    "        # print(\"input shape\", *input_shape)\n",
    "        c, h, w = input_shape\n",
    "        self.fusion_layer = FusionLayer(m=h*w, bottleneck_size=int(h*w*2/3))\n",
    "\n",
    "\n",
    "    def forward(self, P: torch.Tensor, Q: torch.Tensor):\n",
    "    # def forward(self, S: torch.Tensor):\n",
    "        # P, Q = S[0], S[1, :, 0, :, :, :]\n",
    "        # print(\"CrossAttentionModule forward\")\n",
    "        # print(P.shape, Q.shape)\n",
    "        # assert P.shape == Q.shape\n",
    "        b, M, c, h, w = P.shape\n",
    "        assert (b, c, h, w) == Q.shape\n",
    "        m = h*w\n",
    "        # Change representation from tensor c*h*w to c*m (2 dims)\n",
    "        P = P.view(b, M, c, m)\n",
    "        Q = Q.view(b, 1, c, m)\n",
    "\n",
    "        P_norm = F.normalize(P, p=2, dim=1)\n",
    "        Q_norm = F.normalize(Q, p=2, dim=1)\n",
    "        # print(\"P norm:\", P_norm.shape)\n",
    "        P_norm_t = P_norm.transpose(-2, -1)\n",
    "        # print(\"P norm t:\", P_norm_t.shape)\n",
    "        # print(\"Q norm:\", Q_norm.shape)\n",
    "        R_q = torch.matmul(P_norm_t, Q_norm)\n",
    "        R_p = R_q.transpose(-2, -1).view(b, M, m, h, w)\n",
    "        R_q = R_q.view(b, M, m, h, w)\n",
    "        # print(\"R_q\", R_q.shape)\n",
    "        # print(\"R_p\", R_p.shape)\n",
    "\n",
    "\n",
    "\n",
    "        A_p: torch.Tensor = self.fusion_layer(R_p)\n",
    "        # print()\n",
    "        A_q: torch.Tensor = self.fusion_layer(R_q)\n",
    "        # print()\n",
    "\n",
    "        # print(\"P:\", P.shape, \"A_p:\", A_p.shape)\n",
    "        P_feat = torch.mul(P, A_p) + P\n",
    "        # print(\"P_feat:\", P_feat.shape)\n",
    "\n",
    "        # print(\"Q:\", Q.shape, \"A_q:\", A_q.shape)\n",
    "        Q = Q.expand(b, M, c, m)\n",
    "        # print(\"Q exp:\", Q.shape, \"A_q:\", A_q.shape)\n",
    "        Q_feat = torch.mul(Q, A_q) + Q\n",
    "        # print(\"Q_feat:\", Q_feat.shape)\n",
    "        return P_feat, Q_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1],\n",
      "         [2, 3]],\n",
      "\n",
      "        [[4, 5],\n",
      "         [6, 7]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0, 1],\n",
       "          [2, 3]],\n",
       " \n",
       "         [[4, 5],\n",
       "          [6, 7]]]),\n",
       " tensor([[[0, 1],\n",
       "          [2, 3]],\n",
       " \n",
       "         [[4, 5],\n",
       "          [6, 7]]]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(8).reshape(2, 2, 2)\n",
    "print(a)\n",
    "a2 = a.expand(2, 2, 2, 2).transpose(0, 1)\n",
    "a2[:, 0, :, :], a2[:, 1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossAttentionModule forward\n",
      "torch.Size([3, 5, 512, 5, 5]) torch.Size([3, 512, 5, 5])\n",
      "P norm: torch.Size([3, 5, 512, 25])\n",
      "P norm t: torch.Size([3, 5, 25, 512])\n",
      "Q norm: torch.Size([3, 1, 512, 25])\n",
      "R_q torch.Size([3, 5, 25, 5, 5])\n",
      "R_p torch.Size([3, 5, 25, 5, 5])\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([3, 5, 25, 5, 5])\n",
      "spatial w: torch.Size([3, 5, 25, 1, 1])\n",
      "spatial w after: torch.Size([15, 25, 1, 1])\n",
      "conv1 w: torch.Size([15, 15, 1, 1])\n",
      "relu w: torch.Size([15, 15, 1, 1])\n",
      "conv2 w: torch.Size([15, 25, 1, 1])\n",
      "weights: torch.Size([15, 25, 1, 1])\n",
      "weights_t: torch.Size([15, 1, 25])\n",
      "R: torch.Size([15, 25, 25])\n",
      "R_mean: torch.Size([15, 1, 25])\n",
      "Attention: torch.Size([3, 5, 1, 25])\n",
      "\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([3, 5, 25, 5, 5])\n",
      "spatial w: torch.Size([3, 5, 25, 1, 1])\n",
      "spatial w after: torch.Size([15, 25, 1, 1])\n",
      "conv1 w: torch.Size([15, 15, 1, 1])\n",
      "relu w: torch.Size([15, 15, 1, 1])\n",
      "conv2 w: torch.Size([15, 25, 1, 1])\n",
      "weights: torch.Size([15, 25, 1, 1])\n",
      "weights_t: torch.Size([15, 1, 25])\n",
      "R: torch.Size([15, 25, 25])\n",
      "R_mean: torch.Size([15, 1, 25])\n",
      "Attention: torch.Size([3, 5, 1, 25])\n",
      "\n",
      "P: torch.Size([3, 5, 512, 25]) A_p: torch.Size([3, 5, 1, 25])\n",
      "P_feat: torch.Size([3, 5, 512, 25])\n",
      "Q: torch.Size([3, 1, 512, 25]) A_q: torch.Size([3, 5, 1, 25])\n",
      "Q exp: torch.Size([3, 5, 512, 25]) A_q: torch.Size([3, 5, 1, 25])\n",
      "Q_feat: torch.Size([3, 5, 512, 25])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CrossAttentionModule                     [3, 5, 512, 25]           --\n",
       "├─FusionLayer: 1-1                       [3, 5, 1, 25]             --\n",
       "│    └─AdaptiveAvgPool2d: 2-1            [3, 5, 25, 1, 1]          --\n",
       "│    └─Conv2d: 2-2                       [15, 15, 1, 1]            390\n",
       "│    └─ReLU: 2-3                         [15, 15, 1, 1]            --\n",
       "│    └─Conv2d: 2-4                       [15, 25, 1, 1]            400\n",
       "├─FusionLayer: 1-2                       [3, 5, 1, 25]             (recursive)\n",
       "│    └─AdaptiveAvgPool2d: 2-5            [3, 5, 25, 1, 1]          --\n",
       "│    └─Conv2d: 2-6                       [15, 15, 1, 1]            (recursive)\n",
       "│    └─ReLU: 2-7                         [15, 15, 1, 1]            --\n",
       "│    └─Conv2d: 2-8                       [15, 25, 1, 1]            (recursive)\n",
       "==========================================================================================\n",
       "Total params: 790\n",
       "Trainable params: 790\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.02\n",
       "==========================================================================================\n",
       "Input size (MB): 1.54\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 1.55\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 3\n",
    "m = 5\n",
    "\n",
    "summary(CrossAttentionModule((512, 5, 5)), (2, b, m, 512, 5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  4.],\n",
       "         [ 9., 16.]]),\n",
       " tensor([[ 7., 10.],\n",
       "         [15., 22.]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "B = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "torch.mul(A, B), torch.matmul(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7., 22.],\n",
       "        [21., 44.]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1.0, 2.0], [3., 4.]])\n",
    "B = torch.tensor([[7., 11.]])\n",
    "A*B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionNetwork(nn.Module):\n",
    "    def __init__(self, cam: CrossAttentionModule = None):\n",
    "        super(CrossAttentionNetwork, self).__init__()\n",
    "\n",
    "        self.embedding = ResNet_32x32()\n",
    "        self.cam = cam if cam is not None else CrossAttentionModule(self.embedding.output_shape)\n",
    "\n",
    "\n",
    "    def forward(self, support: torch.Tensor, query: torch.Tensor):\n",
    "    # def forward(self, X: torch.Tensor):\n",
    "\n",
    "        # support = X[0]\n",
    "        # query = X[1, :, 0, 0, :, :]\n",
    "        b, M, K, c, h, w = support.shape\n",
    "        # print(\"Support: \", support.shape)\n",
    "        # print(\"query: \", query.shape)\n",
    "\n",
    "        # Shapes of support and query tensors should be\n",
    "        # - support.shape = (b, M, K, c, h, w),\n",
    "        # - query.shape = (b, c, h, w),\n",
    "        # where b - batch size, M - number of classes, K - number of shots (examples per class),\n",
    "        # c - channels, h - height and w - width.\n",
    "\n",
    "\n",
    "        # Class feature P is defined as an average of embedded samples from one class\n",
    "        P = self.embedding(support.view(-1, c, h, w))\n",
    "        embedding_shape = P.shape[1:]\n",
    "        P = P.view(b, M, K, *embedding_shape)\n",
    "        P = torch.mean(P, dim=2)\n",
    "\n",
    "        Q = self.embedding(query)\n",
    "        P_feature, Q_feature = self.cam(P, Q)\n",
    "\n",
    "        # print(\"P_feature:\", P_feature.shape)\n",
    "        # print(\"Q_feature:\", Q_feature.shape)\n",
    "\n",
    "        return P_feature, Q_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape 512 3 3\n",
      "Support:  torch.Size([32, 5, 5, 3, 32, 32])\n",
      "query:  torch.Size([32, 3, 32, 32])\n",
      "CrossAttentionModule forward\n",
      "torch.Size([32, 5, 512, 3, 3]) torch.Size([32, 512, 3, 3])\n",
      "P norm: torch.Size([32, 5, 512, 9])\n",
      "P norm t: torch.Size([32, 5, 9, 512])\n",
      "Q norm: torch.Size([32, 1, 512, 9])\n",
      "R_q torch.Size([32, 5, 9, 3, 3])\n",
      "R_p torch.Size([32, 5, 9, 3, 3])\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([32, 5, 9, 3, 3])\n",
      "spatial w: torch.Size([32, 5, 9, 1, 1])\n",
      "spatial w after: torch.Size([160, 9, 1, 1])\n",
      "conv1 w: torch.Size([160, 6, 1, 1])\n",
      "relu w: torch.Size([160, 6, 1, 1])\n",
      "conv2 w: torch.Size([160, 9, 1, 1])\n",
      "weights: torch.Size([160, 9, 1, 1])\n",
      "weights_t: torch.Size([160, 1, 9])\n",
      "R: torch.Size([160, 9, 9])\n",
      "R_mean: torch.Size([160, 1, 9])\n",
      "Attention: torch.Size([32, 5, 1, 9])\n",
      "\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([32, 5, 9, 3, 3])\n",
      "spatial w: torch.Size([32, 5, 9, 1, 1])\n",
      "spatial w after: torch.Size([160, 9, 1, 1])\n",
      "conv1 w: torch.Size([160, 6, 1, 1])\n",
      "relu w: torch.Size([160, 6, 1, 1])\n",
      "conv2 w: torch.Size([160, 9, 1, 1])\n",
      "weights: torch.Size([160, 9, 1, 1])\n",
      "weights_t: torch.Size([160, 1, 9])\n",
      "R: torch.Size([160, 9, 9])\n",
      "R_mean: torch.Size([160, 1, 9])\n",
      "Attention: torch.Size([32, 5, 1, 9])\n",
      "\n",
      "P: torch.Size([32, 5, 512, 9]) A_p: torch.Size([32, 5, 1, 9])\n",
      "P_feat: torch.Size([32, 5, 512, 9])\n",
      "Q: torch.Size([32, 1, 512, 9]) A_q: torch.Size([32, 5, 1, 9])\n",
      "Q exp: torch.Size([32, 5, 512, 9]) A_q: torch.Size([32, 5, 1, 9])\n",
      "Q_feat: torch.Size([32, 5, 512, 9])\n",
      "P_feature: torch.Size([32, 5, 512, 9])\n",
      "Q_feature: torch.Size([32, 5, 512, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "CrossAttentionNetwork                         [32, 5, 512, 9]           --\n",
       "├─ResNet_32x32: 1-1                           [800, 512, 3, 3]          --\n",
       "│    └─Conv2d: 2-1                            [800, 64, 32, 32]         1,792\n",
       "│    └─BatchNorm2d: 2-2                       [800, 64, 32, 32]         128\n",
       "│    └─ReLU: 2-3                              [800, 64, 32, 32]         --\n",
       "│    └─MaxPool2d: 2-4                         [800, 64, 15, 15]         --\n",
       "│    └─Sequential: 2-5                        [800, 128, 15, 15]        --\n",
       "│    │    └─ResidualBlock: 3-1                [800, 128, 15, 15]        230,400\n",
       "│    │    └─ResidualBlock: 3-2                [800, 128, 15, 15]        295,680\n",
       "│    └─MaxPool2d: 2-6                         [800, 128, 7, 7]          --\n",
       "│    └─Sequential: 2-7                        [800, 256, 7, 7]          --\n",
       "│    │    └─ResidualBlock: 3-3                [800, 256, 7, 7]          919,552\n",
       "│    │    └─ResidualBlock: 3-4                [800, 256, 7, 7]          1,181,184\n",
       "│    └─MaxPool2d: 2-8                         [800, 256, 3, 3]          --\n",
       "│    └─Sequential: 2-9                        [800, 512, 3, 3]          --\n",
       "│    │    └─ResidualBlock: 3-5                [800, 512, 3, 3]          3,674,112\n",
       "│    │    └─ResidualBlock: 3-6                [800, 512, 3, 3]          4,721,664\n",
       "├─ResNet_32x32: 1-2                           [32, 512, 3, 3]           (recursive)\n",
       "│    └─Conv2d: 2-10                           [32, 64, 32, 32]          (recursive)\n",
       "│    └─BatchNorm2d: 2-11                      [32, 64, 32, 32]          (recursive)\n",
       "│    └─ReLU: 2-12                             [32, 64, 32, 32]          --\n",
       "│    └─MaxPool2d: 2-13                        [32, 64, 15, 15]          --\n",
       "│    └─Sequential: 2-14                       [32, 128, 15, 15]         (recursive)\n",
       "│    │    └─ResidualBlock: 3-7                [32, 128, 15, 15]         (recursive)\n",
       "│    │    └─ResidualBlock: 3-8                [32, 128, 15, 15]         (recursive)\n",
       "│    └─MaxPool2d: 2-15                        [32, 128, 7, 7]           --\n",
       "│    └─Sequential: 2-16                       [32, 256, 7, 7]           (recursive)\n",
       "│    │    └─ResidualBlock: 3-9                [32, 256, 7, 7]           (recursive)\n",
       "│    │    └─ResidualBlock: 3-10               [32, 256, 7, 7]           (recursive)\n",
       "│    └─MaxPool2d: 2-17                        [32, 256, 3, 3]           --\n",
       "│    └─Sequential: 2-18                       [32, 512, 3, 3]           (recursive)\n",
       "│    │    └─ResidualBlock: 3-11               [32, 512, 3, 3]           (recursive)\n",
       "│    │    └─ResidualBlock: 3-12               [32, 512, 3, 3]           (recursive)\n",
       "├─CrossAttentionModule: 1-3                   [32, 5, 512, 9]           --\n",
       "│    └─FusionLayer: 2-19                      [32, 5, 1, 9]             --\n",
       "│    │    └─AdaptiveAvgPool2d: 3-13           [32, 5, 9, 1, 1]          --\n",
       "│    │    └─Conv2d: 3-14                      [160, 6, 1, 1]            60\n",
       "│    │    └─ReLU: 3-15                        [160, 6, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-16                      [160, 9, 1, 1]            63\n",
       "│    └─FusionLayer: 2-20                      [32, 5, 1, 9]             (recursive)\n",
       "│    │    └─AdaptiveAvgPool2d: 3-17           [32, 5, 9, 1, 1]          --\n",
       "│    │    └─Conv2d: 3-18                      [160, 6, 1, 1]            (recursive)\n",
       "│    │    └─ReLU: 3-19                        [160, 6, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-20                      [160, 9, 1, 1]            (recursive)\n",
       "===============================================================================================\n",
       "Total params: 11,024,635\n",
       "Trainable params: 11,024,635\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 248.14\n",
       "===============================================================================================\n",
       "Input size (MB): 19.66\n",
       "Forward/backward pass size (MB): 3931.02\n",
       "Params size (MB): 44.10\n",
       "Estimated Total Size (MB): 3994.78\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(CrossAttentionNetwork(), (2, 32, 5, 5, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CANLoss(nn.Module):\n",
    "    def __init__(self, can: CrossAttentionNetwork, lamb: float = 0.5, n_classes: int = 5):\n",
    "        super(CANLoss, self).__init__()\n",
    "\n",
    "        self.can = can\n",
    "        self.lamb = lamb\n",
    "        self.classifier = nn.LazyConv1d(out_channels=n_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def cosine_dist(self, P_gap: torch.Tensor, Q_features: torch.Tensor) -> torch.Tensor:\n",
    "        # print(\"\\ncosine_dist\")\n",
    "        # print(\"Q_features\",  Q_features.shape)\n",
    "        # Q_features: (b, c, m)\n",
    "        b, c, m = Q_features.shape\n",
    "        # print(\"P_gap\",  P_gap.shape)\n",
    "        # P_gap: (b, c)\n",
    "        P_gap_exp = P_gap.expand(m, b, c).transpose(0, 1).transpose(1, 2)\n",
    "        # print(\"P_gap_exp\", P_gap_exp.shape)\n",
    "        cos_dist = F.cosine_similarity(P_gap_exp, Q_features, dim=-2)\n",
    "        # print(\"cos_dist\", cos_dist.shape)\n",
    "        return cos_dist\n",
    "\n",
    "\n",
    "    def L1_loss(self, P_sig: torch.Tensor, Q_sig: torch.Tensor, y_true: torch.Tensor):\n",
    "        # P_features: (b, c, m)\n",
    "        # Q_features: (b, c, m)\n",
    "        # print(\"l1 loss\")\n",
    "        # print(\"P_features:\", P_sig.shape)\n",
    "        # print(\"Q_features:\", Q_sig.shape)\n",
    "\n",
    "\n",
    "        P_gap = torch.mean(P_sig, dim=-1)\n",
    "        # P_gap: (b, c)\n",
    "        distances = self.cosine_dist(P_gap, Q_sig)\n",
    "        # distances: (b, m)\n",
    "        # print(\"Distances:\", distances.shape)\n",
    "\n",
    "        likelihoods = torch.log(F.softmax(-distances, dim=-1))\n",
    "        # print(\"likelihoods\", likelihoods.shape)\n",
    "        l1 = torch.sum(likelihoods)\n",
    "        # print(\"l1\", l1)\n",
    "        return l1\n",
    "\n",
    "\n",
    "    def L2_loss(self, Q_sig: torch.Tensor, y_true: torch.Tensor):\n",
    "        # print(\"l2 loss\")\n",
    "        # print(\"Q_sig:\", Q_sig.shape)\n",
    "        # print()\n",
    "\n",
    "        Z: torch.Tensor = self.classifier(Q_sig)\n",
    "        # print(\"Z\", Z.shape)\n",
    "        Z_significant = []\n",
    "        for y, z in zip(y_true, Z):\n",
    "            Z_significant.append(z[y])\n",
    "        Z_significant = torch.concat(Z_significant)\n",
    "        # print(\"Z_significant:\", Z_significant.shape)\n",
    "\n",
    "        y_pred = F.softmax(Z_significant, dim=-1)\n",
    "        # print(\"y_pred\", y_pred.shape)\n",
    "        l2 = torch.sum(y_pred)\n",
    "        # print(\"l2:\", l2.shape)\n",
    "        return l2\n",
    "\n",
    "\n",
    "    def forward(self, support: torch.Tensor, query: torch.Tensor, y_true: torch.Tensor):\n",
    "        P_features, Q_features = self.can(support, query)\n",
    "        Q_significant = []\n",
    "        P_significant = []\n",
    "        for y, q, p in zip(y_true, Q_features, P_features):\n",
    "            Q_significant.append(q[y])\n",
    "            P_significant.append(p[y])\n",
    "        Q_significant = torch.concatenate(Q_significant, dim=0)\n",
    "        P_significant = torch.concatenate(P_significant, dim=0)\n",
    "        l1 = self.L1_loss(P_significant, Q_significant, y_true)\n",
    "        l2 = self.L2_loss(Q_significant, y_true)\n",
    "        loss = self.lamb*l1 + l2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape 512 3 3\n",
      "Support:  torch.Size([32, 5, 5, 3, 32, 32])\n",
      "query:  torch.Size([32, 3, 32, 32])\n",
      "CrossAttentionModule forward\n",
      "torch.Size([32, 5, 512, 3, 3]) torch.Size([32, 512, 3, 3])\n",
      "P norm: torch.Size([32, 5, 512, 9])\n",
      "P norm t: torch.Size([32, 5, 9, 512])\n",
      "Q norm: torch.Size([32, 1, 512, 9])\n",
      "R_q torch.Size([32, 5, 9, 3, 3])\n",
      "R_p torch.Size([32, 5, 9, 3, 3])\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([32, 5, 9, 3, 3])\n",
      "spatial w: torch.Size([32, 5, 9, 1, 1])\n",
      "spatial w after: torch.Size([160, 9, 1, 1])\n",
      "conv1 w: torch.Size([160, 6, 1, 1])\n",
      "relu w: torch.Size([160, 6, 1, 1])\n",
      "conv2 w: torch.Size([160, 9, 1, 1])\n",
      "weights: torch.Size([160, 9, 1, 1])\n",
      "weights_t: torch.Size([160, 1, 9])\n",
      "R: torch.Size([160, 9, 9])\n",
      "R_mean: torch.Size([160, 1, 9])\n",
      "Attention: torch.Size([32, 5, 1, 9])\n",
      "\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([32, 5, 9, 3, 3])\n",
      "spatial w: torch.Size([32, 5, 9, 1, 1])\n",
      "spatial w after: torch.Size([160, 9, 1, 1])\n",
      "conv1 w: torch.Size([160, 6, 1, 1])\n",
      "relu w: torch.Size([160, 6, 1, 1])\n",
      "conv2 w: torch.Size([160, 9, 1, 1])\n",
      "weights: torch.Size([160, 9, 1, 1])\n",
      "weights_t: torch.Size([160, 1, 9])\n",
      "R: torch.Size([160, 9, 9])\n",
      "R_mean: torch.Size([160, 1, 9])\n",
      "Attention: torch.Size([32, 5, 1, 9])\n",
      "\n",
      "P: torch.Size([32, 5, 512, 9]) A_p: torch.Size([32, 5, 1, 9])\n",
      "P_feat: torch.Size([32, 5, 512, 9])\n",
      "Q: torch.Size([32, 1, 512, 9]) A_q: torch.Size([32, 5, 1, 9])\n",
      "Q exp: torch.Size([32, 5, 512, 9]) A_q: torch.Size([32, 5, 1, 9])\n",
      "Q_feat: torch.Size([32, 5, 512, 9])\n",
      "P_feature: torch.Size([32, 5, 512, 9])\n",
      "Q_feature: torch.Size([32, 5, 512, 9])\n",
      "l1 loss\n",
      "P_features: torch.Size([32, 512, 9])\n",
      "Q_features: torch.Size([32, 512, 9])\n",
      "\n",
      "cosine_dist\n",
      "Q_features torch.Size([32, 512, 9])\n",
      "P_gap torch.Size([32, 512])\n",
      "P_gap_exp torch.Size([32, 512, 9])\n",
      "cos_dist torch.Size([32, 9])\n",
      "Distances: torch.Size([32, 9])\n",
      "likelihoods torch.Size([32, 9])\n",
      "l1 tensor(-632.9370, grad_fn=<SumBackward0>)\n",
      "l2 loss\n",
      "Q_sig: torch.Size([32, 512, 9])\n",
      "\n",
      "Z torch.Size([32, 5, 9])\n",
      "Z_significant: torch.Size([32, 9])\n",
      "y_pred torch.Size([32, 9])\n",
      "l2: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "b = 32\n",
    "M = 5\n",
    "K = 5\n",
    "c = 3\n",
    "height, width = 32, 32\n",
    "can = CrossAttentionNetwork()\n",
    "criterion = CANLoss(can, n_classes=M)\n",
    "\n",
    "support = torch.ones(b, M, K, c, height, width)\n",
    "query = torch.ones(b, c, height, width)\n",
    "y_true = torch.randint(0, 5, (b, 1))\n",
    "loss = criterion(support, query, y_true)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1],\n",
       "          [ 2,  3],\n",
       "          [ 4,  5],\n",
       "          [ 6,  7],\n",
       "          [ 8,  9]],\n",
       " \n",
       "         [[10, 11],\n",
       "          [12, 13],\n",
       "          [14, 15],\n",
       "          [16, 17],\n",
       "          [18, 19]],\n",
       " \n",
       "         [[20, 21],\n",
       "          [22, 23],\n",
       "          [24, 25],\n",
       "          [26, 27],\n",
       "          [28, 29]]]),\n",
       " torch.Size([2, 5, 2]),\n",
       " torch.Size([2]))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(30).view(3, 5, 2)\n",
    "idx = torch.tensor([0, 1])\n",
    "a, a[idx].shape, idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 32\n",
    "M = 5\n",
    "K = 5\n",
    "c = 3\n",
    "height, width = 32, 32\n",
    "can = CrossAttentionNetwork().cuda()\n",
    "criterion = CANLoss(can, n_classes=M).cuda()\n",
    "\n",
    "support = torch.ones(b, M, K, c, height, width).cuda()\n",
    "query = torch.ones(b, c, height, width).cuda()\n",
    "y_true = torch.randint(0, 5, (b, 1)).cuda()\n",
    "loss = criterion(support, query, y_true)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.RandomState(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def get_device():\n",
    "    return \"cuda:0\" if torch.cuda.device_count() > 0 else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "import random\n",
    "\n",
    "class FewShotSampler(Sampler):\n",
    "    def __init__(self, data_source, n_shots: int, shuffle: bool = True):\n",
    "        self.data_source = data_source\n",
    "        self.n_shots = n_shots\n",
    "        self.classes = self._get_classes()\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "\n",
    "    def _get_classes(self):\n",
    "        classes = dict()\n",
    "        for i, (_, label) in enumerate(self.data_source):\n",
    "            if label not in classes:\n",
    "                classes[label] = []\n",
    "            classes[label].append(i)\n",
    "\n",
    "        return classes\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = []\n",
    "        n = len(self.data_source)\n",
    "        n_classes = len(self.classes)\n",
    "        episodes = n // ((self.n_shots + 1) * n_classes)\n",
    "\n",
    "        for i in range(episodes):\n",
    "            query = []\n",
    "            for c in self.classes:\n",
    "                class_ind = self.classes[c]\n",
    "                lower = i*(self.n_shots + 1)\n",
    "                upper = (i+1)*(self.n_shots + 1) - 1\n",
    "                support_one_class = class_ind[lower : upper]\n",
    "                query.append(class_ind[upper])\n",
    "                if self.shuffle:\n",
    "                    random.shuffle(support_one_class)\n",
    "                indices.extend(support_one_class)\n",
    "            indices.extend(query)\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import shutil\n",
    "\n",
    "\n",
    "CINIC_MEAN = [0.47889522, 0.47227842, 0.43047404]\n",
    "CINIC_STD = [0.24205776, 0.23828046, 0.25874835]\n",
    "\n",
    "\n",
    "def prepare_data_folder(data_path: str, few_shot_path: str):\n",
    "    subsets = [\"train\", \"valid\", \"test\"]\n",
    "    classes = {\"train\": [ 'bird', 'cat', 'deer', 'dog', 'frog', 'horse'],\n",
    "               \"valid\": ['airplane','automobile'],\n",
    "               \"test\": [ 'ship','truck']\n",
    "    }\n",
    "    [os.makedirs(os.path.join(few_shot_path, sub), exist_ok=True) for sub in subsets]\n",
    "    [[os.makedirs(os.path.join(few_shot_path, sub, c), exist_ok=True) for c in classes[sub]]\n",
    "     for sub in subsets]\n",
    "    for sub in subsets:\n",
    "        old_sub_path = os.path.join(data_path, sub)\n",
    "        new_sub_path = os.path.join(few_shot_path, sub)\n",
    "        for c in classes[sub]:\n",
    "            old_dir = os.path.join(old_sub_path, c)\n",
    "            new_dir = os.path.join(new_sub_path, c)\n",
    "            imgs = os.listdir(old_dir)\n",
    "            random.shuffle(imgs)\n",
    "            # 600 support imgs -> 120 batches of 5-way\n",
    "            # 120 imgs for query\n",
    "            # 720 in total\n",
    "            new_imgs = imgs[:720]\n",
    "            [shutil.copyfile(os.path.join(old_dir, img), os.path.join(new_dir, img))\n",
    "             for img in new_imgs]\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "        path: str,\n",
    "        batch_size: int,\n",
    "        n_shots: int,\n",
    "        shuffle: bool,\n",
    "        use_augmentations: bool,\n",
    ") -> DataLoader:\n",
    "    augmentations = ([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomErasing()\n",
    "    ] if use_augmentations else [])\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=CINIC_MEAN,std=CINIC_STD),\n",
    "                                    *augmentations])\n",
    "\n",
    "    ds = torchvision.datasets.ImageFolder(path, transform=transform)\n",
    "    sampler = FewShotSampler(ds, n_shots, shuffle=shuffle)\n",
    "    n_way = len(ds.classes)\n",
    "    episode_size = batch_size * n_way * (n_shots + 1)\n",
    "    loader = DataLoader(ds, sampler=sampler, batch_size=episode_size, num_workers=2, pin_memory=True)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_cinic_few(\n",
    "        data_path: str,\n",
    "        batch_size: int,\n",
    "        n_shots: int,\n",
    ") -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    train_path = os.path.join(data_path, \"train\")\n",
    "    valid_path = os.path.join(data_path, \"valid\")\n",
    "    test_path = os.path.join(data_path, \"test\")\n",
    "\n",
    "    cinic_train = get_dataset(train_path, batch_size, n_shots, True, True)\n",
    "    cinic_validation = get_dataset(valid_path, batch_size, n_shots, False, False)\n",
    "    cinic_test = get_dataset(test_path, batch_size, n_shots, False, False)\n",
    "\n",
    "    return cinic_train, cinic_validation, cinic_test\n",
    "\n",
    "\n",
    "def parse_episode_batch(episode_samples: torch.Tensor, episode_labels: torch.Tensor, n_way: int, n_shot: int):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_data_folder(DATA_PATH, FEW_SHOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(os.path.join(FEW_SHOT_PATH, \"train\", \"bird\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440\n",
      "0 : 720\n",
      "1 : 720\n",
      "1440\n",
      "1440 120\n",
      "0 0 (l, u): 0 5\n",
      "support: 5\n",
      "0 1 (l, u): 0 5\n",
      "support: 5\n",
      "query; 2\n",
      "1 0 (l, u): 6 11\n",
      "support: 5\n",
      "1 1 (l, u): 6 11\n",
      "support: 5\n",
      "query; 2\n",
      "2 0 (l, u): 12 17\n",
      "support: 5\n",
      "2 1 (l, u): 12 17\n",
      "support: 5\n",
      "query; 2\n",
      "3 0 (l, u): 18 23\n",
      "support: 5\n",
      "3 1 (l, u): 18 23\n",
      "support: 5\n",
      "query; 2\n",
      "4 0 (l, u): 24 29\n",
      "support: 5\n",
      "4 1 (l, u): 24 29\n",
      "support: 5\n",
      "query; 2\n",
      "5 0 (l, u): 30 35\n",
      "support: 5\n",
      "5 1 (l, u): 30 35\n",
      "support: 5\n",
      "query; 2\n",
      "6 0 (l, u): 36 41\n",
      "support: 5\n",
      "6 1 (l, u): 36 41\n",
      "support: 5\n",
      "query; 2\n",
      "7 0 (l, u): 42 47\n",
      "support: 5\n",
      "7 1 (l, u): 42 47\n",
      "support: 5\n",
      "query; 2\n",
      "8 0 (l, u): 48 53\n",
      "support: 5\n",
      "8 1 (l, u): 48 53\n",
      "support: 5\n",
      "query; 2\n",
      "9 0 (l, u): 54 59\n",
      "support: 5\n",
      "9 1 (l, u): 54 59\n",
      "support: 5\n",
      "query; 2\n",
      "10 0 (l, u): 60 65\n",
      "support: 5\n",
      "10 1 (l, u): 60 65\n",
      "support: 5\n",
      "query; 2\n",
      "11 0 (l, u): 66 71\n",
      "support: 5\n",
      "11 1 (l, u): 66 71\n",
      "support: 5\n",
      "query; 2\n",
      "12 0 (l, u): 72 77\n",
      "support: 5\n",
      "12 1 (l, u): 72 77\n",
      "support: 5\n",
      "query; 2\n",
      "13 0 (l, u): 78 83\n",
      "support: 5\n",
      "13 1 (l, u): 78 83\n",
      "support: 5\n",
      "query; 2\n",
      "14 0 (l, u): 84 89\n",
      "support: 5\n",
      "14 1 (l, u): 84 89\n",
      "support: 5\n",
      "query; 2\n",
      "15 0 (l, u): 90 95\n",
      "support: 5\n",
      "15 1 (l, u): 90 95\n",
      "support: 5\n",
      "query; 2\n",
      "16 0 (l, u): 96 101\n",
      "support: 5\n",
      "16 1 (l, u): 96 101\n",
      "support: 5\n",
      "query; 2\n",
      "17 0 (l, u): 102 107\n",
      "support: 5\n",
      "17 1 (l, u): 102 107\n",
      "support: 5\n",
      "query; 2\n",
      "18 0 (l, u): 108 113\n",
      "support: 5\n",
      "18 1 (l, u): 108 113\n",
      "support: 5\n",
      "query; 2\n",
      "19 0 (l, u): 114 119\n",
      "support: 5\n",
      "19 1 (l, u): 114 119\n",
      "support: 5\n",
      "query; 2\n",
      "20 0 (l, u): 120 125\n",
      "support: 5\n",
      "20 1 (l, u): 120 125\n",
      "support: 5\n",
      "query; 2\n",
      "21 0 (l, u): 126 131\n",
      "support: 5\n",
      "21 1 (l, u): 126 131\n",
      "support: 5\n",
      "query; 2\n",
      "22 0 (l, u): 132 137\n",
      "support: 5\n",
      "22 1 (l, u): 132 137\n",
      "support: 5\n",
      "query; 2\n",
      "23 0 (l, u): 138 143\n",
      "support: 5\n",
      "23 1 (l, u): 138 143\n",
      "support: 5\n",
      "query; 2\n",
      "24 0 (l, u): 144 149\n",
      "support: 5\n",
      "24 1 (l, u): 144 149\n",
      "support: 5\n",
      "query; 2\n",
      "25 0 (l, u): 150 155\n",
      "support: 5\n",
      "25 1 (l, u): 150 155\n",
      "support: 5\n",
      "query; 2\n",
      "26 0 (l, u): 156 161\n",
      "support: 5\n",
      "26 1 (l, u): 156 161\n",
      "support: 5\n",
      "query; 2\n",
      "27 0 (l, u): 162 167\n",
      "support: 5\n",
      "27 1 (l, u): 162 167\n",
      "support: 5\n",
      "query; 2\n",
      "28 0 (l, u): 168 173\n",
      "support: 5\n",
      "28 1 (l, u): 168 173\n",
      "support: 5\n",
      "query; 2\n",
      "29 0 (l, u): 174 179\n",
      "support: 5\n",
      "29 1 (l, u): 174 179\n",
      "support: 5\n",
      "query; 2\n",
      "30 0 (l, u): 180 185\n",
      "support: 5\n",
      "30 1 (l, u): 180 185\n",
      "support: 5\n",
      "query; 2\n",
      "31 0 (l, u): 186 191\n",
      "support: 5\n",
      "31 1 (l, u): 186 191\n",
      "support: 5\n",
      "query; 2\n",
      "32 0 (l, u): 192 197\n",
      "support: 5\n",
      "32 1 (l, u): 192 197\n",
      "support: 5\n",
      "query; 2\n",
      "33 0 (l, u): 198 203\n",
      "support: 5\n",
      "33 1 (l, u): 198 203\n",
      "support: 5\n",
      "query; 2\n",
      "34 0 (l, u): 204 209\n",
      "support: 5\n",
      "34 1 (l, u): 204 209\n",
      "support: 5\n",
      "query; 2\n",
      "35 0 (l, u): 210 215\n",
      "support: 5\n",
      "35 1 (l, u): 210 215\n",
      "support: 5\n",
      "query; 2\n",
      "36 0 (l, u): 216 221\n",
      "support: 5\n",
      "36 1 (l, u): 216 221\n",
      "support: 5\n",
      "query; 2\n",
      "37 0 (l, u): 222 227\n",
      "support: 5\n",
      "37 1 (l, u): 222 227\n",
      "support: 5\n",
      "query; 2\n",
      "38 0 (l, u): 228 233\n",
      "support: 5\n",
      "38 1 (l, u): 228 233\n",
      "support: 5\n",
      "query; 2\n",
      "39 0 (l, u): 234 239\n",
      "support: 5\n",
      "39 1 (l, u): 234 239\n",
      "support: 5\n",
      "query; 2\n",
      "40 0 (l, u): 240 245\n",
      "support: 5\n",
      "40 1 (l, u): 240 245\n",
      "support: 5\n",
      "query; 2\n",
      "41 0 (l, u): 246 251\n",
      "support: 5\n",
      "41 1 (l, u): 246 251\n",
      "support: 5\n",
      "query; 2\n",
      "42 0 (l, u): 252 257\n",
      "support: 5\n",
      "42 1 (l, u): 252 257\n",
      "support: 5\n",
      "query; 2\n",
      "43 0 (l, u): 258 263\n",
      "support: 5\n",
      "43 1 (l, u): 258 263\n",
      "support: 5\n",
      "query; 2\n",
      "44 0 (l, u): 264 269\n",
      "support: 5\n",
      "44 1 (l, u): 264 269\n",
      "support: 5\n",
      "query; 2\n",
      "45 0 (l, u): 270 275\n",
      "support: 5\n",
      "45 1 (l, u): 270 275\n",
      "support: 5\n",
      "query; 2\n",
      "46 0 (l, u): 276 281\n",
      "support: 5\n",
      "46 1 (l, u): 276 281\n",
      "support: 5\n",
      "query; 2\n",
      "47 0 (l, u): 282 287\n",
      "support: 5\n",
      "47 1 (l, u): 282 287\n",
      "support: 5\n",
      "query; 2\n",
      "48 0 (l, u): 288 293\n",
      "support: 5\n",
      "48 1 (l, u): 288 293\n",
      "support: 5\n",
      "query; 2\n",
      "49 0 (l, u): 294 299\n",
      "support: 5\n",
      "49 1 (l, u): 294 299\n",
      "support: 5\n",
      "query; 2\n",
      "50 0 (l, u): 300 305\n",
      "support: 5\n",
      "50 1 (l, u): 300 305\n",
      "support: 5\n",
      "query; 2\n",
      "51 0 (l, u): 306 311\n",
      "support: 5\n",
      "51 1 (l, u): 306 311\n",
      "support: 5\n",
      "query; 2\n",
      "52 0 (l, u): 312 317\n",
      "support: 5\n",
      "52 1 (l, u): 312 317\n",
      "support: 5\n",
      "query; 2\n",
      "53 0 (l, u): 318 323\n",
      "support: 5\n",
      "53 1 (l, u): 318 323\n",
      "support: 5\n",
      "query; 2\n",
      "54 0 (l, u): 324 329\n",
      "support: 5\n",
      "54 1 (l, u): 324 329\n",
      "support: 5\n",
      "query; 2\n",
      "55 0 (l, u): 330 335\n",
      "support: 5\n",
      "55 1 (l, u): 330 335\n",
      "support: 5\n",
      "query; 2\n",
      "56 0 (l, u): 336 341\n",
      "support: 5\n",
      "56 1 (l, u): 336 341\n",
      "support: 5\n",
      "query; 2\n",
      "57 0 (l, u): 342 347\n",
      "support: 5\n",
      "57 1 (l, u): 342 347\n",
      "support: 5\n",
      "query; 2\n",
      "58 0 (l, u): 348 353\n",
      "support: 5\n",
      "58 1 (l, u): 348 353\n",
      "support: 5\n",
      "query; 2\n",
      "59 0 (l, u): 354 359\n",
      "support: 5\n",
      "59 1 (l, u): 354 359\n",
      "support: 5\n",
      "query; 2\n",
      "60 0 (l, u): 360 365\n",
      "support: 5\n",
      "60 1 (l, u): 360 365\n",
      "support: 5\n",
      "query; 2\n",
      "61 0 (l, u): 366 371\n",
      "support: 5\n",
      "61 1 (l, u): 366 371\n",
      "support: 5\n",
      "query; 2\n",
      "62 0 (l, u): 372 377\n",
      "support: 5\n",
      "62 1 (l, u): 372 377\n",
      "support: 5\n",
      "query; 2\n",
      "63 0 (l, u): 378 383\n",
      "support: 5\n",
      "63 1 (l, u): 378 383\n",
      "support: 5\n",
      "query; 2\n",
      "64 0 (l, u): 384 389\n",
      "support: 5\n",
      "64 1 (l, u): 384 389\n",
      "support: 5\n",
      "query; 2\n",
      "65 0 (l, u): 390 395\n",
      "support: 5\n",
      "65 1 (l, u): 390 395\n",
      "support: 5\n",
      "query; 2\n",
      "66 0 (l, u): 396 401\n",
      "support: 5\n",
      "66 1 (l, u): 396 401\n",
      "support: 5\n",
      "query; 2\n",
      "67 0 (l, u): 402 407\n",
      "support: 5\n",
      "67 1 (l, u): 402 407\n",
      "support: 5\n",
      "query; 2\n",
      "68 0 (l, u): 408 413\n",
      "support: 5\n",
      "68 1 (l, u): 408 413\n",
      "support: 5\n",
      "query; 2\n",
      "69 0 (l, u): 414 419\n",
      "support: 5\n",
      "69 1 (l, u): 414 419\n",
      "support: 5\n",
      "query; 2\n",
      "70 0 (l, u): 420 425\n",
      "support: 5\n",
      "70 1 (l, u): 420 425\n",
      "support: 5\n",
      "query; 2\n",
      "71 0 (l, u): 426 431\n",
      "support: 5\n",
      "71 1 (l, u): 426 431\n",
      "support: 5\n",
      "query; 2\n",
      "72 0 (l, u): 432 437\n",
      "support: 5\n",
      "72 1 (l, u): 432 437\n",
      "support: 5\n",
      "query; 2\n",
      "73 0 (l, u): 438 443\n",
      "support: 5\n",
      "73 1 (l, u): 438 443\n",
      "support: 5\n",
      "query; 2\n",
      "74 0 (l, u): 444 449\n",
      "support: 5\n",
      "74 1 (l, u): 444 449\n",
      "support: 5\n",
      "query; 2\n",
      "75 0 (l, u): 450 455\n",
      "support: 5\n",
      "75 1 (l, u): 450 455\n",
      "support: 5\n",
      "query; 2\n",
      "76 0 (l, u): 456 461\n",
      "support: 5\n",
      "76 1 (l, u): 456 461\n",
      "support: 5\n",
      "query; 2\n",
      "77 0 (l, u): 462 467\n",
      "support: 5\n",
      "77 1 (l, u): 462 467\n",
      "support: 5\n",
      "query; 2\n",
      "78 0 (l, u): 468 473\n",
      "support: 5\n",
      "78 1 (l, u): 468 473\n",
      "support: 5\n",
      "query; 2\n",
      "79 0 (l, u): 474 479\n",
      "support: 5\n",
      "79 1 (l, u): 474 479\n",
      "support: 5\n",
      "query; 2\n",
      "80 0 (l, u): 480 485\n",
      "support: 5\n",
      "80 1 (l, u): 480 485\n",
      "support: 5\n",
      "query; 2\n",
      "81 0 (l, u): 486 491\n",
      "support: 5\n",
      "81 1 (l, u): 486 491\n",
      "support: 5\n",
      "query; 2\n",
      "82 0 (l, u): 492 497\n",
      "support: 5\n",
      "82 1 (l, u): 492 497\n",
      "support: 5\n",
      "query; 2\n",
      "83 0 (l, u): 498 503\n",
      "support: 5\n",
      "83 1 (l, u): 498 503\n",
      "support: 5\n",
      "query; 2\n",
      "84 0 (l, u): 504 509\n",
      "support: 5\n",
      "84 1 (l, u): 504 509\n",
      "support: 5\n",
      "query; 2\n",
      "85 0 (l, u): 510 515\n",
      "support: 5\n",
      "85 1 (l, u): 510 515\n",
      "support: 5\n",
      "query; 2\n",
      "86 0 (l, u): 516 521\n",
      "support: 5\n",
      "86 1 (l, u): 516 521\n",
      "support: 5\n",
      "query; 2\n",
      "87 0 (l, u): 522 527\n",
      "support: 5\n",
      "87 1 (l, u): 522 527\n",
      "support: 5\n",
      "query; 2\n",
      "88 0 (l, u): 528 533\n",
      "support: 5\n",
      "88 1 (l, u): 528 533\n",
      "support: 5\n",
      "query; 2\n",
      "89 0 (l, u): 534 539\n",
      "support: 5\n",
      "89 1 (l, u): 534 539\n",
      "support: 5\n",
      "query; 2\n",
      "90 0 (l, u): 540 545\n",
      "support: 5\n",
      "90 1 (l, u): 540 545\n",
      "support: 5\n",
      "query; 2\n",
      "91 0 (l, u): 546 551\n",
      "support: 5\n",
      "91 1 (l, u): 546 551\n",
      "support: 5\n",
      "query; 2\n",
      "92 0 (l, u): 552 557\n",
      "support: 5\n",
      "92 1 (l, u): 552 557\n",
      "support: 5\n",
      "query; 2\n",
      "93 0 (l, u): 558 563\n",
      "support: 5\n",
      "93 1 (l, u): 558 563\n",
      "support: 5\n",
      "query; 2\n",
      "94 0 (l, u): 564 569\n",
      "support: 5\n",
      "94 1 (l, u): 564 569\n",
      "support: 5\n",
      "query; 2\n",
      "95 0 (l, u): 570 575\n",
      "support: 5\n",
      "95 1 (l, u): 570 575\n",
      "support: 5\n",
      "query; 2\n",
      "96 0 (l, u): 576 581\n",
      "support: 5\n",
      "96 1 (l, u): 576 581\n",
      "support: 5\n",
      "query; 2\n",
      "97 0 (l, u): 582 587\n",
      "support: 5\n",
      "97 1 (l, u): 582 587\n",
      "support: 5\n",
      "query; 2\n",
      "98 0 (l, u): 588 593\n",
      "support: 5\n",
      "98 1 (l, u): 588 593\n",
      "support: 5\n",
      "query; 2\n",
      "99 0 (l, u): 594 599\n",
      "support: 5\n",
      "99 1 (l, u): 594 599\n",
      "support: 5\n",
      "query; 2\n",
      "100 0 (l, u): 600 605\n",
      "support: 5\n",
      "100 1 (l, u): 600 605\n",
      "support: 5\n",
      "query; 2\n",
      "101 0 (l, u): 606 611\n",
      "support: 5\n",
      "101 1 (l, u): 606 611\n",
      "support: 5\n",
      "query; 2\n",
      "102 0 (l, u): 612 617\n",
      "support: 5\n",
      "102 1 (l, u): 612 617\n",
      "support: 5\n",
      "query; 2\n",
      "103 0 (l, u): 618 623\n",
      "support: 5\n",
      "103 1 (l, u): 618 623\n",
      "support: 5\n",
      "query; 2\n",
      "104 0 (l, u): 624 629\n",
      "support: 5\n",
      "104 1 (l, u): 624 629\n",
      "support: 5\n",
      "query; 2\n",
      "105 0 (l, u): 630 635\n",
      "support: 5\n",
      "105 1 (l, u): 630 635\n",
      "support: 5\n",
      "query; 2\n",
      "106 0 (l, u): 636 641\n",
      "support: 5\n",
      "106 1 (l, u): 636 641\n",
      "support: 5\n",
      "query; 2\n",
      "107 0 (l, u): 642 647\n",
      "support: 5\n",
      "107 1 (l, u): 642 647\n",
      "support: 5\n",
      "query; 2\n",
      "108 0 (l, u): 648 653\n",
      "support: 5\n",
      "108 1 (l, u): 648 653\n",
      "support: 5\n",
      "query; 2\n",
      "109 0 (l, u): 654 659\n",
      "support: 5\n",
      "109 1 (l, u): 654 659\n",
      "support: 5\n",
      "query; 2\n",
      "110 0 (l, u): 660 665\n",
      "support: 5\n",
      "110 1 (l, u): 660 665\n",
      "support: 5\n",
      "query; 2\n",
      "111 0 (l, u): 666 671\n",
      "support: 5\n",
      "111 1 (l, u): 666 671\n",
      "support: 5\n",
      "query; 2\n",
      "112 0 (l, u): 672 677\n",
      "support: 5\n",
      "112 1 (l, u): 672 677\n",
      "support: 5\n",
      "query; 2\n",
      "113 0 (l, u): 678 683\n",
      "support: 5\n",
      "113 1 (l, u): 678 683\n",
      "support: 5\n",
      "query; 2\n",
      "114 0 (l, u): 684 689\n",
      "support: 5\n",
      "114 1 (l, u): 684 689\n",
      "support: 5\n",
      "query; 2\n",
      "115 0 (l, u): 690 695\n",
      "support: 5\n",
      "115 1 (l, u): 690 695\n",
      "support: 5\n",
      "query; 2\n",
      "116 0 (l, u): 696 701\n",
      "support: 5\n",
      "116 1 (l, u): 696 701\n",
      "support: 5\n",
      "query; 2\n",
      "117 0 (l, u): 702 707\n",
      "support: 5\n",
      "117 1 (l, u): 702 707\n",
      "support: 5\n",
      "query; 2\n",
      "118 0 (l, u): 708 713\n",
      "support: 5\n",
      "118 1 (l, u): 708 713\n",
      "support: 5\n",
      "query; 2\n",
      "119 0 (l, u): 714 719\n",
      "support: 5\n",
      "119 1 (l, u): 714 719\n",
      "support: 5\n",
      "query; 2\n",
      "indicies [2, 3, 0, 4, 1, 720, 724, 722, 723, 721, 5, 725, 8, 7, 6, 9, 10, 729, 726, 730, 727, 728, 11, 731, 13, 14, 15, 16, 12, 732, 735, 733, 734, 736, 17, 737, 20, 19, 22, 18, 21, 739, 738, 740, 741, 742, 23, 743, 26, 25, 24, 28, 27, 746, 747, 748, 744, 745, 29, 749, 34, 30, 32, 33, 31, 752, 750, 753, 754, 751, 35, 755, 37, 38, 40, 36, 39, 760, 759, 756, 758, 757, 41, 761, 43, 45, 46, 44, 42, 764, 766, 762, 763, 765, 47, 767, 51, 50, 52, 48, 49, 768, 769, 771, 772, 770, 53, 773, 56, 58, 57, 54, 55, 774, 777, 778, 775, 776, 59, 779, 63, 60, 64, 61, 62, 783, 781, 784, 782, 780, 65, 785, 67, 69, 66, 68, 70, 786, 788, 787, 789, 790, 71, 791, 76, 72, 73, 75, 74, 793, 794, 792, 796, 795, 77, 797, 80, 79, 82, 81, 78, 800, 802, 798, 799, 801, 83, 803, 85, 84, 86, 87, 88, 806, 805, 807, 804, 808, 89, 809, 92, 90, 91, 93, 94, 812, 810, 811, 814, 813, 95, 815, 100, 98, 99, 97, 96, 820, 818, 817, 816, 819, 101, 821, 102, 105, 104, 106, 103, 826, 824, 822, 823, 825, 107, 827, 110, 111, 108, 112, 109, 828, 829, 830, 832, 831, 113, 833, 116, 117, 118, 114, 115, 835, 834, 836, 837, 838, 119, 839, 122, 124, 120, 121, 123, 840, 842, 844, 843, 841, 125, 845, 129, 127, 130, 128, 126, 846, 849, 847, 850, 848, 131, 851, 132, 134, 136, 135, 133, 852, 856, 853, 855, 854, 137, 857, 139, 142, 140, 138, 141, 861, 858, 862, 859, 860, 143, 863, 147, 148, 145, 146, 144, 865, 867, 866, 868, 864, 149, 869, 152, 154, 150, 153, 151, 873, 874, 870, 872, 871, 155, 875, 156, 160, 157, 158, 159, 877, 878, 876, 879, 880, 161, 881, 165, 162, 163, 164, 166, 882, 886, 885, 884, 883, 167, 887, 169, 171, 170, 168, 172, 891, 888, 892, 890, 889, 173, 893, 177, 175, 176, 178, 174, 897, 898, 895, 896, 894, 179, 899, 183, 184, 181, 180, 182, 902, 900, 901, 904, 903, 185, 905, 188, 187, 186, 190, 189, 910, 907, 906, 909, 908, 191, 911, 195, 193, 196, 192, 194, 915, 913, 912, 916, 914, 197, 917, 199, 201, 198, 202, 200, 920, 922, 921, 919, 918, 203, 923, 204, 208, 205, 206, 207, 925, 926, 927, 924, 928, 209, 929, 212, 213, 210, 214, 211, 932, 933, 930, 934, 931, 215, 935, 220, 216, 219, 218, 217, 936, 938, 937, 939, 940, 221, 941, 224, 225, 226, 223, 222, 942, 945, 944, 946, 943, 227, 947, 231, 232, 228, 229, 230, 948, 951, 949, 950, 952, 233, 953, 237, 236, 234, 238, 235, 957, 954, 955, 958, 956, 239, 959, 242, 243, 240, 241, 244, 964, 961, 960, 963, 962, 245, 965, 247, 248, 249, 250, 246, 966, 970, 969, 968, 967, 251, 971, 253, 256, 252, 254, 255, 973, 974, 975, 976, 972, 257, 977, 259, 262, 258, 260, 261, 982, 979, 981, 980, 978, 263, 983, 265, 268, 267, 264, 266, 987, 985, 988, 984, 986, 269, 989, 274, 270, 273, 272, 271, 992, 991, 994, 993, 990, 275, 995, 278, 276, 277, 279, 280, 998, 996, 999, 1000, 997, 281, 1001, 282, 284, 285, 286, 283, 1005, 1006, 1002, 1004, 1003, 287, 1007, 288, 292, 291, 290, 289, 1009, 1008, 1010, 1012, 1011, 293, 1013, 294, 297, 296, 295, 298, 1017, 1015, 1016, 1018, 1014, 299, 1019, 303, 304, 300, 302, 301, 1023, 1022, 1024, 1020, 1021, 305, 1025, 309, 308, 310, 307, 306, 1029, 1026, 1030, 1028, 1027, 311, 1031, 312, 313, 315, 314, 316, 1036, 1034, 1032, 1035, 1033, 317, 1037, 321, 322, 318, 319, 320, 1038, 1041, 1039, 1040, 1042, 323, 1043, 324, 325, 327, 326, 328, 1044, 1047, 1045, 1048, 1046, 329, 1049, 334, 330, 331, 333, 332, 1052, 1050, 1054, 1053, 1051, 335, 1055, 340, 336, 338, 339, 337, 1057, 1060, 1058, 1059, 1056, 341, 1061, 343, 342, 344, 345, 346, 1063, 1062, 1064, 1065, 1066, 347, 1067, 350, 351, 352, 348, 349, 1069, 1071, 1072, 1070, 1068, 353, 1073, 357, 356, 358, 354, 355, 1074, 1078, 1076, 1075, 1077, 359, 1079, 360, 364, 363, 362, 361, 1080, 1082, 1084, 1081, 1083, 365, 1085, 366, 367, 370, 368, 369, 1089, 1090, 1088, 1087, 1086, 371, 1091, 375, 376, 372, 374, 373, 1092, 1095, 1093, 1094, 1096, 377, 1097, 378, 380, 379, 382, 381, 1099, 1101, 1098, 1100, 1102, 383, 1103, 388, 385, 384, 387, 386, 1107, 1105, 1108, 1104, 1106, 389, 1109, 394, 392, 393, 391, 390, 1110, 1112, 1113, 1111, 1114, 395, 1115, 396, 400, 397, 399, 398, 1119, 1118, 1120, 1117, 1116, 401, 1121, 402, 405, 403, 406, 404, 1124, 1123, 1126, 1122, 1125, 407, 1127, 412, 408, 411, 409, 410, 1128, 1130, 1132, 1131, 1129, 413, 1133, 418, 416, 417, 415, 414, 1138, 1134, 1136, 1137, 1135, 419, 1139, 422, 423, 420, 424, 421, 1142, 1140, 1143, 1141, 1144, 425, 1145, 430, 428, 429, 426, 427, 1146, 1149, 1147, 1150, 1148, 431, 1151, 436, 432, 434, 433, 435, 1153, 1155, 1156, 1152, 1154, 437, 1157, 439, 441, 442, 440, 438, 1162, 1160, 1161, 1158, 1159, 443, 1163, 444, 446, 445, 448, 447, 1164, 1167, 1166, 1168, 1165, 449, 1169, 451, 454, 450, 452, 453, 1170, 1173, 1174, 1172, 1171, 455, 1175, 458, 457, 460, 459, 456, 1176, 1177, 1180, 1178, 1179, 461, 1181, 465, 463, 464, 462, 466, 1184, 1183, 1186, 1182, 1185, 467, 1187, 472, 471, 468, 469, 470, 1190, 1192, 1191, 1189, 1188, 473, 1193, 478, 474, 476, 475, 477, 1197, 1195, 1194, 1198, 1196, 479, 1199, 481, 482, 480, 483, 484, 1203, 1200, 1202, 1201, 1204, 485, 1205, 490, 488, 486, 489, 487, 1208, 1207, 1210, 1209, 1206, 491, 1211, 495, 494, 493, 492, 496, 1214, 1213, 1216, 1215, 1212, 497, 1217, 501, 498, 499, 500, 502, 1221, 1218, 1222, 1220, 1219, 503, 1223, 507, 504, 505, 508, 506, 1228, 1227, 1224, 1225, 1226, 509, 1229, 514, 513, 512, 510, 511, 1233, 1230, 1234, 1232, 1231, 515, 1235, 519, 516, 518, 517, 520, 1240, 1239, 1238, 1237, 1236, 521, 1241, 522, 523, 526, 524, 525, 1242, 1243, 1244, 1245, 1246, 527, 1247, 529, 528, 532, 531, 530, 1252, 1248, 1251, 1250, 1249, 533, 1253, 534, 535, 537, 538, 536, 1254, 1256, 1257, 1258, 1255, 539, 1259, 543, 542, 541, 544, 540, 1261, 1263, 1264, 1262, 1260, 545, 1265, 550, 548, 546, 547, 549, 1266, 1270, 1269, 1268, 1267, 551, 1271, 554, 553, 556, 552, 555, 1274, 1276, 1273, 1272, 1275, 557, 1277, 561, 562, 560, 559, 558, 1278, 1279, 1282, 1281, 1280, 563, 1283, 568, 564, 565, 567, 566, 1284, 1285, 1287, 1286, 1288, 569, 1289, 571, 572, 574, 570, 573, 1294, 1290, 1292, 1291, 1293, 575, 1295, 576, 578, 577, 580, 579, 1298, 1299, 1300, 1296, 1297, 581, 1301, 584, 582, 586, 585, 583, 1302, 1303, 1304, 1305, 1306, 587, 1307, 590, 591, 588, 592, 589, 1312, 1309, 1311, 1308, 1310, 593, 1313, 594, 595, 596, 598, 597, 1316, 1317, 1315, 1318, 1314, 599, 1319, 600, 603, 602, 601, 604, 1323, 1320, 1322, 1324, 1321, 605, 1325, 607, 609, 610, 606, 608, 1326, 1328, 1330, 1327, 1329, 611, 1331, 613, 615, 612, 616, 614, 1332, 1333, 1336, 1334, 1335, 617, 1337, 618, 619, 622, 621, 620, 1339, 1340, 1341, 1338, 1342, 623, 1343, 627, 628, 624, 625, 626, 1344, 1346, 1347, 1345, 1348, 629, 1349, 633, 630, 632, 631, 634, 1350, 1351, 1352, 1354, 1353, 635, 1355, 640, 638, 637, 639, 636, 1360, 1356, 1357, 1359, 1358, 641, 1361, 645, 643, 644, 642, 646, 1362, 1363, 1366, 1365, 1364, 647, 1367, 650, 649, 651, 652, 648, 1370, 1368, 1372, 1369, 1371, 653, 1373, 656, 658, 657, 655, 654, 1377, 1374, 1378, 1376, 1375, 659, 1379, 662, 660, 661, 664, 663, 1384, 1381, 1383, 1380, 1382, 665, 1385, 670, 667, 668, 666, 669, 1388, 1389, 1390, 1387, 1386, 671, 1391, 673, 674, 676, 675, 672, 1396, 1392, 1395, 1394, 1393, 677, 1397, 679, 682, 680, 678, 681, 1402, 1401, 1398, 1399, 1400, 683, 1403, 684, 686, 687, 685, 688, 1407, 1405, 1404, 1408, 1406, 689, 1409, 690, 691, 693, 692, 694, 1412, 1413, 1414, 1411, 1410, 695, 1415, 699, 698, 697, 696, 700, 1417, 1416, 1420, 1418, 1419, 701, 1421, 706, 704, 703, 702, 705, 1422, 1424, 1426, 1425, 1423, 707, 1427, 709, 710, 711, 712, 708, 1430, 1428, 1432, 1429, 1431, 713, 1433, 716, 717, 715, 714, 718, 1438, 1437, 1435, 1434, 1436, 719, 1439]\n",
      "len indices 1440\n",
      "tensor([[[0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 1]]])\n"
     ]
    }
   ],
   "source": [
    "n_way = 2\n",
    "n_shots = 5\n",
    "ds = torchvision.datasets.ImageFolder(os.path.join(FEW_SHOT_PATH, \"test\"), transform=transforms.ToTensor())\n",
    "print(len(ds))\n",
    "sampler = FewShotSampler(ds, n_shots)\n",
    "loader = DataLoader(ds, sampler=sampler, batch_size=(n_way * n_shots + n_way))\n",
    "\n",
    "for samples, labels in loader:\n",
    "    support = samples[:n_way * n_shots]\n",
    "    support_labels = labels[:n_way * n_shots]\n",
    "    support_labels = support_labels.view(1, n_way, n_shots)\n",
    "    print(support_labels)\n",
    "    break\n",
    "    # print(samples.shape, label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ship', 'truck']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[-0.0343, -0.0505, -0.0505,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.0181, -0.0505, -0.0505,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.0019, -0.0181, -0.0343,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.7472, -0.6824, -0.0505,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.6986, -0.6014, -0.6662,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.1246,  0.1081,  0.1081,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.1739,  0.1410,  0.1410,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.2233,  0.1904,  0.1739,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.7642, -0.6983, -0.0071,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.5996, -0.5008, -0.5831,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[-0.6785, -0.6937, -0.6937,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.6634, -0.6937, -0.7089,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.6634, -0.6937, -0.7089,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.7695, -0.6482, -0.0268,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.6331, -0.4967, -0.5270,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.1763,  0.1601,  0.1277,  ...,  0.1601,  0.1601,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0305, -0.0019,  0.1277,  ...,  0.0467,  0.0305,  0.0000],\n",
       "           [ 0.0143, -0.0343,  0.1439,  ...,  0.0629,  0.0305,  0.0000],\n",
       "           [-0.0019, -0.0505,  0.1763,  ...,  0.0467,  0.0305,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.0236, -0.0400, -0.0729,  ..., -0.0236, -0.0236,  0.0000],\n",
       "           ...,\n",
       "           [-0.0894, -0.1058, -0.0565,  ..., -0.0071, -0.0071,  0.0000],\n",
       "           [-0.1058, -0.1058, -0.0565,  ..., -0.0071, -0.0071,  0.0000],\n",
       "           [-0.1223, -0.1223, -0.0236,  ..., -0.0071, -0.0071,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.4512, -0.4664, -0.4967,  ..., -0.5270, -0.5270,  0.0000],\n",
       "           ...,\n",
       "           [-0.6179, -0.5118, -0.3148,  ..., -0.5573, -0.5573,  0.0000],\n",
       "           [-0.6331, -0.5270, -0.3148,  ..., -0.5421, -0.5573,  0.0000],\n",
       "           [-0.6482, -0.5421, -0.2845,  ..., -0.5421, -0.5573,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.8081,  0.7919,  0.8081,  ...,  0.0305,  0.0467,  0.1115],\n",
       "           [ 0.8405,  0.8243,  0.8567,  ...,  0.1115,  0.1601,  0.2411],\n",
       "           ...,\n",
       "           [-0.0667, -0.0667, -0.2611,  ...,  1.3104,  1.6182,  1.3752],\n",
       "           [-0.0505, -0.0505, -0.1477,  ...,  1.3266,  1.5048,  1.4562],\n",
       "           [-0.0343, -0.0019, -0.0343,  ...,  1.2942,  1.3428,  1.4238]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.4537,  0.4373,  0.4702,  ..., -0.2540, -0.2540, -0.1881],\n",
       "           [ 0.4866,  0.4702,  0.5031,  ..., -0.1881, -0.1388, -0.0729],\n",
       "           ...,\n",
       "           [-0.4350, -0.4185, -0.4844,  ...,  0.9968,  1.2766,  1.0297],\n",
       "           [-0.4021, -0.4350, -0.4185,  ...,  1.0297,  1.1943,  1.1450],\n",
       "           [-0.3692, -0.4185, -0.3856,  ...,  1.0462,  1.0791,  1.1450]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.1853,  0.1702,  0.2005,  ..., -0.4512, -0.4815, -0.4209],\n",
       "           [ 0.2157,  0.2005,  0.2308,  ..., -0.3906, -0.3754, -0.3148],\n",
       "           ...,\n",
       "           [-0.4209, -0.4057, -0.5118,  ...,  0.7764,  0.9583,  0.7158],\n",
       "           [-0.4057, -0.4057, -0.4360,  ...,  0.8522,  0.8825,  0.8370],\n",
       "           [-0.3754, -0.3906, -0.3754,  ...,  0.8674,  0.8067,  0.8674]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.1801, -0.2773, -0.6014,  ..., -0.8444, -1.3142,  0.0000],\n",
       "           [-0.1801, -0.6662, -0.9416,  ..., -0.5527, -1.2494,  0.0000],\n",
       "           [ 0.5327, -0.5689, -1.1198,  ..., -0.6662,  0.1115,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.2046, -0.3362, -0.6819,  ..., -0.9123, -1.3895,  0.0000],\n",
       "           [-0.1388, -0.7312, -1.0275,  ..., -0.7312, -1.1921,  0.0000],\n",
       "           [ 0.6018, -0.6160, -1.1427,  ..., -0.8300, -0.1717,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.1853, -0.0117, -0.3451,  ..., -0.6482, -1.1029,  0.0000],\n",
       "           [ 0.1702, -0.4209, -0.6785,  ..., -0.4664, -0.9059,  0.0000],\n",
       "           [ 0.7764, -0.2996, -0.8453,  ..., -0.5573,  0.0186,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.2773, -0.2935, -0.2125,  ..., -1.1360,  0.0000,  0.0000],\n",
       "           [-0.2935, -0.3907, -0.5689,  ..., -1.1522,  0.0000,  0.0000],\n",
       "           [-0.5689, -0.5041, -0.7148,  ..., -1.0874,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0258,  0.0094,  0.0587,  ..., -1.0275,  0.0000,  0.0000],\n",
       "           [-0.0236, -0.0894, -0.2704,  ..., -1.0439,  0.0000,  0.0000],\n",
       "           [-0.2704, -0.2210, -0.4021,  ..., -0.9781,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.4360, -0.5118, -0.3603,  ..., -0.8301,  0.0000,  0.0000],\n",
       "           [-0.4057, -0.5118, -0.6785,  ..., -0.8604,  0.0000,  0.0000],\n",
       "           [-0.6482, -0.5725, -0.7392,  ..., -0.8301,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-1.1846, -0.8282, -0.9740,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.6824, -0.3097, -1.4600,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-1.0226, -0.9578, -1.3142,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-1.3731, -1.0933, -1.1756,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.8958, -0.5337, -1.5541,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-1.3402, -1.2085, -1.5048,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-1.2242, -1.0574, -1.0878,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.9059, -0.6179, -1.3454,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-1.1938, -1.1181, -1.3151,  ...,  0.0000,  0.0000,  0.0000]]]]),\n",
       " tensor([0, 0, 0,  ..., 3, 4, 5])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(1)\n",
    "ds_train, ds_valid, ds_test = get_cinic_few(FEW_SHOT_PATH, 32, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "samples, labels = next(iter(ds_train))\n",
    "print(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can = CrossAttentionNetwork().cuda()\n",
    "criterion = CANLoss(can, n_classes=M).cuda()\n",
    "optimizer = SGD(CANLoss.parameters())\n",
    "\n",
    "for"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

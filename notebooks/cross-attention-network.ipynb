{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Project 1\n",
    "Wojciech Kutak\n",
    "\n",
    "---\n",
    "\n",
    "### Cross Attention Network for few-shot learning problem\n",
    "#### 1. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "DATA_PATH = os.path.join(\"..\", \"data\")\n",
    "FEW_SHOT_PATH = os.path.join(\"..\", \"data_few_shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsampler = None):\n",
    "        super().__init__()\n",
    "        self.downsampler = downsampler\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsampler is not None:\n",
    "            identity = self.downsampler(identity)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet_32x32(nn.Module):\n",
    "    def __init__(self, block: nn.Module = ResidualBlock):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 64, (3, 3), stride=1, padding=1)\n",
    "        self.bnorm = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.res_layer1 = self._residual_layer(block, 64, 128, 2)\n",
    "\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.res_layer2 = self._residual_layer(block, 128, 256, 2)\n",
    "\n",
    "        self.max_pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.res_layer3 = self._residual_layer(block, 256, 512, 2)\n",
    "\n",
    "        self.output_shape = (512, 3, 3)\n",
    "\n",
    "\n",
    "    def _residual_layer(self, block, in_channels, out_channels, blocks_num, stride=1):\n",
    "        \"\"\"Creates a residual layer consisting out of residual blocks\"\"\"\n",
    "        downsampler = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsampler = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(in_channels, out_channels, stride, downsampler))\n",
    "        for _ in range(blocks_num - 1):\n",
    "            layers.append(block(out_channels, out_channels, stride))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bnorm(self.conv(x)))\n",
    "\n",
    "        x = self.res_layer1(self.max_pool1(x))\n",
    "        x = self.res_layer2(self.max_pool2(x))\n",
    "        x = self.res_layer3(self.max_pool3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet_32x32                             [30, 512, 3, 3]           --\n",
       "├─Conv2d: 1-1                            [30, 64, 32, 32]          1,792\n",
       "├─BatchNorm2d: 1-2                       [30, 64, 32, 32]          128\n",
       "├─ReLU: 1-3                              [30, 64, 32, 32]          --\n",
       "├─MaxPool2d: 1-4                         [30, 64, 15, 15]          --\n",
       "├─Sequential: 1-5                        [30, 128, 15, 15]         --\n",
       "│    └─ResidualBlock: 2-1                [30, 128, 15, 15]         --\n",
       "│    │    └─Sequential: 3-1              [30, 128, 15, 15]         8,448\n",
       "│    │    └─Conv2d: 3-2                  [30, 128, 15, 15]         73,856\n",
       "│    │    └─BatchNorm2d: 3-3             [30, 128, 15, 15]         256\n",
       "│    │    └─ReLU: 3-4                    [30, 128, 15, 15]         --\n",
       "│    │    └─Conv2d: 3-5                  [30, 128, 15, 15]         147,584\n",
       "│    │    └─BatchNorm2d: 3-6             [30, 128, 15, 15]         256\n",
       "│    │    └─ReLU: 3-7                    [30, 128, 15, 15]         --\n",
       "│    └─ResidualBlock: 2-2                [30, 128, 15, 15]         --\n",
       "│    │    └─Conv2d: 3-8                  [30, 128, 15, 15]         147,584\n",
       "│    │    └─BatchNorm2d: 3-9             [30, 128, 15, 15]         256\n",
       "│    │    └─ReLU: 3-10                   [30, 128, 15, 15]         --\n",
       "│    │    └─Conv2d: 3-11                 [30, 128, 15, 15]         147,584\n",
       "│    │    └─BatchNorm2d: 3-12            [30, 128, 15, 15]         256\n",
       "│    │    └─ReLU: 3-13                   [30, 128, 15, 15]         --\n",
       "├─MaxPool2d: 1-6                         [30, 128, 7, 7]           --\n",
       "├─Sequential: 1-7                        [30, 256, 7, 7]           --\n",
       "│    └─ResidualBlock: 2-3                [30, 256, 7, 7]           --\n",
       "│    │    └─Sequential: 3-14             [30, 256, 7, 7]           33,280\n",
       "│    │    └─Conv2d: 3-15                 [30, 256, 7, 7]           295,168\n",
       "│    │    └─BatchNorm2d: 3-16            [30, 256, 7, 7]           512\n",
       "│    │    └─ReLU: 3-17                   [30, 256, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-18                 [30, 256, 7, 7]           590,080\n",
       "│    │    └─BatchNorm2d: 3-19            [30, 256, 7, 7]           512\n",
       "│    │    └─ReLU: 3-20                   [30, 256, 7, 7]           --\n",
       "│    └─ResidualBlock: 2-4                [30, 256, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-21                 [30, 256, 7, 7]           590,080\n",
       "│    │    └─BatchNorm2d: 3-22            [30, 256, 7, 7]           512\n",
       "│    │    └─ReLU: 3-23                   [30, 256, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-24                 [30, 256, 7, 7]           590,080\n",
       "│    │    └─BatchNorm2d: 3-25            [30, 256, 7, 7]           512\n",
       "│    │    └─ReLU: 3-26                   [30, 256, 7, 7]           --\n",
       "├─MaxPool2d: 1-8                         [30, 256, 3, 3]           --\n",
       "├─Sequential: 1-9                        [30, 512, 3, 3]           --\n",
       "│    └─ResidualBlock: 2-5                [30, 512, 3, 3]           --\n",
       "│    │    └─Sequential: 3-27             [30, 512, 3, 3]           132,096\n",
       "│    │    └─Conv2d: 3-28                 [30, 512, 3, 3]           1,180,160\n",
       "│    │    └─BatchNorm2d: 3-29            [30, 512, 3, 3]           1,024\n",
       "│    │    └─ReLU: 3-30                   [30, 512, 3, 3]           --\n",
       "│    │    └─Conv2d: 3-31                 [30, 512, 3, 3]           2,359,808\n",
       "│    │    └─BatchNorm2d: 3-32            [30, 512, 3, 3]           1,024\n",
       "│    │    └─ReLU: 3-33                   [30, 512, 3, 3]           --\n",
       "│    └─ResidualBlock: 2-6                [30, 512, 3, 3]           --\n",
       "│    │    └─Conv2d: 3-34                 [30, 512, 3, 3]           2,359,808\n",
       "│    │    └─BatchNorm2d: 3-35            [30, 512, 3, 3]           1,024\n",
       "│    │    └─ReLU: 3-36                   [30, 512, 3, 3]           --\n",
       "│    │    └─Conv2d: 3-37                 [30, 512, 3, 3]           2,359,808\n",
       "│    │    └─BatchNorm2d: 3-38            [30, 512, 3, 3]           1,024\n",
       "│    │    └─ReLU: 3-39                   [30, 512, 3, 3]           --\n",
       "==========================================================================================\n",
       "Total params: 11,024,512\n",
       "Trainable params: 11,024,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 8.95\n",
       "==========================================================================================\n",
       "Input size (MB): 0.37\n",
       "Forward/backward pass size (MB): 141.74\n",
       "Params size (MB): 44.10\n",
       "Estimated Total Size (MB): 186.21\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(ResNet_32x32(), (30, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class FusionLayer(nn.Module):\n",
    "    def __init__(self, m: int, bottleneck_size: int):\n",
    "        super(FusionLayer, self).__init__()\n",
    "\n",
    "        self.temperature = 1.0\n",
    "        self.m = m\n",
    "        self.bottleneck_size = bottleneck_size\n",
    "        self.conv1 = nn.Conv2d(self.m, self.bottleneck_size, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(self.bottleneck_size)\n",
    "        self.conv2 = nn.Conv2d(self.bottleneck_size, self.m, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # TODO:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "\n",
    "\n",
    "    def forward(self, R: torch.Tensor):\n",
    "        # print(\"\\nFusionLayer forward\")\n",
    "        # print(\"R:\", R.shape)\n",
    "\n",
    "        # If R = R_p\n",
    "        # Dim=3 corresponds to all queries and dim=4 to all support examples\n",
    "        # We are averaging over all queries to extract info about the mean relevance of\n",
    "        # certain pixel from support to all other queries\n",
    "        # Otherwise it's the other way around\n",
    "        w = torch.mean(R, dim=3)\n",
    "        # print(\"w\", w.shape)\n",
    "        w = w.transpose(1, 3)\n",
    "        # print(\"w\", w.shape)\n",
    "\n",
    "        w = self.conv1(w)\n",
    "        # print(\"w\", w.shape)\n",
    "        w = self.bn(w)\n",
    "        w = self.relu(w)\n",
    "        w = self.conv2(w)\n",
    "        # print(\"w\", w.shape)\n",
    "        w = w.transpose(1, 3).unsqueeze(3)\n",
    "        # print(\"w\", w.shape)\n",
    "\n",
    "        # I guess... Unfortunately not well described in paper\n",
    "        # I suppose it's a weighted average over query features\n",
    "        A = torch.mean(w * R, dim=-1)\n",
    "        A = F.softmax(A / self.temperature, dim=-1) + 1\n",
    "\n",
    "        # print(\"A:\", A.shape)\n",
    "        return A\n",
    "\n",
    "        # print(\"R shape:\", R.shape)\n",
    "\n",
    "        # b, M, m1, m2 = R.shape\n",
    "        # assert m1 == m2\n",
    "\n",
    "        # # Global average pooling\n",
    "        # w: torch.Tensor = R.mean(dim=-2)\n",
    "        # print(\"spatial w:\", w.shape)\n",
    "        # w = w.transpose(1, 3)\n",
    "        # print(\"spatial w after:\", w.shape)\n",
    "\n",
    "        # # w = w.unsqueeze(-2)\n",
    "        # # Meta learner\n",
    "        # w = self.conv1(w)\n",
    "        # w = self.bn(w)\n",
    "        # print(\"conv1 w:\", w.shape)\n",
    "        # w = self.relu(w)\n",
    "        # print(\"relu w:\",w.shape)\n",
    "        # w = self.conv2(w)\n",
    "        # print(\"conv2 w:\",w.shape)\n",
    "\n",
    "        # w = w.transpose(1, 3)\n",
    "        # print(\"w tra\", w.shape)\n",
    "        # w = w.squeeze((-2, -1))\n",
    "        # print(\"w squeeze\", w.shape)\n",
    "\n",
    "        # # Convolution operation\n",
    "        # # w is now a vector of average class\n",
    "        # A = self.attention(w, R)\n",
    "        # print(\"Attention:\", A.shape)\n",
    "\n",
    "\n",
    "        # return A\n",
    "\n",
    "\n",
    "    # def attention(self, weights: torch.Tensor, R: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    #     weights_t = weights.transpose(-3, -1).squeeze(1)\n",
    "\n",
    "    #     b, M, m, h, w = R.shape\n",
    "    #     R = R.view(b * M, m, h*w)\n",
    "    #     # print(\"weights:\", weights.shape)\n",
    "    #     # print(\"weights_t:\", weights_t.shape)\n",
    "    #     # print(\"R:\", R.shape)\n",
    "\n",
    "    #     R_mean = torch.bmm(weights_t, R) / self.temperature\n",
    "    #     # print(\"R_mean:\", R_mean.shape)\n",
    "    #     R_mean = R_mean.view(b, M, 1, m)\n",
    "    #     A = F.softmax(R_mean, dim=-1)\n",
    "    #     return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FusionLayer forward\n",
      "R shape: torch.Size([3, 5, 25, 5, 5])\n",
      "spatial w: torch.Size([3, 5, 25, 1, 1])\n",
      "spatial w after: torch.Size([15, 25, 1, 1])\n",
      "conv1 w: torch.Size([15, 15, 1, 1])\n",
      "relu w: torch.Size([15, 15, 1, 1])\n",
      "conv2 w: torch.Size([15, 25, 1, 1])\n",
      "weights: torch.Size([15, 25, 1, 1])\n",
      "weights_t: torch.Size([15, 1, 25])\n",
      "R: torch.Size([15, 25, 25])\n",
      "R_mean: torch.Size([15, 1, 25])\n",
      "Attention: torch.Size([3, 5, 1, 25])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FusionLayer                              [3, 5, 1, 25]             --\n",
       "├─AdaptiveAvgPool2d: 1-1                 [3, 5, 25, 1, 1]          --\n",
       "├─Conv2d: 1-2                            [15, 15, 1, 1]            390\n",
       "├─ReLU: 1-3                              [15, 15, 1, 1]            --\n",
       "├─Conv2d: 1-4                            [15, 25, 1, 1]            400\n",
       "==========================================================================================\n",
       "Total params: 790\n",
       "Trainable params: 790\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.04\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width, height = 5, 5\n",
    "m = width*height\n",
    "M = 5\n",
    "batch_size = 3\n",
    "summary(FusionLayer(m=m, bottleneck_size=15), (batch_size, M, m, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  2.,   4.],\n",
       "          [  3.,   6.],\n",
       "          [  4.,   8.]],\n",
       "\n",
       "         [[ 10.,  20.],\n",
       "          [ 11.,  22.],\n",
       "          [112., 224.]]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 2.0]).view(1, 2)\n",
    "b = torch.tensor([[2.0, 3.0, 4.0], [10.0, 11.0, 112.0]]).view(1, 2, 3, 1)\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionModule(nn.Module):\n",
    "    def __init__(self, input_shape: tuple[int,int,int]):\n",
    "        super(CrossAttentionModule, self).__init__()\n",
    "\n",
    "        # print(\"input shape\", *input_shape)\n",
    "        c, h, w = input_shape\n",
    "        self.fusion_layer = FusionLayer(m=h*w, bottleneck_size=int(h*w*2/3))\n",
    "\n",
    "\n",
    "    def forward(self, P: torch.Tensor, Q: torch.Tensor):\n",
    "        # print(\"\\nCross Attention forward\")\n",
    "        b, n_p, c, h, w = P.shape\n",
    "        n_q = Q.shape[1]\n",
    "        # print(\"P\", P.shape)\n",
    "        # print(\"Q\", Q.shape)\n",
    "\n",
    "        P = P.view(b, n_p, c, h*w)\n",
    "        Q = Q.view(b, n_q, c, h*w)\n",
    "        # print(\"P\", P.shape)\n",
    "        # print(\"Q\", Q.shape)\n",
    "\n",
    "        P_norm = F.normalize(P, p=2, dim=2)\n",
    "        Q_norm = F.normalize(Q, p=2, dim=2)\n",
    "        # print(\"P_norm\", P_norm.shape)\n",
    "        # print(\"Q_norm\", Q_norm.shape)\n",
    "\n",
    "        P_norm = P.transpose(2, 3).view(b, n_p, 1, h*w, c)\n",
    "        Q_norm = Q.view(b, 1, n_q, c, h*w)\n",
    "        # print(\"P_norm\", P_norm.shape)\n",
    "        # print(\"Q_norm\", Q_norm.shape)\n",
    "\n",
    "        # Now it will be broadcasted so all pairs of per batch will be miltiplied\n",
    "        R_p = torch.matmul(P_norm, Q_norm)\n",
    "        R_q = R_p.transpose(3, 4)\n",
    "        # print(\"R_p\", R_p.shape)\n",
    "        # print(\"R_q\", R_q.shape)\n",
    "\n",
    "        A_p: torch.Tensor = self.fusion_layer(R_p)\n",
    "        A_q: torch.Tensor = self.fusion_layer(R_q)\n",
    "\n",
    "        # print(\"P\", P.shape, \"A_p\", A_p.shape)\n",
    "        P = P.unsqueeze(2)\n",
    "        A_p = A_p.unsqueeze(3)\n",
    "        # print(\"P\", P.shape, \"A_p\", A_p.shape)\n",
    "        P = P * A_p\n",
    "        # print(\"P\", P.shape)\n",
    "        P = P.view(b, n_p, n_q, c, h, w)\n",
    "        # print(\"P\", P.shape)\n",
    "\n",
    "        # print(\"Q\", Q.shape, \"A_q\", A_q.shape)\n",
    "        Q = Q.unsqueeze(1)\n",
    "        A_q = A_q.unsqueeze(3)\n",
    "        Q = Q * A_q\n",
    "        Q = Q.view(b, n_p, n_q, c, h, w)\n",
    "        # print(\"Q\", Q.shape)\n",
    "\n",
    "        return P.transpose(1, 2), Q.transpose(1, 2)\n",
    "\n",
    "    # def forward(self, S: torch.Tensor):\n",
    "        # P, Q = S[0], S[1, :, 0, :, :, :]\n",
    "        # print(\"CrossAttentionModule forward\")\n",
    "        # print(P.shape, Q.shape)\n",
    "        # assert P.shape == Q.shape\n",
    "        # b, M, c, h, w = P.shape\n",
    "        # assert (b, c, h, w) == Q.shape\n",
    "        # m = h*w\n",
    "        # # Change representation from tensor c*h*w to c*m (2 dims)\n",
    "        # P = P.view(b, M, c, m)\n",
    "        # Q = Q.view(b, 1, c, m)\n",
    "\n",
    "        # P_norm = F.normalize(P, p=2, dim=2)\n",
    "        # Q_norm = F.normalize(Q, p=2, dim=2)\n",
    "        # print(\"P norm:\", P_norm.shape)\n",
    "        # P_norm_t = P_norm.transpose(-2, -1).unsqueeze(2)\n",
    "        # Q_norm = Q_norm.unsqueeze(1)\n",
    "        # print(\"P norm t:\", P_norm_t.shape)\n",
    "        # print(\"Q norm:\", Q_norm.shape)\n",
    "        # R_q = torch.matmul(P_norm_t, Q_norm)\n",
    "        # print(\"R_q:\", R_q.shape)\n",
    "        # R_p = R_q.transpose(-2, -1)\n",
    "        # R_q = R_q\n",
    "        # print(\"R_q\", R_q.shape)\n",
    "        # print(\"R_p\", R_p.shape)\n",
    "\n",
    "\n",
    "\n",
    "        # A_p: torch.Tensor = self.fusion_layer(R_p)\n",
    "        # print()\n",
    "        # A_q: torch.Tensor = self.fusion_layer(R_q)\n",
    "        # print()\n",
    "\n",
    "        # print(\"P:\", P.shape, \"A_p:\", A_p.shape)\n",
    "        # P_feat = torch.mul(P, A_p) + P\n",
    "        # print(\"P_feat:\", P_feat.shape)\n",
    "\n",
    "        # print(\"Q:\", Q.shape, \"A_q:\", A_q.shape)\n",
    "        # Q = Q.expand(b, M, c, m)\n",
    "        # print(\"Q exp:\", Q.shape, \"A_q:\", A_q.shape)\n",
    "        # Q_feat = torch.mul(Q, A_q) + Q\n",
    "        # print(\"Q_feat:\", Q_feat.shape)\n",
    "        # return P_feat, Q_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P norm: torch.Size([6, 5, 512, 25])\n",
      "P norm t: torch.Size([6, 5, 1, 25, 512])\n",
      "Q norm: torch.Size([6, 1, 1, 512, 25])\n",
      "R_q: torch.Size([6, 5, 1, 25, 25])\n",
      "R_q torch.Size([6, 5, 1, 25, 25])\n",
      "R_p torch.Size([6, 5, 1, 25, 25])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m support \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(M, K, c, height, width)\n\u001b[0;32m      9\u001b[0m query \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(M, c, height, width)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mcan\u001b[49m\u001b[43m(\u001b[49m\u001b[43msupport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Dev\\DeepLearning\\CINIC10_CNN_Classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Dev\\DeepLearning\\CINIC10_CNN_Classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[64], line 39\u001b[0m, in \u001b[0;36mCrossAttentionModule.forward\u001b[1;34m(self, P, Q)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR_q\u001b[39m\u001b[38;5;124m\"\u001b[39m, R_q\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR_p\u001b[39m\u001b[38;5;124m\"\u001b[39m, R_p\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 39\u001b[0m A_p: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfusion_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     41\u001b[0m A_q: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfusion_layer(R_q)\n",
      "File \u001b[1;32md:\\Dev\\DeepLearning\\CINIC10_CNN_Classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Dev\\DeepLearning\\CINIC10_CNN_Classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[70], line 25\u001b[0m, in \u001b[0;36mFusionLayer.forward\u001b[1;34m(self, R)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, R: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# print(\"FusionLayer forward\")\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# print(\"R shape:\", R.shape)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     b, M, m1, m2 \u001b[38;5;241m=\u001b[39m R\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m m1 \u001b[38;5;241m==\u001b[39m m2\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "b = 1\n",
    "M = 6\n",
    "K = 5\n",
    "c = 512\n",
    "height, width = 5, 5\n",
    "can = CrossAttentionModule((c, height, width))\n",
    "\n",
    "support = torch.ones(M, K, c, height, width)\n",
    "query = torch.ones(M, c, height, width)\n",
    "can(support, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossAttentionModule forward\n",
      "torch.Size([3, 5, 512, 5, 5]) torch.Size([3, 512, 5, 5])\n",
      "P norm: torch.Size([3, 5, 512, 25])\n",
      "P norm t: torch.Size([3, 5, 25, 512])\n",
      "Q norm: torch.Size([3, 1, 512, 25])\n",
      "R_q torch.Size([3, 5, 25, 5, 5])\n",
      "R_p torch.Size([3, 5, 25, 5, 5])\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([3, 5, 25, 5, 5])\n",
      "spatial w: torch.Size([3, 5, 25, 1, 1])\n",
      "spatial w after: torch.Size([15, 25, 1, 1])\n",
      "conv1 w: torch.Size([15, 15, 1, 1])\n",
      "relu w: torch.Size([15, 15, 1, 1])\n",
      "conv2 w: torch.Size([15, 25, 1, 1])\n",
      "weights: torch.Size([15, 25, 1, 1])\n",
      "weights_t: torch.Size([15, 1, 25])\n",
      "R: torch.Size([15, 25, 25])\n",
      "R_mean: torch.Size([15, 1, 25])\n",
      "Attention: torch.Size([3, 5, 1, 25])\n",
      "\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([3, 5, 25, 5, 5])\n",
      "spatial w: torch.Size([3, 5, 25, 1, 1])\n",
      "spatial w after: torch.Size([15, 25, 1, 1])\n",
      "conv1 w: torch.Size([15, 15, 1, 1])\n",
      "relu w: torch.Size([15, 15, 1, 1])\n",
      "conv2 w: torch.Size([15, 25, 1, 1])\n",
      "weights: torch.Size([15, 25, 1, 1])\n",
      "weights_t: torch.Size([15, 1, 25])\n",
      "R: torch.Size([15, 25, 25])\n",
      "R_mean: torch.Size([15, 1, 25])\n",
      "Attention: torch.Size([3, 5, 1, 25])\n",
      "\n",
      "P: torch.Size([3, 5, 512, 25]) A_p: torch.Size([3, 5, 1, 25])\n",
      "P_feat: torch.Size([3, 5, 512, 25])\n",
      "Q: torch.Size([3, 1, 512, 25]) A_q: torch.Size([3, 5, 1, 25])\n",
      "Q exp: torch.Size([3, 5, 512, 25]) A_q: torch.Size([3, 5, 1, 25])\n",
      "Q_feat: torch.Size([3, 5, 512, 25])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CrossAttentionModule                     [3, 5, 512, 25]           --\n",
       "├─FusionLayer: 1-1                       [3, 5, 1, 25]             --\n",
       "│    └─AdaptiveAvgPool2d: 2-1            [3, 5, 25, 1, 1]          --\n",
       "│    └─Conv2d: 2-2                       [15, 15, 1, 1]            390\n",
       "│    └─ReLU: 2-3                         [15, 15, 1, 1]            --\n",
       "│    └─Conv2d: 2-4                       [15, 25, 1, 1]            400\n",
       "├─FusionLayer: 1-2                       [3, 5, 1, 25]             (recursive)\n",
       "│    └─AdaptiveAvgPool2d: 2-5            [3, 5, 25, 1, 1]          --\n",
       "│    └─Conv2d: 2-6                       [15, 15, 1, 1]            (recursive)\n",
       "│    └─ReLU: 2-7                         [15, 15, 1, 1]            --\n",
       "│    └─Conv2d: 2-8                       [15, 25, 1, 1]            (recursive)\n",
       "==========================================================================================\n",
       "Total params: 790\n",
       "Trainable params: 790\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.02\n",
       "==========================================================================================\n",
       "Input size (MB): 1.54\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 1.55\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 3\n",
    "m = 5\n",
    "\n",
    "\n",
    "\n",
    "summary(CrossAttentionModule((512, 5, 5)), (2, b, m, 512, 5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  4.],\n",
       "         [ 9., 16.]]),\n",
       " tensor([[ 7., 10.],\n",
       "         [15., 22.]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "B = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "torch.mul(A, B), torch.matmul(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7., 22.],\n",
       "        [21., 44.]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1.0, 2.0], [3., 4.]])\n",
    "B = torch.tensor([[7., 11.]])\n",
    "A*B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionNetwork(nn.Module):\n",
    "    def __init__(self, cam: CrossAttentionModule = None, n_classes: int = 6):\n",
    "        super(CrossAttentionNetwork, self).__init__()\n",
    "\n",
    "        self.embedding = ResNet_32x32()\n",
    "        self.cam = cam if cam is not None else CrossAttentionModule(self.embedding.output_shape)\n",
    "        self.classifer = nn.Conv2d(self.embedding.output_shape[0], n_classes, kernel_size=1)\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                support: torch.Tensor, query: torch.Tensor,\n",
    "                y_support: torch.Tensor, y_query: torch.Tensor\n",
    "    ):\n",
    "        # n_support = number of classes * number of examples per class\n",
    "        b, n_support, c, h, w = support.shape\n",
    "\n",
    "        n_y_support_classes = y_support.shape[-1]\n",
    "\n",
    "        # n_query = number of queries to classify\n",
    "        n_query = query.size(1)\n",
    "\n",
    "        assert support.shape[0] == query.shape[0] and \\\n",
    "            support.shape[2:] == query.shape[2:]\n",
    "\n",
    "        # Embed all support and query images at once\n",
    "        support = support.view(-1, c, h, w)\n",
    "        query = query.view(-1, c, h, w)\n",
    "        X = torch.cat((support, query), dim=0)\n",
    "        # E is of shape (b*(n_support + n_query), c, h, w)\n",
    "        E: torch.Tensor = self.embedding(X)\n",
    "        # print(\"E\", E.shape)\n",
    "\n",
    "        E_support = E[:b*n_support]\n",
    "        # Flattening images from (c,h,w) to (c*h*w)\n",
    "        E_support = E_support.view(b, n_support, -1)\n",
    "        # print(\"E_support\", E_support.shape)\n",
    "        y_support = y_support.transpose(1, 2)\n",
    "        # print(\"y_supp\", y_support.shape)\n",
    "\n",
    "        # Smart way of averaging embedding vectors, generalized for unbalanced number of\n",
    "        # support images wrt to class\n",
    "        E_support_mean = torch.bmm(y_support, E_support)\n",
    "        # print(\"y sum\", torch.sum(y_support, dim=2, keepdim=True).shape)\n",
    "        y_support_class_count = torch.sum(y_support, dim=2, keepdim=True).expand_as(E_support_mean)\n",
    "        # print(\"y_support_class_count\", y_support_class_count.shape)\n",
    "        E_support_mean = torch.div(E_support_mean, y_support_class_count)\n",
    "        # print(\"E_support mean\", E_support_mean.shape)\n",
    "        # print(*E.shape[1:])\n",
    "        E_support_mean = E_support_mean.view(b, n_y_support_classes, *E.shape[1:])\n",
    "\n",
    "        E_query = E[b*n_support:]\n",
    "        E_query = E_query.view(b, n_query, *E.shape[1:])\n",
    "\n",
    "        # Embedding with saturated features which are common between\n",
    "        # each support class and query sample\n",
    "        # E_support_mean/E_query: (b, n_query, n_support, c, h, w)\n",
    "        E_support_mean, E_query = self.cam(E_support_mean, E_query)\n",
    "        # print(\"\\nE_support_mean:\", E_support_mean.shape)\n",
    "        # print(\"E_query:\", E_query.shape)\n",
    "\n",
    "        # Computing mean activations across height and width of image (means of active maps)\n",
    "        E_support_mean = torch.mean(E_support_mean, dim=(-2, -1))\n",
    "        # E_query = torch.mean(E_query, dim=(-2, -1))\n",
    "        # print(\"E_support_mean:\", E_support_mean.shape)\n",
    "        # print(\"E_query:\", E_query.shape)\n",
    "\n",
    "        if self.training:\n",
    "            return self._classification(E_support_mean, E_query, y_query)\n",
    "        else:\n",
    "            return self._test(E_support_mean, E_query)\n",
    "\n",
    "\n",
    "    def _classification(self, P: torch.Tensor, Q: torch.Tensor, y_query: torch.Tensor):\n",
    "        # print(\"\\n_classification\")\n",
    "        # Nearest neighbor classification\n",
    "        # Compute cosine distance\n",
    "        P_norm = F.normalize(P, p=2, dim=3)\n",
    "        Q_norm  = F.normalize(Q, p=2, dim=3)\n",
    "        # print(\"P_norm\", P_norm.shape)\n",
    "        # print(\"Q\", Q.shape)\n",
    "\n",
    "        P_norm = P_norm.unsqueeze(-1)\n",
    "        P_norm = P_norm.unsqueeze(-1)\n",
    "        # print(\"P_norm\", P_norm.shape)\n",
    "        # print(\"Q_norm\", Q_norm.shape)\n",
    "\n",
    "\n",
    "        similarity_score = torch.sum(P_norm * Q_norm, dim=3)\n",
    "        # similarity_score = similarity_score.view(-1, *similarity_score.shape[2:])\n",
    "        # print(\"similarity_score\", similarity_score.shape)\n",
    "\n",
    "        # Global classification\n",
    "        b, n_query, n_support, c, h, w = Q.shape\n",
    "        # print(\"\\nQ\", Q.size())\n",
    "        Q = Q.reshape(b, n_query, self.n_classes, -1)\n",
    "        Q = Q.transpose(2, 3)\n",
    "        # print(\"Q\", Q.size())\n",
    "        y_query = y_query.unsqueeze(-1)\n",
    "        # print(\"y_query:\", y_query.shape)\n",
    "        y_pred = torch.matmul(Q, y_query)\n",
    "        # print(\"y_pred:\", y_pred.shape)\n",
    "        y_pred = y_pred.view(b*n_query, c, h, w)\n",
    "        # print(\"y_pred:\", y_pred.shape)\n",
    "        y_pred = self.classifer(y_pred)\n",
    "        # print(\"y_pred:\", y_pred.shape)\n",
    "        y_pred = y_pred.view(b, n_query, self.n_classes, h, w)\n",
    "\n",
    "        return y_pred, similarity_score\n",
    "\n",
    "\n",
    "    def _test(self, P: torch.Tensor, Q: torch.Tensor):\n",
    "        # Global average pooling over spatial dimensions\n",
    "        Q = Q.mean(-1)\n",
    "        Q = Q.mean(-1)\n",
    "\n",
    "        P_norm = F.normalize(P, p=2, dim=-1)\n",
    "        Q_norm = F.normalize(Q, p=2, dim=-1)\n",
    "        similarity_scores = torch.sum(P_norm*Q_norm, dim=-1)\n",
    "        return similarity_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 15, 5, 3, 3]), torch.Size([1, 15, 5, 3, 3]))"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 1\n",
    "n_way, n_shot = 5, 5\n",
    "c, height, width = 3, 32, 32\n",
    "\n",
    "n_query = 15\n",
    "\n",
    "support = torch.ones((b, n_way*n_shot, c, height, width))\n",
    "y_support = torch.randint(0, n_way, (b, n_way*n_shot))\n",
    "y_support_ohe = F.one_hot(y_support, n_way).type(torch.float)\n",
    "\n",
    "query = torch.randn((b, n_query, c, height, width))\n",
    "y_query = torch.randint(0, n_way, (b, n_query))\n",
    "y_query_ohe = F.one_hot(y_query, n_way).type(torch.float)\n",
    "\n",
    "\n",
    "can2test = CrossAttentionNetwork(n_classes=n_way)\n",
    "y_pred2, sim_score2 = can2test(support, query, y_support_ohe, y_query_ohe)\n",
    "y_pred2.shape, sim_score2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E torch.Size([40, 512, 3, 3])\n",
      "E_support torch.Size([1, 25, 4608])\n",
      "y_supp torch.Size([1, 5, 25])\n",
      "y sum torch.Size([1, 5, 1])\n",
      "y_support_class_count torch.Size([1, 5, 4608])\n",
      "E_support mean torch.Size([1, 5, 4608])\n",
      "512 3 3\n",
      "\n",
      "Cross Attention forward\n",
      "P torch.Size([1, 5, 512, 3, 3])\n",
      "Q torch.Size([1, 15, 512, 3, 3])\n",
      "P torch.Size([1, 5, 512, 9])\n",
      "Q torch.Size([1, 15, 512, 9])\n",
      "P_norm torch.Size([1, 5, 512, 9])\n",
      "Q_norm torch.Size([1, 15, 512, 9])\n",
      "P_norm torch.Size([1, 5, 1, 9, 512])\n",
      "Q_norm torch.Size([1, 1, 15, 512, 9])\n",
      "R_p torch.Size([1, 5, 15, 9, 9])\n",
      "R_q torch.Size([1, 5, 15, 9, 9])\n",
      "\n",
      "FusionLayer forward\n",
      "R: torch.Size([1, 5, 15, 9, 9])\n",
      "w torch.Size([1, 5, 15, 9])\n",
      "w torch.Size([1, 9, 15, 5])\n",
      "w torch.Size([1, 6, 15, 5])\n",
      "w torch.Size([1, 9, 15, 5])\n",
      "w torch.Size([1, 5, 15, 1, 9])\n",
      "A: torch.Size([1, 5, 15, 9])\n",
      "\n",
      "FusionLayer forward\n",
      "R: torch.Size([1, 5, 15, 9, 9])\n",
      "w torch.Size([1, 5, 15, 9])\n",
      "w torch.Size([1, 9, 15, 5])\n",
      "w torch.Size([1, 6, 15, 5])\n",
      "w torch.Size([1, 9, 15, 5])\n",
      "w torch.Size([1, 5, 15, 1, 9])\n",
      "A: torch.Size([1, 5, 15, 9])\n",
      "P torch.Size([1, 5, 512, 9]) A_p torch.Size([1, 5, 15, 9])\n",
      "P torch.Size([1, 5, 1, 512, 9]) A_p torch.Size([1, 5, 15, 1, 9])\n",
      "P torch.Size([1, 5, 15, 512, 9])\n",
      "P torch.Size([1, 5, 15, 512, 3, 3])\n",
      "Q torch.Size([1, 15, 512, 9]) A_q torch.Size([1, 5, 15, 9])\n",
      "Q torch.Size([1, 5, 15, 512, 3, 3])\n",
      "\n",
      "E_support_mean: torch.Size([1, 15, 5, 512, 3, 3])\n",
      "E_query: torch.Size([1, 15, 5, 512, 3, 3])\n",
      "E_support_mean: torch.Size([1, 15, 5, 512])\n",
      "E_query: torch.Size([1, 15, 5, 512, 3, 3])\n",
      "\n",
      "_classification\n",
      "P_norm torch.Size([1, 15, 5, 512])\n",
      "Q torch.Size([1, 15, 5, 512, 3, 3])\n",
      "P_norm torch.Size([1, 15, 5, 512, 1, 1])\n",
      "Q_norm torch.Size([1, 15, 5, 512, 3, 3])\n",
      "similarity_score torch.Size([15, 5, 3, 3])\n",
      "\n",
      "Q torch.Size([1, 15, 5, 512, 3, 3])\n",
      "Q torch.Size([1, 15, 4608, 5])\n",
      "y_query: torch.Size([1, 15, 5, 1])\n",
      "y_pred: torch.Size([1, 15, 4608, 1])\n",
      "y_pred: torch.Size([15, 512, 3, 3])\n",
      "y_pred: torch.Size([15, 5, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-4.5964e-01, -6.7759e-01, -1.9571e+00],\n",
       "           [-1.5642e+00, -1.6503e+00, -2.4744e+00],\n",
       "           [-4.6587e+00, -3.3465e+00, -1.6853e+00]],\n",
       " \n",
       "          [[-1.8017e+00, -1.5339e+00, -4.1703e-02],\n",
       "           [-1.7650e+00, -2.0061e+00, -1.2024e+00],\n",
       "           [-3.2324e+00, -2.1887e+00, -9.3838e-01]],\n",
       " \n",
       "          [[ 3.9101e-01,  5.7364e-01, -2.7385e-02],\n",
       "           [-9.0061e-01, -1.0135e+00,  7.5178e-01],\n",
       "           [-5.4327e-01, -1.7640e-01, -6.1134e-01]],\n",
       " \n",
       "          [[ 9.8394e-01,  7.5506e-01,  9.8741e-01],\n",
       "           [ 6.5876e-01,  4.0617e-01, -1.1633e+00],\n",
       "           [-1.6457e+00,  5.9972e-02,  1.9820e-01]],\n",
       " \n",
       "          [[-4.3297e-01,  1.4053e+00,  5.0144e-01],\n",
       "           [-3.4192e-01, -3.5208e-02,  1.0980e+00],\n",
       "           [-7.9379e-01, -1.0404e+00,  8.9025e-01]]],\n",
       " \n",
       " \n",
       "         [[[-7.4148e-01, -1.0890e+00, -2.5846e-01],\n",
       "           [-4.8078e-01, -8.7759e-01, -2.4776e+00],\n",
       "           [-7.5128e-01, -6.8779e-01, -1.2025e+00]],\n",
       " \n",
       "          [[-2.8457e+00, -1.1334e+00, -8.6582e-01],\n",
       "           [-2.9770e+00, -2.2909e+00, -1.6027e+00],\n",
       "           [-2.3637e+00, -1.5135e+00, -1.3212e+00]],\n",
       " \n",
       "          [[ 1.4068e+00,  5.3994e-01,  3.1176e-01],\n",
       "           [ 7.6094e-01, -7.7825e-01, -6.8470e-01],\n",
       "           [ 1.6526e-01,  7.8091e-01,  7.4143e-01]],\n",
       " \n",
       "          [[ 8.6570e-01, -1.4325e-01,  4.3870e-01],\n",
       "           [-1.0611e+00,  3.1664e-02, -1.5903e+00],\n",
       "           [-1.3656e+00, -1.4598e-01, -2.7296e-01]],\n",
       " \n",
       "          [[-1.1698e+00, -1.1962e+00,  1.7421e+00],\n",
       "           [-1.3221e+00, -2.1114e+00,  7.1060e-01],\n",
       "           [-8.0169e-01, -7.0587e-01,  6.1579e-01]]],\n",
       " \n",
       " \n",
       "         [[[-8.1228e-01, -1.5020e+00, -3.0827e+00],\n",
       "           [ 4.1309e-01, -1.9432e+00, -7.5639e-01],\n",
       "           [-1.7264e+00, -2.4354e+00, -2.1829e+00]],\n",
       " \n",
       "          [[-1.0117e+00, -6.5285e-01, -7.5144e-01],\n",
       "           [-2.1332e+00, -1.7308e+00, -1.4456e+00],\n",
       "           [-2.4115e+00, -1.4204e+00, -1.6345e+00]],\n",
       " \n",
       "          [[ 9.7901e-01,  1.0880e+00,  1.4568e+00],\n",
       "           [ 1.6989e-01,  1.5915e-01,  1.1871e+00],\n",
       "           [ 2.6354e-01, -4.0337e-01,  5.0373e-01]],\n",
       " \n",
       "          [[ 4.7051e-01,  1.6046e+00,  1.2406e+00],\n",
       "           [-5.8089e-02, -1.0983e+00, -9.3552e-01],\n",
       "           [-1.6882e+00,  6.4204e-01, -6.4324e-02]],\n",
       " \n",
       "          [[-1.0231e+00,  3.9367e-01,  2.1574e+00],\n",
       "           [-6.4378e-01, -3.7712e-01,  1.5107e+00],\n",
       "           [-2.2647e-01, -6.7811e-01,  5.8036e-01]]],\n",
       " \n",
       " \n",
       "         [[[-5.2853e-02, -2.3484e+00, -6.2355e-01],\n",
       "           [-1.5292e+00, -1.7289e+00, -2.5931e+00],\n",
       "           [-2.8369e+00, -5.6148e-01, -1.9497e+00]],\n",
       " \n",
       "          [[-2.1911e+00, -1.3331e+00,  1.7158e-01],\n",
       "           [-2.1843e+00, -2.2404e+00, -1.3446e+00],\n",
       "           [-2.4858e+00, -3.1180e+00, -1.2531e+00]],\n",
       " \n",
       "          [[ 8.5937e-01,  8.2635e-01, -1.5308e-01],\n",
       "           [-1.9326e-01, -1.7956e+00,  1.5686e+00],\n",
       "           [ 3.0681e+00,  8.5377e-01, -1.5596e-01]],\n",
       " \n",
       "          [[ 2.2165e+00,  5.6220e-01,  1.5868e-01],\n",
       "           [ 3.3468e-01,  9.7579e-01,  1.4990e-01],\n",
       "           [-8.5874e-01, -1.5110e+00, -8.0190e-01]],\n",
       " \n",
       "          [[ 4.4553e-01,  5.2210e-01, -3.6762e-01],\n",
       "           [ 4.5982e-01,  1.0835e+00,  7.6426e-01],\n",
       "           [ 1.5686e+00,  7.6402e-01,  7.9109e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 6.7302e-02, -4.0820e+00, -1.0125e+00],\n",
       "           [-1.5742e+00, -2.6946e+00, -1.3359e+00],\n",
       "           [-1.4738e+00, -1.8209e+00, -2.0172e+00]],\n",
       " \n",
       "          [[-1.5673e+00, -3.4597e+00, -1.0456e+00],\n",
       "           [-3.2505e+00, -3.2564e+00, -1.9913e+00],\n",
       "           [-2.0591e+00, -2.2674e+00, -1.6983e+00]],\n",
       " \n",
       "          [[ 4.2999e-02,  1.1169e+00, -5.4661e-01],\n",
       "           [-5.8186e-01, -9.1950e-01,  1.0218e+00],\n",
       "           [ 3.6141e-01,  1.0625e+00, -5.3716e-03]],\n",
       " \n",
       "          [[ 9.1169e-01, -2.5510e-01,  5.6332e-01],\n",
       "           [-5.2433e-01, -4.1493e-01, -7.3329e-01],\n",
       "           [-4.1350e-01, -6.4814e-01,  1.6842e-01]],\n",
       " \n",
       "          [[-9.8489e-01,  5.5860e-01,  1.3613e+00],\n",
       "           [-7.1144e-01, -1.1464e+00,  1.0933e+00],\n",
       "           [-1.0234e+00, -1.5610e+00,  4.3122e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.4138e-01, -4.2426e-01, -1.6939e+00],\n",
       "           [-4.9247e-01, -2.0923e+00, -2.6496e+00],\n",
       "           [-9.1847e-01, -1.5519e+00, -1.0036e+00]],\n",
       " \n",
       "          [[-1.1322e+00, -1.5107e+00, -9.0343e-01],\n",
       "           [-1.8686e+00, -1.1136e+00, -2.1486e+00],\n",
       "           [-1.3227e+00, -1.6059e+00, -1.6670e+00]],\n",
       " \n",
       "          [[-5.0165e-01, -1.6149e-02, -1.0591e+00],\n",
       "           [-7.7134e-02, -6.2612e-01,  7.6052e-01],\n",
       "           [-8.9974e-02, -8.9785e-01,  6.2836e-01]],\n",
       " \n",
       "          [[ 1.5814e-01,  9.4655e-01,  1.2115e+00],\n",
       "           [-9.5251e-01, -1.0874e+00, -1.0413e-01],\n",
       "           [-1.0364e+00, -1.2906e+00,  9.1324e-02]],\n",
       " \n",
       "          [[-8.9221e-01, -7.3744e-01,  2.5427e+00],\n",
       "           [-1.3340e+00,  1.3008e-01,  7.7691e-01],\n",
       "           [-1.0855e+00, -1.3124e+00,  2.8106e-01]]],\n",
       " \n",
       " \n",
       "         [[[-7.5767e-01, -1.0928e+00, -1.8989e-01],\n",
       "           [-1.3640e+00, -1.7731e+00, -1.3154e+00],\n",
       "           [-2.6892e+00, -2.6870e+00, -7.6705e-01]],\n",
       " \n",
       "          [[-1.2037e+00, -1.1116e+00, -1.0136e+00],\n",
       "           [-2.9886e+00, -2.2848e+00, -2.0110e+00],\n",
       "           [-2.8077e+00, -1.8765e+00, -1.8835e+00]],\n",
       " \n",
       "          [[-3.3830e-02, -1.8561e-01,  2.4979e-01],\n",
       "           [-7.2038e-01, -1.6138e+00,  9.8324e-01],\n",
       "           [ 1.5456e+00, -3.5542e-01, -2.3628e-01]],\n",
       " \n",
       "          [[ 4.7001e-01,  1.5928e-01, -8.0535e-02],\n",
       "           [ 8.3654e-01, -6.4764e-01, -6.4826e-01],\n",
       "           [-2.3602e+00, -9.1300e-01,  1.3167e+00]],\n",
       " \n",
       "          [[-3.6345e-01, -8.0421e-01,  1.5154e+00],\n",
       "           [-4.0863e-01,  1.5268e+00,  3.0702e+00],\n",
       "           [ 4.7410e-01, -4.8509e-01,  6.7289e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.2194e+00, -1.4894e+00, -2.8653e+00],\n",
       "           [-4.5581e-01, -2.3856e+00, -2.7055e+00],\n",
       "           [-1.5319e+00, -2.0372e+00, -2.0659e+00]],\n",
       " \n",
       "          [[-4.8762e-01, -1.9602e+00, -1.5467e+00],\n",
       "           [-3.1080e+00, -2.3600e+00, -2.2255e+00],\n",
       "           [-2.5917e+00, -2.4243e+00, -5.8670e-01]],\n",
       " \n",
       "          [[-9.3599e-02,  3.1570e-01,  1.6283e+00],\n",
       "           [-2.8554e-01,  1.9188e+00,  1.9177e+00],\n",
       "           [-1.0194e+00, -4.1371e-01, -2.6575e-01]],\n",
       " \n",
       "          [[ 1.1898e+00,  1.2990e+00,  1.3135e-02],\n",
       "           [-4.4705e-01,  4.8504e-01, -2.3310e-01],\n",
       "           [-8.0292e-01, -7.4863e-01,  4.4292e-01]],\n",
       " \n",
       "          [[-7.6313e-01, -1.5899e+00,  3.1110e+00],\n",
       "           [-9.2407e-02,  7.2303e-01,  9.3267e-01],\n",
       "           [-1.1719e+00, -1.3668e+00,  3.4079e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.8826e+00, -2.9350e+00, -1.1477e+00],\n",
       "           [-1.3853e+00, -2.5968e+00, -1.9949e+00],\n",
       "           [-2.5732e+00, -3.4879e+00, -1.6549e+00]],\n",
       " \n",
       "          [[-1.6833e+00, -1.0711e+00, -4.5694e-01],\n",
       "           [-1.6099e+00, -1.5408e+00, -1.1757e+00],\n",
       "           [-1.6932e+00, -3.9015e+00, -2.3533e+00]],\n",
       " \n",
       "          [[ 9.7260e-02, -9.5350e-02, -8.4565e-01],\n",
       "           [-4.6638e-01, -1.0165e+00,  4.4839e-02],\n",
       "           [ 9.6576e-01, -2.6897e-01, -4.7763e-01]],\n",
       " \n",
       "          [[ 1.2552e+00, -3.7038e-02,  1.9929e-01],\n",
       "           [ 3.5780e-01, -7.5494e-01, -1.2077e+00],\n",
       "           [-9.2042e-01, -4.3726e-01,  1.0187e-01]],\n",
       " \n",
       "          [[ 1.0731e-01, -5.0797e-01,  9.6791e-01],\n",
       "           [-5.3603e-02, -9.4357e-01,  8.3590e-01],\n",
       "           [ 3.6286e-02,  1.6285e+00,  3.3744e-01]]],\n",
       " \n",
       " \n",
       "         [[[-5.6570e-01, -2.6199e+00, -1.8057e+00],\n",
       "           [-1.1783e+00, -1.6443e+00, -2.5999e+00],\n",
       "           [-1.9735e+00, -9.9649e-01, -1.4189e+00]],\n",
       " \n",
       "          [[-1.5206e+00, -4.4146e+00, -1.0306e+00],\n",
       "           [-1.4297e+00, -3.5943e-01, -1.0402e+00],\n",
       "           [-2.3787e+00, -2.1648e+00, -1.5590e+00]],\n",
       " \n",
       "          [[ 2.4443e-01,  7.8530e-01, -1.6170e-01],\n",
       "           [-4.3683e-01, -4.4935e-01,  1.6007e+00],\n",
       "           [ 1.0107e+00,  2.9391e-01, -4.2590e-01]],\n",
       " \n",
       "          [[ 2.6056e-01, -1.5742e+00,  1.1851e+00],\n",
       "           [ 5.1774e-01, -5.5279e-01, -3.9294e-01],\n",
       "           [ 3.0779e-01, -1.7543e-02, -1.6755e-01]],\n",
       " \n",
       "          [[-3.4551e-01,  6.0562e-01,  1.7225e+00],\n",
       "           [-6.4764e-02, -1.0797e+00, -5.9355e-01],\n",
       "           [-6.7825e-01,  2.0985e-01,  2.1208e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.3848e-01, -1.2543e+00, -7.7671e-01],\n",
       "           [-1.0402e+00, -2.0552e+00, -2.2904e+00],\n",
       "           [-2.6242e+00, -6.8425e-01, -1.2016e+00]],\n",
       " \n",
       "          [[-2.1280e+00, -2.1584e+00, -6.7379e-01],\n",
       "           [-1.7806e+00, -1.8777e+00, -2.4740e+00],\n",
       "           [-2.0853e+00, -2.1305e+00, -1.6934e+00]],\n",
       " \n",
       "          [[ 5.6417e-01,  6.7786e-01, -1.2142e+00],\n",
       "           [-7.7619e-01,  5.1495e-01,  1.4907e+00],\n",
       "           [ 8.1447e-01, -6.2247e-01, -5.8435e-01]],\n",
       " \n",
       "          [[ 9.4256e-01,  2.1933e-01, -1.2676e-01],\n",
       "           [ 5.6966e-01, -9.9800e-02, -6.8729e-01],\n",
       "           [-9.7105e-01,  3.0267e-01,  5.3336e-01]],\n",
       " \n",
       "          [[-1.1442e+00, -4.8665e-01,  1.2444e+00],\n",
       "           [-7.2311e-01,  1.2544e-03,  1.3136e+00],\n",
       "           [ 4.1714e-01, -1.1389e+00,  4.2738e-01]]],\n",
       " \n",
       " \n",
       "         [[[-6.3565e-01,  5.0530e-01, -2.4326e-01],\n",
       "           [-1.8617e+00, -1.7172e+00, -2.4424e+00],\n",
       "           [-2.3841e+00, -1.3363e+00, -9.8598e-01]],\n",
       " \n",
       "          [[-1.2245e+00, -1.7267e+00, -4.9679e-01],\n",
       "           [-1.0887e+00, -2.2732e+00, -2.3355e+00],\n",
       "           [-2.7595e+00, -2.1675e+00, -6.4702e-01]],\n",
       " \n",
       "          [[-4.7459e-01,  5.9845e-01,  9.5820e-01],\n",
       "           [ 3.4461e-01, -7.9336e-01,  1.5759e+00],\n",
       "           [ 9.4483e-01,  3.6342e-01, -8.6769e-01]],\n",
       " \n",
       "          [[ 9.7701e-01,  6.3272e-01,  3.5226e-01],\n",
       "           [ 5.0757e-01,  4.9681e-01, -4.8919e-01],\n",
       "           [-4.4320e-01,  2.6896e-01,  8.9085e-01]],\n",
       " \n",
       "          [[-5.8592e-01, -1.1096e-01,  9.5635e-01],\n",
       "           [-6.3579e-01,  7.8676e-01,  3.3405e+00],\n",
       "           [-2.4733e-01, -1.1009e+00,  6.3517e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.1791e+00, -1.7303e+00, -1.3662e+00],\n",
       "           [-1.2196e+00, -2.9872e+00, -1.1446e+00],\n",
       "           [-2.0661e+00, -2.2826e+00, -1.5372e+00]],\n",
       " \n",
       "          [[-1.6632e+00, -2.0457e+00,  9.4059e-02],\n",
       "           [-1.3627e+00, -2.8305e+00, -1.1859e+00],\n",
       "           [-2.2530e+00, -3.2332e+00, -1.6637e+00]],\n",
       " \n",
       "          [[ 4.1007e-01,  4.8363e-01, -7.3071e-02],\n",
       "           [-4.2340e-01, -4.2141e-01, -2.4881e-01],\n",
       "           [-6.7227e-02, -3.5426e-01,  9.1139e-02]],\n",
       " \n",
       "          [[-1.2923e-02,  3.2337e-01,  7.2403e-01],\n",
       "           [ 6.5297e-02, -6.4473e-01, -9.3211e-01],\n",
       "           [-9.8533e-01, -1.3823e+00,  5.5996e-01]],\n",
       " \n",
       "          [[-1.9940e+00, -3.7404e-01,  4.1839e-01],\n",
       "           [ 1.2396e-01,  1.7134e+00,  1.7718e+00],\n",
       "           [ 1.5075e-01, -4.5178e-01,  2.9331e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.9433e-01, -1.6242e+00, -1.0106e+00],\n",
       "           [-2.2005e-02, -2.1847e+00, -1.2439e+00],\n",
       "           [-2.1084e+00, -1.6935e+00, -8.4063e-01]],\n",
       " \n",
       "          [[-9.7148e-01, -9.1519e-01, -3.8413e-02],\n",
       "           [-1.5258e-01, -1.5471e+00, -3.4990e-02],\n",
       "           [-1.4537e+00, -2.3874e+00, -1.4741e+00]],\n",
       " \n",
       "          [[ 6.9026e-01, -8.3418e-01, -1.0696e+00],\n",
       "           [-4.9543e-01,  1.8488e-01,  5.4952e-01],\n",
       "           [ 5.0119e-01, -6.5594e-01, -1.2000e+00]],\n",
       " \n",
       "          [[ 1.4653e+00,  1.1340e+00,  7.6973e-01],\n",
       "           [-1.5329e-01, -6.2124e-01, -5.5224e-01],\n",
       "           [-1.0502e+00,  1.0683e-03,  2.0845e-01]],\n",
       " \n",
       "          [[-1.9528e+00, -7.2344e-01,  1.6490e+00],\n",
       "           [ 1.0272e+00, -5.8420e-01,  3.5928e-01],\n",
       "           [-2.0502e-01, -3.3746e-01, -2.1098e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 3.6635e-01, -1.3387e+00, -1.2846e+00],\n",
       "           [-1.0155e+00, -1.7938e+00, -2.3290e-01],\n",
       "           [-1.0552e+00, -9.2509e-01, -1.2921e+00]],\n",
       " \n",
       "          [[-1.1585e+00, -1.8835e+00, -2.0138e+00],\n",
       "           [-2.1107e+00, -1.7679e+00, -1.2037e+00],\n",
       "           [-1.5976e+00, -1.6657e+00, -1.6382e+00]],\n",
       " \n",
       "          [[-2.5196e-01,  4.3717e-01,  3.8156e-01],\n",
       "           [ 3.9482e-01, -4.1611e-01,  3.5476e-01],\n",
       "           [ 4.5635e-01,  2.1490e-01, -3.9559e-01]],\n",
       " \n",
       "          [[ 7.0651e-01,  1.0104e+00,  1.1149e+00],\n",
       "           [-4.1936e-01,  3.1305e-02,  6.8910e-02],\n",
       "           [-2.1421e-01, -2.7200e-02, -3.9927e-03]],\n",
       " \n",
       "          [[-3.2626e-02,  4.7887e-01,  3.5511e+00],\n",
       "           [-2.1755e+00,  1.5052e-01,  9.4284e-01],\n",
       "           [-7.3796e-01, -1.1320e+00,  5.4767e-01]]]],\n",
       "        grad_fn=<ConvolutionBackward0>),\n",
       " tensor([[[[0.3957, 0.3655, 0.4160],\n",
       "           [0.3974, 0.3388, 0.4161],\n",
       "           [0.4135, 0.4025, 0.4143]],\n",
       " \n",
       "          [[0.3957, 0.3655, 0.4160],\n",
       "           [0.3974, 0.3388, 0.4161],\n",
       "           [0.4135, 0.4025, 0.4143]],\n",
       " \n",
       "          [[0.3957, 0.3655, 0.4160],\n",
       "           [0.3974, 0.3388, 0.4161],\n",
       "           [0.4135, 0.4025, 0.4143]],\n",
       " \n",
       "          [[0.3957, 0.3655, 0.4160],\n",
       "           [0.3974, 0.3388, 0.4161],\n",
       "           [0.4135, 0.4025, 0.4143]],\n",
       " \n",
       "          [[0.3957, 0.3655, 0.4160],\n",
       "           [0.3974, 0.3388, 0.4161],\n",
       "           [0.4135, 0.4025, 0.4143]]],\n",
       " \n",
       " \n",
       "         [[[0.4384, 0.3878, 0.4437],\n",
       "           [0.3836, 0.3529, 0.4055],\n",
       "           [0.4251, 0.3812, 0.4303]],\n",
       " \n",
       "          [[0.4384, 0.3878, 0.4437],\n",
       "           [0.3836, 0.3529, 0.4055],\n",
       "           [0.4251, 0.3812, 0.4303]],\n",
       " \n",
       "          [[0.4384, 0.3878, 0.4437],\n",
       "           [0.3836, 0.3529, 0.4055],\n",
       "           [0.4251, 0.3812, 0.4303]],\n",
       " \n",
       "          [[0.4384, 0.3878, 0.4437],\n",
       "           [0.3836, 0.3529, 0.4055],\n",
       "           [0.4251, 0.3812, 0.4303]],\n",
       " \n",
       "          [[0.4384, 0.3878, 0.4437],\n",
       "           [0.3836, 0.3529, 0.4055],\n",
       "           [0.4251, 0.3812, 0.4303]]],\n",
       " \n",
       " \n",
       "         [[[0.4306, 0.3752, 0.4821],\n",
       "           [0.3882, 0.3318, 0.4165],\n",
       "           [0.4547, 0.4206, 0.4294]],\n",
       " \n",
       "          [[0.4306, 0.3752, 0.4821],\n",
       "           [0.3882, 0.3318, 0.4165],\n",
       "           [0.4547, 0.4206, 0.4294]],\n",
       " \n",
       "          [[0.4306, 0.3752, 0.4821],\n",
       "           [0.3882, 0.3318, 0.4165],\n",
       "           [0.4547, 0.4206, 0.4294]],\n",
       " \n",
       "          [[0.4306, 0.3752, 0.4821],\n",
       "           [0.3882, 0.3318, 0.4165],\n",
       "           [0.4547, 0.4206, 0.4294]],\n",
       " \n",
       "          [[0.4306, 0.3752, 0.4821],\n",
       "           [0.3882, 0.3318, 0.4165],\n",
       "           [0.4547, 0.4206, 0.4294]]],\n",
       " \n",
       " \n",
       "         [[[0.3996, 0.3934, 0.4164],\n",
       "           [0.3875, 0.3732, 0.4250],\n",
       "           [0.4497, 0.4324, 0.4292]],\n",
       " \n",
       "          [[0.3996, 0.3934, 0.4164],\n",
       "           [0.3875, 0.3732, 0.4250],\n",
       "           [0.4497, 0.4324, 0.4292]],\n",
       " \n",
       "          [[0.3996, 0.3934, 0.4164],\n",
       "           [0.3875, 0.3732, 0.4250],\n",
       "           [0.4497, 0.4324, 0.4292]],\n",
       " \n",
       "          [[0.3996, 0.3934, 0.4164],\n",
       "           [0.3875, 0.3732, 0.4250],\n",
       "           [0.4497, 0.4324, 0.4292]],\n",
       " \n",
       "          [[0.3996, 0.3934, 0.4164],\n",
       "           [0.3875, 0.3732, 0.4250],\n",
       "           [0.4497, 0.4324, 0.4292]]],\n",
       " \n",
       " \n",
       "         [[[0.4125, 0.3832, 0.4386],\n",
       "           [0.3922, 0.3266, 0.3994],\n",
       "           [0.4276, 0.3985, 0.3903]],\n",
       " \n",
       "          [[0.4125, 0.3832, 0.4386],\n",
       "           [0.3922, 0.3266, 0.3994],\n",
       "           [0.4276, 0.3985, 0.3903]],\n",
       " \n",
       "          [[0.4125, 0.3832, 0.4386],\n",
       "           [0.3922, 0.3266, 0.3994],\n",
       "           [0.4276, 0.3985, 0.3903]],\n",
       " \n",
       "          [[0.4125, 0.3832, 0.4386],\n",
       "           [0.3922, 0.3266, 0.3994],\n",
       "           [0.4276, 0.3985, 0.3903]],\n",
       " \n",
       "          [[0.4125, 0.3832, 0.4386],\n",
       "           [0.3922, 0.3266, 0.3994],\n",
       "           [0.4276, 0.3985, 0.3903]]],\n",
       " \n",
       " \n",
       "         [[[0.4424, 0.3811, 0.4461],\n",
       "           [0.3664, 0.3438, 0.4031],\n",
       "           [0.4235, 0.4170, 0.4121]],\n",
       " \n",
       "          [[0.4424, 0.3811, 0.4461],\n",
       "           [0.3664, 0.3438, 0.4031],\n",
       "           [0.4235, 0.4170, 0.4121]],\n",
       " \n",
       "          [[0.4424, 0.3811, 0.4461],\n",
       "           [0.3664, 0.3438, 0.4031],\n",
       "           [0.4235, 0.4170, 0.4121]],\n",
       " \n",
       "          [[0.4424, 0.3811, 0.4461],\n",
       "           [0.3664, 0.3438, 0.4031],\n",
       "           [0.4235, 0.4170, 0.4121]],\n",
       " \n",
       "          [[0.4424, 0.3811, 0.4461],\n",
       "           [0.3664, 0.3438, 0.4031],\n",
       "           [0.4235, 0.4170, 0.4121]]],\n",
       " \n",
       " \n",
       "         [[[0.4299, 0.4221, 0.4664],\n",
       "           [0.3762, 0.3675, 0.3783],\n",
       "           [0.4242, 0.4177, 0.4228]],\n",
       " \n",
       "          [[0.4299, 0.4221, 0.4664],\n",
       "           [0.3762, 0.3675, 0.3783],\n",
       "           [0.4242, 0.4177, 0.4228]],\n",
       " \n",
       "          [[0.4299, 0.4221, 0.4664],\n",
       "           [0.3762, 0.3675, 0.3783],\n",
       "           [0.4242, 0.4177, 0.4228]],\n",
       " \n",
       "          [[0.4299, 0.4221, 0.4664],\n",
       "           [0.3762, 0.3675, 0.3783],\n",
       "           [0.4242, 0.4177, 0.4228]],\n",
       " \n",
       "          [[0.4299, 0.4221, 0.4664],\n",
       "           [0.3762, 0.3675, 0.3783],\n",
       "           [0.4242, 0.4177, 0.4228]]],\n",
       " \n",
       " \n",
       "         [[[0.4217, 0.3883, 0.4461],\n",
       "           [0.3734, 0.3548, 0.3979],\n",
       "           [0.4030, 0.4084, 0.4126]],\n",
       " \n",
       "          [[0.4217, 0.3883, 0.4461],\n",
       "           [0.3734, 0.3548, 0.3979],\n",
       "           [0.4030, 0.4084, 0.4126]],\n",
       " \n",
       "          [[0.4217, 0.3883, 0.4461],\n",
       "           [0.3734, 0.3548, 0.3979],\n",
       "           [0.4030, 0.4084, 0.4126]],\n",
       " \n",
       "          [[0.4217, 0.3883, 0.4461],\n",
       "           [0.3734, 0.3548, 0.3979],\n",
       "           [0.4030, 0.4084, 0.4126]],\n",
       " \n",
       "          [[0.4217, 0.3883, 0.4461],\n",
       "           [0.3734, 0.3548, 0.3979],\n",
       "           [0.4030, 0.4084, 0.4126]]],\n",
       " \n",
       " \n",
       "         [[[0.4340, 0.4075, 0.4835],\n",
       "           [0.4079, 0.3606, 0.3988],\n",
       "           [0.4233, 0.4312, 0.4243]],\n",
       " \n",
       "          [[0.4340, 0.4075, 0.4835],\n",
       "           [0.4079, 0.3606, 0.3988],\n",
       "           [0.4233, 0.4312, 0.4243]],\n",
       " \n",
       "          [[0.4340, 0.4075, 0.4835],\n",
       "           [0.4079, 0.3606, 0.3988],\n",
       "           [0.4233, 0.4312, 0.4243]],\n",
       " \n",
       "          [[0.4340, 0.4075, 0.4835],\n",
       "           [0.4079, 0.3606, 0.3988],\n",
       "           [0.4233, 0.4312, 0.4243]],\n",
       " \n",
       "          [[0.4340, 0.4075, 0.4835],\n",
       "           [0.4079, 0.3606, 0.3988],\n",
       "           [0.4233, 0.4312, 0.4243]]],\n",
       " \n",
       " \n",
       "         [[[0.4204, 0.4104, 0.4274],\n",
       "           [0.3851, 0.3259, 0.3795],\n",
       "           [0.4560, 0.4123, 0.4041]],\n",
       " \n",
       "          [[0.4204, 0.4104, 0.4274],\n",
       "           [0.3851, 0.3259, 0.3795],\n",
       "           [0.4560, 0.4123, 0.4041]],\n",
       " \n",
       "          [[0.4204, 0.4104, 0.4274],\n",
       "           [0.3851, 0.3259, 0.3795],\n",
       "           [0.4560, 0.4123, 0.4041]],\n",
       " \n",
       "          [[0.4204, 0.4104, 0.4274],\n",
       "           [0.3851, 0.3259, 0.3795],\n",
       "           [0.4560, 0.4123, 0.4041]],\n",
       " \n",
       "          [[0.4204, 0.4104, 0.4274],\n",
       "           [0.3851, 0.3259, 0.3795],\n",
       "           [0.4560, 0.4123, 0.4041]]],\n",
       " \n",
       " \n",
       "         [[[0.4257, 0.3869, 0.4526],\n",
       "           [0.3719, 0.3301, 0.4091],\n",
       "           [0.4494, 0.3920, 0.4073]],\n",
       " \n",
       "          [[0.4257, 0.3869, 0.4526],\n",
       "           [0.3719, 0.3301, 0.4091],\n",
       "           [0.4494, 0.3920, 0.4073]],\n",
       " \n",
       "          [[0.4257, 0.3869, 0.4526],\n",
       "           [0.3719, 0.3301, 0.4091],\n",
       "           [0.4494, 0.3920, 0.4073]],\n",
       " \n",
       "          [[0.4257, 0.3869, 0.4526],\n",
       "           [0.3719, 0.3301, 0.4091],\n",
       "           [0.4494, 0.3920, 0.4073]],\n",
       " \n",
       "          [[0.4257, 0.3869, 0.4526],\n",
       "           [0.3719, 0.3301, 0.4091],\n",
       "           [0.4494, 0.3920, 0.4073]]],\n",
       " \n",
       " \n",
       "         [[[0.4398, 0.3987, 0.4529],\n",
       "           [0.3987, 0.3629, 0.4493],\n",
       "           [0.4483, 0.4378, 0.4108]],\n",
       " \n",
       "          [[0.4398, 0.3987, 0.4529],\n",
       "           [0.3987, 0.3629, 0.4493],\n",
       "           [0.4483, 0.4378, 0.4108]],\n",
       " \n",
       "          [[0.4398, 0.3987, 0.4529],\n",
       "           [0.3987, 0.3629, 0.4493],\n",
       "           [0.4483, 0.4378, 0.4108]],\n",
       " \n",
       "          [[0.4398, 0.3987, 0.4529],\n",
       "           [0.3987, 0.3629, 0.4493],\n",
       "           [0.4483, 0.4378, 0.4108]],\n",
       " \n",
       "          [[0.4398, 0.3987, 0.4529],\n",
       "           [0.3987, 0.3629, 0.4493],\n",
       "           [0.4483, 0.4378, 0.4108]]],\n",
       " \n",
       " \n",
       "         [[[0.4155, 0.3858, 0.4392],\n",
       "           [0.3737, 0.3414, 0.4112],\n",
       "           [0.4215, 0.3789, 0.4046]],\n",
       " \n",
       "          [[0.4155, 0.3858, 0.4392],\n",
       "           [0.3737, 0.3414, 0.4112],\n",
       "           [0.4215, 0.3789, 0.4046]],\n",
       " \n",
       "          [[0.4155, 0.3858, 0.4392],\n",
       "           [0.3737, 0.3414, 0.4112],\n",
       "           [0.4215, 0.3789, 0.4046]],\n",
       " \n",
       "          [[0.4155, 0.3858, 0.4392],\n",
       "           [0.3737, 0.3414, 0.4112],\n",
       "           [0.4215, 0.3789, 0.4046]],\n",
       " \n",
       "          [[0.4155, 0.3858, 0.4392],\n",
       "           [0.3737, 0.3414, 0.4112],\n",
       "           [0.4215, 0.3789, 0.4046]]],\n",
       " \n",
       " \n",
       "         [[[0.4141, 0.3796, 0.4542],\n",
       "           [0.3744, 0.3706, 0.4238],\n",
       "           [0.4406, 0.4156, 0.4398]],\n",
       " \n",
       "          [[0.4141, 0.3796, 0.4542],\n",
       "           [0.3744, 0.3706, 0.4238],\n",
       "           [0.4406, 0.4156, 0.4398]],\n",
       " \n",
       "          [[0.4141, 0.3796, 0.4542],\n",
       "           [0.3744, 0.3706, 0.4238],\n",
       "           [0.4406, 0.4156, 0.4398]],\n",
       " \n",
       "          [[0.4141, 0.3796, 0.4542],\n",
       "           [0.3744, 0.3706, 0.4238],\n",
       "           [0.4406, 0.4156, 0.4398]],\n",
       " \n",
       "          [[0.4141, 0.3796, 0.4542],\n",
       "           [0.3744, 0.3706, 0.4238],\n",
       "           [0.4406, 0.4156, 0.4398]]],\n",
       " \n",
       " \n",
       "         [[[0.4516, 0.3903, 0.4304],\n",
       "           [0.4323, 0.3685, 0.4115],\n",
       "           [0.4443, 0.3910, 0.4065]],\n",
       " \n",
       "          [[0.4516, 0.3903, 0.4304],\n",
       "           [0.4323, 0.3685, 0.4115],\n",
       "           [0.4443, 0.3910, 0.4065]],\n",
       " \n",
       "          [[0.4516, 0.3903, 0.4304],\n",
       "           [0.4323, 0.3685, 0.4115],\n",
       "           [0.4443, 0.3910, 0.4065]],\n",
       " \n",
       "          [[0.4516, 0.3903, 0.4304],\n",
       "           [0.4323, 0.3685, 0.4115],\n",
       "           [0.4443, 0.3910, 0.4065]],\n",
       " \n",
       "          [[0.4516, 0.3903, 0.4304],\n",
       "           [0.4323, 0.3685, 0.4115],\n",
       "           [0.4443, 0.3910, 0.4065]]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 1\n",
    "n_way, n_shot = 5, 5\n",
    "c, height, width = 3, 32, 32\n",
    "\n",
    "n_query = 15\n",
    "\n",
    "support = torch.ones((b, n_way*n_shot, c, height, width))\n",
    "y_support = torch.randint(0, n_way, (b, n_way*n_shot))\n",
    "y_support_ohe = F.one_hot(y_support, n_way).type(torch.float)\n",
    "\n",
    "query = torch.randn((b, n_query, c, height, width))\n",
    "y_query = torch.randint(0, n_way, (b, n_query))\n",
    "y_query_ohe = F.one_hot(y_query, n_way).type(torch.float)\n",
    "# y_support = torch.randint(0, n_way, (b, n_query))\n",
    "# y_support_ohe = F.one_hot(y_support, n_way)\n",
    "\n",
    "\n",
    "can2test = CrossAttentionNetwork(n_classes=n_way)\n",
    "can2test(support, query, y_support_ohe, y_query_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[2., 2.],\n",
      "          [2., 2.]]],\n",
      "\n",
      "\n",
      "        [[[3., 3.],\n",
      "          [3., 3.]]]]) tensor([[[[5., 5.],\n",
      "          [5., 5.]],\n",
      "\n",
      "         [[7., 7.],\n",
      "          [7., 7.]]]])\n",
      "torch.Size([2, 2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[20., 20.],\n",
       "          [20., 20.]],\n",
       "\n",
       "         [[28., 28.],\n",
       "          [28., 28.]]],\n",
       "\n",
       "\n",
       "        [[[30., 30.],\n",
       "          [30., 30.]],\n",
       "\n",
       "         [[42., 42.],\n",
       "          [42., 42.]]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2, 1, 2, 2)\n",
    "b = torch.ones(1, 2, 2, 2)\n",
    "a[0] = a[0] * 2\n",
    "a[1] = a[1] * 3\n",
    "b[:,0] = b[:,0] * 5\n",
    "b[:,1] = b[:,1] * 7\n",
    "print(a,b)\n",
    "print(torch.matmul(a,b).shape)\n",
    "torch.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape 512 3 3\n",
      "Support:  torch.Size([32, 5, 5, 3, 32, 32])\n",
      "query:  torch.Size([32, 3, 32, 32])\n",
      "CrossAttentionModule forward\n",
      "torch.Size([32, 5, 512, 3, 3]) torch.Size([32, 512, 3, 3])\n",
      "P norm: torch.Size([32, 5, 512, 9])\n",
      "P norm t: torch.Size([32, 5, 9, 512])\n",
      "Q norm: torch.Size([32, 1, 512, 9])\n",
      "R_q torch.Size([32, 5, 9, 3, 3])\n",
      "R_p torch.Size([32, 5, 9, 3, 3])\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([32, 5, 9, 3, 3])\n",
      "spatial w: torch.Size([32, 5, 9, 1, 1])\n",
      "spatial w after: torch.Size([160, 9, 1, 1])\n",
      "conv1 w: torch.Size([160, 6, 1, 1])\n",
      "relu w: torch.Size([160, 6, 1, 1])\n",
      "conv2 w: torch.Size([160, 9, 1, 1])\n",
      "weights: torch.Size([160, 9, 1, 1])\n",
      "weights_t: torch.Size([160, 1, 9])\n",
      "R: torch.Size([160, 9, 9])\n",
      "R_mean: torch.Size([160, 1, 9])\n",
      "Attention: torch.Size([32, 5, 1, 9])\n",
      "\n",
      "FusionLayer forward\n",
      "R shape: torch.Size([32, 5, 9, 3, 3])\n",
      "spatial w: torch.Size([32, 5, 9, 1, 1])\n",
      "spatial w after: torch.Size([160, 9, 1, 1])\n",
      "conv1 w: torch.Size([160, 6, 1, 1])\n",
      "relu w: torch.Size([160, 6, 1, 1])\n",
      "conv2 w: torch.Size([160, 9, 1, 1])\n",
      "weights: torch.Size([160, 9, 1, 1])\n",
      "weights_t: torch.Size([160, 1, 9])\n",
      "R: torch.Size([160, 9, 9])\n",
      "R_mean: torch.Size([160, 1, 9])\n",
      "Attention: torch.Size([32, 5, 1, 9])\n",
      "\n",
      "P: torch.Size([32, 5, 512, 9]) A_p: torch.Size([32, 5, 1, 9])\n",
      "P_feat: torch.Size([32, 5, 512, 9])\n",
      "Q: torch.Size([32, 1, 512, 9]) A_q: torch.Size([32, 5, 1, 9])\n",
      "Q exp: torch.Size([32, 5, 512, 9]) A_q: torch.Size([32, 5, 1, 9])\n",
      "Q_feat: torch.Size([32, 5, 512, 9])\n",
      "P_feature: torch.Size([32, 5, 512, 9])\n",
      "Q_feature: torch.Size([32, 5, 512, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "CrossAttentionNetwork                         [32, 5, 512, 9]           --\n",
       "├─ResNet_32x32: 1-1                           [800, 512, 3, 3]          --\n",
       "│    └─Conv2d: 2-1                            [800, 64, 32, 32]         1,792\n",
       "│    └─BatchNorm2d: 2-2                       [800, 64, 32, 32]         128\n",
       "│    └─ReLU: 2-3                              [800, 64, 32, 32]         --\n",
       "│    └─MaxPool2d: 2-4                         [800, 64, 15, 15]         --\n",
       "│    └─Sequential: 2-5                        [800, 128, 15, 15]        --\n",
       "│    │    └─ResidualBlock: 3-1                [800, 128, 15, 15]        230,400\n",
       "│    │    └─ResidualBlock: 3-2                [800, 128, 15, 15]        295,680\n",
       "│    └─MaxPool2d: 2-6                         [800, 128, 7, 7]          --\n",
       "│    └─Sequential: 2-7                        [800, 256, 7, 7]          --\n",
       "│    │    └─ResidualBlock: 3-3                [800, 256, 7, 7]          919,552\n",
       "│    │    └─ResidualBlock: 3-4                [800, 256, 7, 7]          1,181,184\n",
       "│    └─MaxPool2d: 2-8                         [800, 256, 3, 3]          --\n",
       "│    └─Sequential: 2-9                        [800, 512, 3, 3]          --\n",
       "│    │    └─ResidualBlock: 3-5                [800, 512, 3, 3]          3,674,112\n",
       "│    │    └─ResidualBlock: 3-6                [800, 512, 3, 3]          4,721,664\n",
       "├─ResNet_32x32: 1-2                           [32, 512, 3, 3]           (recursive)\n",
       "│    └─Conv2d: 2-10                           [32, 64, 32, 32]          (recursive)\n",
       "│    └─BatchNorm2d: 2-11                      [32, 64, 32, 32]          (recursive)\n",
       "│    └─ReLU: 2-12                             [32, 64, 32, 32]          --\n",
       "│    └─MaxPool2d: 2-13                        [32, 64, 15, 15]          --\n",
       "│    └─Sequential: 2-14                       [32, 128, 15, 15]         (recursive)\n",
       "│    │    └─ResidualBlock: 3-7                [32, 128, 15, 15]         (recursive)\n",
       "│    │    └─ResidualBlock: 3-8                [32, 128, 15, 15]         (recursive)\n",
       "│    └─MaxPool2d: 2-15                        [32, 128, 7, 7]           --\n",
       "│    └─Sequential: 2-16                       [32, 256, 7, 7]           (recursive)\n",
       "│    │    └─ResidualBlock: 3-9                [32, 256, 7, 7]           (recursive)\n",
       "│    │    └─ResidualBlock: 3-10               [32, 256, 7, 7]           (recursive)\n",
       "│    └─MaxPool2d: 2-17                        [32, 256, 3, 3]           --\n",
       "│    └─Sequential: 2-18                       [32, 512, 3, 3]           (recursive)\n",
       "│    │    └─ResidualBlock: 3-11               [32, 512, 3, 3]           (recursive)\n",
       "│    │    └─ResidualBlock: 3-12               [32, 512, 3, 3]           (recursive)\n",
       "├─CrossAttentionModule: 1-3                   [32, 5, 512, 9]           --\n",
       "│    └─FusionLayer: 2-19                      [32, 5, 1, 9]             --\n",
       "│    │    └─AdaptiveAvgPool2d: 3-13           [32, 5, 9, 1, 1]          --\n",
       "│    │    └─Conv2d: 3-14                      [160, 6, 1, 1]            60\n",
       "│    │    └─ReLU: 3-15                        [160, 6, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-16                      [160, 9, 1, 1]            63\n",
       "│    └─FusionLayer: 2-20                      [32, 5, 1, 9]             (recursive)\n",
       "│    │    └─AdaptiveAvgPool2d: 3-17           [32, 5, 9, 1, 1]          --\n",
       "│    │    └─Conv2d: 3-18                      [160, 6, 1, 1]            (recursive)\n",
       "│    │    └─ReLU: 3-19                        [160, 6, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-20                      [160, 9, 1, 1]            (recursive)\n",
       "===============================================================================================\n",
       "Total params: 11,024,635\n",
       "Trainable params: 11,024,635\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 248.14\n",
       "===============================================================================================\n",
       "Input size (MB): 19.66\n",
       "Forward/backward pass size (MB): 3931.02\n",
       "Params size (MB): 44.10\n",
       "Estimated Total Size (MB): 3994.78\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(CrossAttentionNetwork(), (2, 32, 5, 5, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CANLoss(nn.Module):\n",
    "    def __init__(self, can: CrossAttentionNetwork, lamb: float = 0.5, n_classes: int = 5):\n",
    "        super(CANLoss, self).__init__()\n",
    "\n",
    "        self.can = can\n",
    "        self.lamb = lamb\n",
    "        self.classifier = nn.LazyConv1d(out_channels=n_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def cosine_dist(self, P_gap: torch.Tensor, Q_features: torch.Tensor) -> torch.Tensor:\n",
    "        # print(\"\\ncosine_dist\")\n",
    "        # print(\"Q_features\",  Q_features.shape)\n",
    "        # Q_features: (b, c, m)\n",
    "        b, c, m = Q_features.shape\n",
    "        # print(\"P_gap\",  P_gap.shape)\n",
    "        # P_gap: (b, c)\n",
    "        P_gap_exp = P_gap.expand(m, b, c).transpose(0, 1).transpose(1, 2)\n",
    "        # print(\"P_gap_exp\", P_gap_exp.shape)\n",
    "        cos_dist = F.cosine_similarity(P_gap_exp, Q_features, dim=-2)\n",
    "        # print(\"cos_dist\", cos_dist.shape)\n",
    "        return cos_dist\n",
    "\n",
    "\n",
    "    def L1_loss(self, P_sig: torch.Tensor, Q_sig: torch.Tensor, y_true: torch.Tensor):\n",
    "        # P_features: (b, c, m)\n",
    "        # Q_features: (b, c, m)\n",
    "        # print(\"l1 loss\")\n",
    "        # print(\"P_features:\", P_sig.shape)\n",
    "        # print(\"Q_features:\", Q_sig.shape)\n",
    "\n",
    "\n",
    "        P_gap = torch.mean(P_sig, dim=-1)\n",
    "        # P_gap: (b, c)\n",
    "        distances = self.cosine_dist(P_gap, Q_sig)\n",
    "        # distances: (b, m)\n",
    "        # print(\"Distances:\", distances.shape)\n",
    "\n",
    "        # print(\"distances:\", distances)\n",
    "        likelihoods = -torch.log(F.softmax(-distances, dim=-1))\n",
    "        # print(\"likelihoods\", likelihoods)\n",
    "        # print(\"likelihoods\", likelihoods.shape)\n",
    "        l1 = torch.sum(likelihoods)\n",
    "        # print(\"l1\", l1)\n",
    "        return l1\n",
    "\n",
    "\n",
    "    def L2_loss(self, Q_sig: torch.Tensor, y_true: torch.Tensor):\n",
    "        # print(\"l2 loss\")\n",
    "        # print(\"Q_sig:\", Q_sig.shape)\n",
    "        # print()\n",
    "\n",
    "        Z: torch.Tensor = self.classifier(Q_sig)\n",
    "        # print(\"Z\", Z.shape)\n",
    "        Z_significant = []\n",
    "        for y, z in zip(y_true, Z):\n",
    "            Z_significant.append(z[y])\n",
    "        Z_significant = torch.concat(Z_significant)\n",
    "        # print(\"Z_significant:\", Z_significant.shape)\n",
    "\n",
    "        y_pred = -torch.log(F.softmax(Z_significant, dim=-1))\n",
    "        # print(\"y_pred\", y_pred.shape)\n",
    "        l2 = torch.sum(y_pred)\n",
    "        # print(\"l2:\", l2.shape)\n",
    "        return l2\n",
    "\n",
    "\n",
    "    def forward(self, support: torch.Tensor, query: torch.Tensor, y_true: torch.Tensor):\n",
    "        # print(\"support:\", support.shape, \"query:\", query.shape, \"y_true:\", y_true.shape)\n",
    "        P_features, Q_features = self.can(support, query)\n",
    "        # print(\"P_features:\", P_features.shape, \"Q_features:\", Q_features.shape)\n",
    "        Q_significant = []\n",
    "        P_significant = []\n",
    "        for y, q, p in zip(y_true, Q_features, P_features):\n",
    "            Q_significant.append(q[y])\n",
    "            P_significant.append(p[y])\n",
    "        Q_significant = torch.concatenate(Q_significant, dim=0)\n",
    "        P_significant = torch.concatenate(P_significant, dim=0)\n",
    "        # print(\"P_significant:\", P_significant.shape, \"Q_significant:\", Q_significant.shape)\n",
    "        l1 = self.L1_loss(P_significant, Q_significant, y_true)\n",
    "        # print(\"L1 loss\", l1.item())\n",
    "        l2 = self.L2_loss(Q_significant, y_true)\n",
    "        # print(\"L2 loss\", l2.item())\n",
    "        loss = self.lamb*l1 + l2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support: torch.Size([6, 6, 5, 3, 32, 32]) query: torch.Size([6, 3, 32, 32]) y_true: torch.Size([6, 1])\n",
      "P_features: torch.Size([6, 6, 512, 9]) Q_features: torch.Size([6, 6, 512, 9])\n",
      "P_significant: torch.Size([6, 512, 9]) Q_significant: torch.Size([6, 512, 9])\n"
     ]
    }
   ],
   "source": [
    "b = 1\n",
    "M = 6\n",
    "K = 5\n",
    "c = 3\n",
    "height, width = 32, 32\n",
    "can = CrossAttentionNetwork()\n",
    "criterion = CANLoss(can, n_classes=M)\n",
    "\n",
    "support = torch.ones(M, M, K, c, height, width)\n",
    "query = torch.ones(M, c, height, width)\n",
    "y_true = torch.randint(0, 5, (M, 1))\n",
    "loss = criterion(support, query, y_true)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1],\n",
       "          [ 2,  3],\n",
       "          [ 4,  5],\n",
       "          [ 6,  7],\n",
       "          [ 8,  9]],\n",
       " \n",
       "         [[10, 11],\n",
       "          [12, 13],\n",
       "          [14, 15],\n",
       "          [16, 17],\n",
       "          [18, 19]],\n",
       " \n",
       "         [[20, 21],\n",
       "          [22, 23],\n",
       "          [24, 25],\n",
       "          [26, 27],\n",
       "          [28, 29]]]),\n",
       " torch.Size([2, 5, 2]),\n",
       " torch.Size([2]))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(30).view(3, 5, 2)\n",
    "idx = torch.tensor([0, 1])\n",
    "a, a[idx].shape, idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 32\n",
    "M = 5\n",
    "K = 5\n",
    "c = 3\n",
    "height, width = 32, 32\n",
    "can = CrossAttentionNetwork().cuda()\n",
    "criterion = CANLoss(can, n_classes=M).cuda()\n",
    "\n",
    "support = torch.ones(b, M, K, c, height, width).cuda()\n",
    "query = torch.ones(b, c, height, width).cuda()\n",
    "y_true = torch.randint(0, 5, (b, 1)).cuda()\n",
    "loss = criterion(support, query, y_true)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.RandomState(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def get_device():\n",
    "    return \"cuda:0\" if torch.cuda.device_count() > 0 else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "import random\n",
    "\n",
    "class FewShotSampler(Sampler):\n",
    "    def __init__(self, data_source, n_shots: int, shuffle: bool = True):\n",
    "        self.data_source = data_source\n",
    "        self.n_shots = n_shots\n",
    "        self.classes = self._get_classes()\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "\n",
    "    def _get_classes(self):\n",
    "        classes = dict()\n",
    "        for i, (_, label) in enumerate(self.data_source):\n",
    "            if label not in classes:\n",
    "                classes[label] = []\n",
    "            classes[label].append(i)\n",
    "\n",
    "        return classes\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = []\n",
    "        n = len(self.data_source)\n",
    "        n_classes = len(self.classes)\n",
    "        episodes = n // ((self.n_shots + 1) * n_classes)\n",
    "\n",
    "        for i in range(episodes):\n",
    "            query = []\n",
    "            for c in self.classes:\n",
    "                class_ind = self.classes[c]\n",
    "                lower = i*(self.n_shots + 1)\n",
    "                upper = (i+1)*(self.n_shots + 1) - 1\n",
    "                support_one_class = class_ind[lower : upper]\n",
    "                query.append(class_ind[upper])\n",
    "                if self.shuffle:\n",
    "                    random.shuffle(support_one_class)\n",
    "                indices.extend(support_one_class)\n",
    "            if self.shuffle:\n",
    "                random.shuffle(query)\n",
    "            indices.extend(query)\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import shutil\n",
    "\n",
    "\n",
    "CINIC_MEAN = [0.47889522, 0.47227842, 0.43047404]\n",
    "CINIC_STD = [0.24205776, 0.23828046, 0.25874835]\n",
    "\n",
    "\n",
    "def prepare_data_folder(data_path: str, few_shot_path: str):\n",
    "    subsets = [\"train\", \"valid\", \"test\"]\n",
    "    classes = {\"train\": [ 'bird', 'cat', 'deer', 'dog', 'frog', 'horse'],\n",
    "               \"valid\": ['airplane','automobile'],\n",
    "               \"test\": [ 'ship','truck']\n",
    "    }\n",
    "    [os.makedirs(os.path.join(few_shot_path, sub), exist_ok=True) for sub in subsets]\n",
    "    [[os.makedirs(os.path.join(few_shot_path, sub, c), exist_ok=True) for c in classes[sub]]\n",
    "     for sub in subsets]\n",
    "    for sub in subsets:\n",
    "        old_sub_path = os.path.join(data_path, sub)\n",
    "        new_sub_path = os.path.join(few_shot_path, sub)\n",
    "        for c in classes[sub]:\n",
    "            old_dir = os.path.join(old_sub_path, c)\n",
    "            new_dir = os.path.join(new_sub_path, c)\n",
    "            imgs = os.listdir(old_dir)\n",
    "            random.shuffle(imgs)\n",
    "            # 600 support imgs -> 120 batches of 5-way\n",
    "            # 120 imgs for query\n",
    "            # 720 in total\n",
    "            new_imgs = imgs[:720]\n",
    "            [shutil.copyfile(os.path.join(old_dir, img), os.path.join(new_dir, img))\n",
    "             for img in new_imgs]\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "        path: str,\n",
    "        batch_size: int,\n",
    "        n_shots: int,\n",
    "        shuffle: bool,\n",
    "        use_augmentations: bool,\n",
    ") -> DataLoader:\n",
    "    augmentations = ([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomErasing()\n",
    "    ] if use_augmentations else [])\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=CINIC_MEAN,std=CINIC_STD),\n",
    "                                    *augmentations])\n",
    "\n",
    "    ds = torchvision.datasets.ImageFolder(path, transform=transform)\n",
    "    sampler = FewShotSampler(ds, n_shots, shuffle=shuffle)\n",
    "    n_way = len(ds.classes)\n",
    "    episode_size = batch_size * n_way * (n_shots + 1)\n",
    "    loader = DataLoader(ds, sampler=sampler, batch_size=episode_size, num_workers=2, pin_memory=True)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_cinic_few(\n",
    "        data_path: str,\n",
    "        batch_size: int,\n",
    "        n_shots: int,\n",
    ") -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    train_path = os.path.join(data_path, \"train\")\n",
    "    valid_path = os.path.join(data_path, \"valid\")\n",
    "    test_path = os.path.join(data_path, \"test\")\n",
    "\n",
    "    cinic_train = get_dataset(train_path, batch_size, n_shots, True, True)\n",
    "    cinic_validation = get_dataset(valid_path, batch_size, n_shots, False, False)\n",
    "    cinic_test = get_dataset(test_path, batch_size, n_shots, False, False)\n",
    "\n",
    "    return cinic_train, cinic_validation, cinic_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_data_folder(DATA_PATH, FEW_SHOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440\n",
      "0 : 720\n",
      "1 : 720\n",
      "1440\n",
      "1440 120\n",
      "0 0 (l, u): 0 5\n",
      "support: 5\n",
      "0 1 (l, u): 0 5\n",
      "support: 5\n",
      "query; 2\n",
      "1 0 (l, u): 6 11\n",
      "support: 5\n",
      "1 1 (l, u): 6 11\n",
      "support: 5\n",
      "query; 2\n",
      "2 0 (l, u): 12 17\n",
      "support: 5\n",
      "2 1 (l, u): 12 17\n",
      "support: 5\n",
      "query; 2\n",
      "3 0 (l, u): 18 23\n",
      "support: 5\n",
      "3 1 (l, u): 18 23\n",
      "support: 5\n",
      "query; 2\n",
      "4 0 (l, u): 24 29\n",
      "support: 5\n",
      "4 1 (l, u): 24 29\n",
      "support: 5\n",
      "query; 2\n",
      "5 0 (l, u): 30 35\n",
      "support: 5\n",
      "5 1 (l, u): 30 35\n",
      "support: 5\n",
      "query; 2\n",
      "6 0 (l, u): 36 41\n",
      "support: 5\n",
      "6 1 (l, u): 36 41\n",
      "support: 5\n",
      "query; 2\n",
      "7 0 (l, u): 42 47\n",
      "support: 5\n",
      "7 1 (l, u): 42 47\n",
      "support: 5\n",
      "query; 2\n",
      "8 0 (l, u): 48 53\n",
      "support: 5\n",
      "8 1 (l, u): 48 53\n",
      "support: 5\n",
      "query; 2\n",
      "9 0 (l, u): 54 59\n",
      "support: 5\n",
      "9 1 (l, u): 54 59\n",
      "support: 5\n",
      "query; 2\n",
      "10 0 (l, u): 60 65\n",
      "support: 5\n",
      "10 1 (l, u): 60 65\n",
      "support: 5\n",
      "query; 2\n",
      "11 0 (l, u): 66 71\n",
      "support: 5\n",
      "11 1 (l, u): 66 71\n",
      "support: 5\n",
      "query; 2\n",
      "12 0 (l, u): 72 77\n",
      "support: 5\n",
      "12 1 (l, u): 72 77\n",
      "support: 5\n",
      "query; 2\n",
      "13 0 (l, u): 78 83\n",
      "support: 5\n",
      "13 1 (l, u): 78 83\n",
      "support: 5\n",
      "query; 2\n",
      "14 0 (l, u): 84 89\n",
      "support: 5\n",
      "14 1 (l, u): 84 89\n",
      "support: 5\n",
      "query; 2\n",
      "15 0 (l, u): 90 95\n",
      "support: 5\n",
      "15 1 (l, u): 90 95\n",
      "support: 5\n",
      "query; 2\n",
      "16 0 (l, u): 96 101\n",
      "support: 5\n",
      "16 1 (l, u): 96 101\n",
      "support: 5\n",
      "query; 2\n",
      "17 0 (l, u): 102 107\n",
      "support: 5\n",
      "17 1 (l, u): 102 107\n",
      "support: 5\n",
      "query; 2\n",
      "18 0 (l, u): 108 113\n",
      "support: 5\n",
      "18 1 (l, u): 108 113\n",
      "support: 5\n",
      "query; 2\n",
      "19 0 (l, u): 114 119\n",
      "support: 5\n",
      "19 1 (l, u): 114 119\n",
      "support: 5\n",
      "query; 2\n",
      "20 0 (l, u): 120 125\n",
      "support: 5\n",
      "20 1 (l, u): 120 125\n",
      "support: 5\n",
      "query; 2\n",
      "21 0 (l, u): 126 131\n",
      "support: 5\n",
      "21 1 (l, u): 126 131\n",
      "support: 5\n",
      "query; 2\n",
      "22 0 (l, u): 132 137\n",
      "support: 5\n",
      "22 1 (l, u): 132 137\n",
      "support: 5\n",
      "query; 2\n",
      "23 0 (l, u): 138 143\n",
      "support: 5\n",
      "23 1 (l, u): 138 143\n",
      "support: 5\n",
      "query; 2\n",
      "24 0 (l, u): 144 149\n",
      "support: 5\n",
      "24 1 (l, u): 144 149\n",
      "support: 5\n",
      "query; 2\n",
      "25 0 (l, u): 150 155\n",
      "support: 5\n",
      "25 1 (l, u): 150 155\n",
      "support: 5\n",
      "query; 2\n",
      "26 0 (l, u): 156 161\n",
      "support: 5\n",
      "26 1 (l, u): 156 161\n",
      "support: 5\n",
      "query; 2\n",
      "27 0 (l, u): 162 167\n",
      "support: 5\n",
      "27 1 (l, u): 162 167\n",
      "support: 5\n",
      "query; 2\n",
      "28 0 (l, u): 168 173\n",
      "support: 5\n",
      "28 1 (l, u): 168 173\n",
      "support: 5\n",
      "query; 2\n",
      "29 0 (l, u): 174 179\n",
      "support: 5\n",
      "29 1 (l, u): 174 179\n",
      "support: 5\n",
      "query; 2\n",
      "30 0 (l, u): 180 185\n",
      "support: 5\n",
      "30 1 (l, u): 180 185\n",
      "support: 5\n",
      "query; 2\n",
      "31 0 (l, u): 186 191\n",
      "support: 5\n",
      "31 1 (l, u): 186 191\n",
      "support: 5\n",
      "query; 2\n",
      "32 0 (l, u): 192 197\n",
      "support: 5\n",
      "32 1 (l, u): 192 197\n",
      "support: 5\n",
      "query; 2\n",
      "33 0 (l, u): 198 203\n",
      "support: 5\n",
      "33 1 (l, u): 198 203\n",
      "support: 5\n",
      "query; 2\n",
      "34 0 (l, u): 204 209\n",
      "support: 5\n",
      "34 1 (l, u): 204 209\n",
      "support: 5\n",
      "query; 2\n",
      "35 0 (l, u): 210 215\n",
      "support: 5\n",
      "35 1 (l, u): 210 215\n",
      "support: 5\n",
      "query; 2\n",
      "36 0 (l, u): 216 221\n",
      "support: 5\n",
      "36 1 (l, u): 216 221\n",
      "support: 5\n",
      "query; 2\n",
      "37 0 (l, u): 222 227\n",
      "support: 5\n",
      "37 1 (l, u): 222 227\n",
      "support: 5\n",
      "query; 2\n",
      "38 0 (l, u): 228 233\n",
      "support: 5\n",
      "38 1 (l, u): 228 233\n",
      "support: 5\n",
      "query; 2\n",
      "39 0 (l, u): 234 239\n",
      "support: 5\n",
      "39 1 (l, u): 234 239\n",
      "support: 5\n",
      "query; 2\n",
      "40 0 (l, u): 240 245\n",
      "support: 5\n",
      "40 1 (l, u): 240 245\n",
      "support: 5\n",
      "query; 2\n",
      "41 0 (l, u): 246 251\n",
      "support: 5\n",
      "41 1 (l, u): 246 251\n",
      "support: 5\n",
      "query; 2\n",
      "42 0 (l, u): 252 257\n",
      "support: 5\n",
      "42 1 (l, u): 252 257\n",
      "support: 5\n",
      "query; 2\n",
      "43 0 (l, u): 258 263\n",
      "support: 5\n",
      "43 1 (l, u): 258 263\n",
      "support: 5\n",
      "query; 2\n",
      "44 0 (l, u): 264 269\n",
      "support: 5\n",
      "44 1 (l, u): 264 269\n",
      "support: 5\n",
      "query; 2\n",
      "45 0 (l, u): 270 275\n",
      "support: 5\n",
      "45 1 (l, u): 270 275\n",
      "support: 5\n",
      "query; 2\n",
      "46 0 (l, u): 276 281\n",
      "support: 5\n",
      "46 1 (l, u): 276 281\n",
      "support: 5\n",
      "query; 2\n",
      "47 0 (l, u): 282 287\n",
      "support: 5\n",
      "47 1 (l, u): 282 287\n",
      "support: 5\n",
      "query; 2\n",
      "48 0 (l, u): 288 293\n",
      "support: 5\n",
      "48 1 (l, u): 288 293\n",
      "support: 5\n",
      "query; 2\n",
      "49 0 (l, u): 294 299\n",
      "support: 5\n",
      "49 1 (l, u): 294 299\n",
      "support: 5\n",
      "query; 2\n",
      "50 0 (l, u): 300 305\n",
      "support: 5\n",
      "50 1 (l, u): 300 305\n",
      "support: 5\n",
      "query; 2\n",
      "51 0 (l, u): 306 311\n",
      "support: 5\n",
      "51 1 (l, u): 306 311\n",
      "support: 5\n",
      "query; 2\n",
      "52 0 (l, u): 312 317\n",
      "support: 5\n",
      "52 1 (l, u): 312 317\n",
      "support: 5\n",
      "query; 2\n",
      "53 0 (l, u): 318 323\n",
      "support: 5\n",
      "53 1 (l, u): 318 323\n",
      "support: 5\n",
      "query; 2\n",
      "54 0 (l, u): 324 329\n",
      "support: 5\n",
      "54 1 (l, u): 324 329\n",
      "support: 5\n",
      "query; 2\n",
      "55 0 (l, u): 330 335\n",
      "support: 5\n",
      "55 1 (l, u): 330 335\n",
      "support: 5\n",
      "query; 2\n",
      "56 0 (l, u): 336 341\n",
      "support: 5\n",
      "56 1 (l, u): 336 341\n",
      "support: 5\n",
      "query; 2\n",
      "57 0 (l, u): 342 347\n",
      "support: 5\n",
      "57 1 (l, u): 342 347\n",
      "support: 5\n",
      "query; 2\n",
      "58 0 (l, u): 348 353\n",
      "support: 5\n",
      "58 1 (l, u): 348 353\n",
      "support: 5\n",
      "query; 2\n",
      "59 0 (l, u): 354 359\n",
      "support: 5\n",
      "59 1 (l, u): 354 359\n",
      "support: 5\n",
      "query; 2\n",
      "60 0 (l, u): 360 365\n",
      "support: 5\n",
      "60 1 (l, u): 360 365\n",
      "support: 5\n",
      "query; 2\n",
      "61 0 (l, u): 366 371\n",
      "support: 5\n",
      "61 1 (l, u): 366 371\n",
      "support: 5\n",
      "query; 2\n",
      "62 0 (l, u): 372 377\n",
      "support: 5\n",
      "62 1 (l, u): 372 377\n",
      "support: 5\n",
      "query; 2\n",
      "63 0 (l, u): 378 383\n",
      "support: 5\n",
      "63 1 (l, u): 378 383\n",
      "support: 5\n",
      "query; 2\n",
      "64 0 (l, u): 384 389\n",
      "support: 5\n",
      "64 1 (l, u): 384 389\n",
      "support: 5\n",
      "query; 2\n",
      "65 0 (l, u): 390 395\n",
      "support: 5\n",
      "65 1 (l, u): 390 395\n",
      "support: 5\n",
      "query; 2\n",
      "66 0 (l, u): 396 401\n",
      "support: 5\n",
      "66 1 (l, u): 396 401\n",
      "support: 5\n",
      "query; 2\n",
      "67 0 (l, u): 402 407\n",
      "support: 5\n",
      "67 1 (l, u): 402 407\n",
      "support: 5\n",
      "query; 2\n",
      "68 0 (l, u): 408 413\n",
      "support: 5\n",
      "68 1 (l, u): 408 413\n",
      "support: 5\n",
      "query; 2\n",
      "69 0 (l, u): 414 419\n",
      "support: 5\n",
      "69 1 (l, u): 414 419\n",
      "support: 5\n",
      "query; 2\n",
      "70 0 (l, u): 420 425\n",
      "support: 5\n",
      "70 1 (l, u): 420 425\n",
      "support: 5\n",
      "query; 2\n",
      "71 0 (l, u): 426 431\n",
      "support: 5\n",
      "71 1 (l, u): 426 431\n",
      "support: 5\n",
      "query; 2\n",
      "72 0 (l, u): 432 437\n",
      "support: 5\n",
      "72 1 (l, u): 432 437\n",
      "support: 5\n",
      "query; 2\n",
      "73 0 (l, u): 438 443\n",
      "support: 5\n",
      "73 1 (l, u): 438 443\n",
      "support: 5\n",
      "query; 2\n",
      "74 0 (l, u): 444 449\n",
      "support: 5\n",
      "74 1 (l, u): 444 449\n",
      "support: 5\n",
      "query; 2\n",
      "75 0 (l, u): 450 455\n",
      "support: 5\n",
      "75 1 (l, u): 450 455\n",
      "support: 5\n",
      "query; 2\n",
      "76 0 (l, u): 456 461\n",
      "support: 5\n",
      "76 1 (l, u): 456 461\n",
      "support: 5\n",
      "query; 2\n",
      "77 0 (l, u): 462 467\n",
      "support: 5\n",
      "77 1 (l, u): 462 467\n",
      "support: 5\n",
      "query; 2\n",
      "78 0 (l, u): 468 473\n",
      "support: 5\n",
      "78 1 (l, u): 468 473\n",
      "support: 5\n",
      "query; 2\n",
      "79 0 (l, u): 474 479\n",
      "support: 5\n",
      "79 1 (l, u): 474 479\n",
      "support: 5\n",
      "query; 2\n",
      "80 0 (l, u): 480 485\n",
      "support: 5\n",
      "80 1 (l, u): 480 485\n",
      "support: 5\n",
      "query; 2\n",
      "81 0 (l, u): 486 491\n",
      "support: 5\n",
      "81 1 (l, u): 486 491\n",
      "support: 5\n",
      "query; 2\n",
      "82 0 (l, u): 492 497\n",
      "support: 5\n",
      "82 1 (l, u): 492 497\n",
      "support: 5\n",
      "query; 2\n",
      "83 0 (l, u): 498 503\n",
      "support: 5\n",
      "83 1 (l, u): 498 503\n",
      "support: 5\n",
      "query; 2\n",
      "84 0 (l, u): 504 509\n",
      "support: 5\n",
      "84 1 (l, u): 504 509\n",
      "support: 5\n",
      "query; 2\n",
      "85 0 (l, u): 510 515\n",
      "support: 5\n",
      "85 1 (l, u): 510 515\n",
      "support: 5\n",
      "query; 2\n",
      "86 0 (l, u): 516 521\n",
      "support: 5\n",
      "86 1 (l, u): 516 521\n",
      "support: 5\n",
      "query; 2\n",
      "87 0 (l, u): 522 527\n",
      "support: 5\n",
      "87 1 (l, u): 522 527\n",
      "support: 5\n",
      "query; 2\n",
      "88 0 (l, u): 528 533\n",
      "support: 5\n",
      "88 1 (l, u): 528 533\n",
      "support: 5\n",
      "query; 2\n",
      "89 0 (l, u): 534 539\n",
      "support: 5\n",
      "89 1 (l, u): 534 539\n",
      "support: 5\n",
      "query; 2\n",
      "90 0 (l, u): 540 545\n",
      "support: 5\n",
      "90 1 (l, u): 540 545\n",
      "support: 5\n",
      "query; 2\n",
      "91 0 (l, u): 546 551\n",
      "support: 5\n",
      "91 1 (l, u): 546 551\n",
      "support: 5\n",
      "query; 2\n",
      "92 0 (l, u): 552 557\n",
      "support: 5\n",
      "92 1 (l, u): 552 557\n",
      "support: 5\n",
      "query; 2\n",
      "93 0 (l, u): 558 563\n",
      "support: 5\n",
      "93 1 (l, u): 558 563\n",
      "support: 5\n",
      "query; 2\n",
      "94 0 (l, u): 564 569\n",
      "support: 5\n",
      "94 1 (l, u): 564 569\n",
      "support: 5\n",
      "query; 2\n",
      "95 0 (l, u): 570 575\n",
      "support: 5\n",
      "95 1 (l, u): 570 575\n",
      "support: 5\n",
      "query; 2\n",
      "96 0 (l, u): 576 581\n",
      "support: 5\n",
      "96 1 (l, u): 576 581\n",
      "support: 5\n",
      "query; 2\n",
      "97 0 (l, u): 582 587\n",
      "support: 5\n",
      "97 1 (l, u): 582 587\n",
      "support: 5\n",
      "query; 2\n",
      "98 0 (l, u): 588 593\n",
      "support: 5\n",
      "98 1 (l, u): 588 593\n",
      "support: 5\n",
      "query; 2\n",
      "99 0 (l, u): 594 599\n",
      "support: 5\n",
      "99 1 (l, u): 594 599\n",
      "support: 5\n",
      "query; 2\n",
      "100 0 (l, u): 600 605\n",
      "support: 5\n",
      "100 1 (l, u): 600 605\n",
      "support: 5\n",
      "query; 2\n",
      "101 0 (l, u): 606 611\n",
      "support: 5\n",
      "101 1 (l, u): 606 611\n",
      "support: 5\n",
      "query; 2\n",
      "102 0 (l, u): 612 617\n",
      "support: 5\n",
      "102 1 (l, u): 612 617\n",
      "support: 5\n",
      "query; 2\n",
      "103 0 (l, u): 618 623\n",
      "support: 5\n",
      "103 1 (l, u): 618 623\n",
      "support: 5\n",
      "query; 2\n",
      "104 0 (l, u): 624 629\n",
      "support: 5\n",
      "104 1 (l, u): 624 629\n",
      "support: 5\n",
      "query; 2\n",
      "105 0 (l, u): 630 635\n",
      "support: 5\n",
      "105 1 (l, u): 630 635\n",
      "support: 5\n",
      "query; 2\n",
      "106 0 (l, u): 636 641\n",
      "support: 5\n",
      "106 1 (l, u): 636 641\n",
      "support: 5\n",
      "query; 2\n",
      "107 0 (l, u): 642 647\n",
      "support: 5\n",
      "107 1 (l, u): 642 647\n",
      "support: 5\n",
      "query; 2\n",
      "108 0 (l, u): 648 653\n",
      "support: 5\n",
      "108 1 (l, u): 648 653\n",
      "support: 5\n",
      "query; 2\n",
      "109 0 (l, u): 654 659\n",
      "support: 5\n",
      "109 1 (l, u): 654 659\n",
      "support: 5\n",
      "query; 2\n",
      "110 0 (l, u): 660 665\n",
      "support: 5\n",
      "110 1 (l, u): 660 665\n",
      "support: 5\n",
      "query; 2\n",
      "111 0 (l, u): 666 671\n",
      "support: 5\n",
      "111 1 (l, u): 666 671\n",
      "support: 5\n",
      "query; 2\n",
      "112 0 (l, u): 672 677\n",
      "support: 5\n",
      "112 1 (l, u): 672 677\n",
      "support: 5\n",
      "query; 2\n",
      "113 0 (l, u): 678 683\n",
      "support: 5\n",
      "113 1 (l, u): 678 683\n",
      "support: 5\n",
      "query; 2\n",
      "114 0 (l, u): 684 689\n",
      "support: 5\n",
      "114 1 (l, u): 684 689\n",
      "support: 5\n",
      "query; 2\n",
      "115 0 (l, u): 690 695\n",
      "support: 5\n",
      "115 1 (l, u): 690 695\n",
      "support: 5\n",
      "query; 2\n",
      "116 0 (l, u): 696 701\n",
      "support: 5\n",
      "116 1 (l, u): 696 701\n",
      "support: 5\n",
      "query; 2\n",
      "117 0 (l, u): 702 707\n",
      "support: 5\n",
      "117 1 (l, u): 702 707\n",
      "support: 5\n",
      "query; 2\n",
      "118 0 (l, u): 708 713\n",
      "support: 5\n",
      "118 1 (l, u): 708 713\n",
      "support: 5\n",
      "query; 2\n",
      "119 0 (l, u): 714 719\n",
      "support: 5\n",
      "119 1 (l, u): 714 719\n",
      "support: 5\n",
      "query; 2\n",
      "indicies [2, 3, 0, 4, 1, 720, 724, 722, 723, 721, 5, 725, 8, 7, 6, 9, 10, 729, 726, 730, 727, 728, 11, 731, 13, 14, 15, 16, 12, 732, 735, 733, 734, 736, 17, 737, 20, 19, 22, 18, 21, 739, 738, 740, 741, 742, 23, 743, 26, 25, 24, 28, 27, 746, 747, 748, 744, 745, 29, 749, 34, 30, 32, 33, 31, 752, 750, 753, 754, 751, 35, 755, 37, 38, 40, 36, 39, 760, 759, 756, 758, 757, 41, 761, 43, 45, 46, 44, 42, 764, 766, 762, 763, 765, 47, 767, 51, 50, 52, 48, 49, 768, 769, 771, 772, 770, 53, 773, 56, 58, 57, 54, 55, 774, 777, 778, 775, 776, 59, 779, 63, 60, 64, 61, 62, 783, 781, 784, 782, 780, 65, 785, 67, 69, 66, 68, 70, 786, 788, 787, 789, 790, 71, 791, 76, 72, 73, 75, 74, 793, 794, 792, 796, 795, 77, 797, 80, 79, 82, 81, 78, 800, 802, 798, 799, 801, 83, 803, 85, 84, 86, 87, 88, 806, 805, 807, 804, 808, 89, 809, 92, 90, 91, 93, 94, 812, 810, 811, 814, 813, 95, 815, 100, 98, 99, 97, 96, 820, 818, 817, 816, 819, 101, 821, 102, 105, 104, 106, 103, 826, 824, 822, 823, 825, 107, 827, 110, 111, 108, 112, 109, 828, 829, 830, 832, 831, 113, 833, 116, 117, 118, 114, 115, 835, 834, 836, 837, 838, 119, 839, 122, 124, 120, 121, 123, 840, 842, 844, 843, 841, 125, 845, 129, 127, 130, 128, 126, 846, 849, 847, 850, 848, 131, 851, 132, 134, 136, 135, 133, 852, 856, 853, 855, 854, 137, 857, 139, 142, 140, 138, 141, 861, 858, 862, 859, 860, 143, 863, 147, 148, 145, 146, 144, 865, 867, 866, 868, 864, 149, 869, 152, 154, 150, 153, 151, 873, 874, 870, 872, 871, 155, 875, 156, 160, 157, 158, 159, 877, 878, 876, 879, 880, 161, 881, 165, 162, 163, 164, 166, 882, 886, 885, 884, 883, 167, 887, 169, 171, 170, 168, 172, 891, 888, 892, 890, 889, 173, 893, 177, 175, 176, 178, 174, 897, 898, 895, 896, 894, 179, 899, 183, 184, 181, 180, 182, 902, 900, 901, 904, 903, 185, 905, 188, 187, 186, 190, 189, 910, 907, 906, 909, 908, 191, 911, 195, 193, 196, 192, 194, 915, 913, 912, 916, 914, 197, 917, 199, 201, 198, 202, 200, 920, 922, 921, 919, 918, 203, 923, 204, 208, 205, 206, 207, 925, 926, 927, 924, 928, 209, 929, 212, 213, 210, 214, 211, 932, 933, 930, 934, 931, 215, 935, 220, 216, 219, 218, 217, 936, 938, 937, 939, 940, 221, 941, 224, 225, 226, 223, 222, 942, 945, 944, 946, 943, 227, 947, 231, 232, 228, 229, 230, 948, 951, 949, 950, 952, 233, 953, 237, 236, 234, 238, 235, 957, 954, 955, 958, 956, 239, 959, 242, 243, 240, 241, 244, 964, 961, 960, 963, 962, 245, 965, 247, 248, 249, 250, 246, 966, 970, 969, 968, 967, 251, 971, 253, 256, 252, 254, 255, 973, 974, 975, 976, 972, 257, 977, 259, 262, 258, 260, 261, 982, 979, 981, 980, 978, 263, 983, 265, 268, 267, 264, 266, 987, 985, 988, 984, 986, 269, 989, 274, 270, 273, 272, 271, 992, 991, 994, 993, 990, 275, 995, 278, 276, 277, 279, 280, 998, 996, 999, 1000, 997, 281, 1001, 282, 284, 285, 286, 283, 1005, 1006, 1002, 1004, 1003, 287, 1007, 288, 292, 291, 290, 289, 1009, 1008, 1010, 1012, 1011, 293, 1013, 294, 297, 296, 295, 298, 1017, 1015, 1016, 1018, 1014, 299, 1019, 303, 304, 300, 302, 301, 1023, 1022, 1024, 1020, 1021, 305, 1025, 309, 308, 310, 307, 306, 1029, 1026, 1030, 1028, 1027, 311, 1031, 312, 313, 315, 314, 316, 1036, 1034, 1032, 1035, 1033, 317, 1037, 321, 322, 318, 319, 320, 1038, 1041, 1039, 1040, 1042, 323, 1043, 324, 325, 327, 326, 328, 1044, 1047, 1045, 1048, 1046, 329, 1049, 334, 330, 331, 333, 332, 1052, 1050, 1054, 1053, 1051, 335, 1055, 340, 336, 338, 339, 337, 1057, 1060, 1058, 1059, 1056, 341, 1061, 343, 342, 344, 345, 346, 1063, 1062, 1064, 1065, 1066, 347, 1067, 350, 351, 352, 348, 349, 1069, 1071, 1072, 1070, 1068, 353, 1073, 357, 356, 358, 354, 355, 1074, 1078, 1076, 1075, 1077, 359, 1079, 360, 364, 363, 362, 361, 1080, 1082, 1084, 1081, 1083, 365, 1085, 366, 367, 370, 368, 369, 1089, 1090, 1088, 1087, 1086, 371, 1091, 375, 376, 372, 374, 373, 1092, 1095, 1093, 1094, 1096, 377, 1097, 378, 380, 379, 382, 381, 1099, 1101, 1098, 1100, 1102, 383, 1103, 388, 385, 384, 387, 386, 1107, 1105, 1108, 1104, 1106, 389, 1109, 394, 392, 393, 391, 390, 1110, 1112, 1113, 1111, 1114, 395, 1115, 396, 400, 397, 399, 398, 1119, 1118, 1120, 1117, 1116, 401, 1121, 402, 405, 403, 406, 404, 1124, 1123, 1126, 1122, 1125, 407, 1127, 412, 408, 411, 409, 410, 1128, 1130, 1132, 1131, 1129, 413, 1133, 418, 416, 417, 415, 414, 1138, 1134, 1136, 1137, 1135, 419, 1139, 422, 423, 420, 424, 421, 1142, 1140, 1143, 1141, 1144, 425, 1145, 430, 428, 429, 426, 427, 1146, 1149, 1147, 1150, 1148, 431, 1151, 436, 432, 434, 433, 435, 1153, 1155, 1156, 1152, 1154, 437, 1157, 439, 441, 442, 440, 438, 1162, 1160, 1161, 1158, 1159, 443, 1163, 444, 446, 445, 448, 447, 1164, 1167, 1166, 1168, 1165, 449, 1169, 451, 454, 450, 452, 453, 1170, 1173, 1174, 1172, 1171, 455, 1175, 458, 457, 460, 459, 456, 1176, 1177, 1180, 1178, 1179, 461, 1181, 465, 463, 464, 462, 466, 1184, 1183, 1186, 1182, 1185, 467, 1187, 472, 471, 468, 469, 470, 1190, 1192, 1191, 1189, 1188, 473, 1193, 478, 474, 476, 475, 477, 1197, 1195, 1194, 1198, 1196, 479, 1199, 481, 482, 480, 483, 484, 1203, 1200, 1202, 1201, 1204, 485, 1205, 490, 488, 486, 489, 487, 1208, 1207, 1210, 1209, 1206, 491, 1211, 495, 494, 493, 492, 496, 1214, 1213, 1216, 1215, 1212, 497, 1217, 501, 498, 499, 500, 502, 1221, 1218, 1222, 1220, 1219, 503, 1223, 507, 504, 505, 508, 506, 1228, 1227, 1224, 1225, 1226, 509, 1229, 514, 513, 512, 510, 511, 1233, 1230, 1234, 1232, 1231, 515, 1235, 519, 516, 518, 517, 520, 1240, 1239, 1238, 1237, 1236, 521, 1241, 522, 523, 526, 524, 525, 1242, 1243, 1244, 1245, 1246, 527, 1247, 529, 528, 532, 531, 530, 1252, 1248, 1251, 1250, 1249, 533, 1253, 534, 535, 537, 538, 536, 1254, 1256, 1257, 1258, 1255, 539, 1259, 543, 542, 541, 544, 540, 1261, 1263, 1264, 1262, 1260, 545, 1265, 550, 548, 546, 547, 549, 1266, 1270, 1269, 1268, 1267, 551, 1271, 554, 553, 556, 552, 555, 1274, 1276, 1273, 1272, 1275, 557, 1277, 561, 562, 560, 559, 558, 1278, 1279, 1282, 1281, 1280, 563, 1283, 568, 564, 565, 567, 566, 1284, 1285, 1287, 1286, 1288, 569, 1289, 571, 572, 574, 570, 573, 1294, 1290, 1292, 1291, 1293, 575, 1295, 576, 578, 577, 580, 579, 1298, 1299, 1300, 1296, 1297, 581, 1301, 584, 582, 586, 585, 583, 1302, 1303, 1304, 1305, 1306, 587, 1307, 590, 591, 588, 592, 589, 1312, 1309, 1311, 1308, 1310, 593, 1313, 594, 595, 596, 598, 597, 1316, 1317, 1315, 1318, 1314, 599, 1319, 600, 603, 602, 601, 604, 1323, 1320, 1322, 1324, 1321, 605, 1325, 607, 609, 610, 606, 608, 1326, 1328, 1330, 1327, 1329, 611, 1331, 613, 615, 612, 616, 614, 1332, 1333, 1336, 1334, 1335, 617, 1337, 618, 619, 622, 621, 620, 1339, 1340, 1341, 1338, 1342, 623, 1343, 627, 628, 624, 625, 626, 1344, 1346, 1347, 1345, 1348, 629, 1349, 633, 630, 632, 631, 634, 1350, 1351, 1352, 1354, 1353, 635, 1355, 640, 638, 637, 639, 636, 1360, 1356, 1357, 1359, 1358, 641, 1361, 645, 643, 644, 642, 646, 1362, 1363, 1366, 1365, 1364, 647, 1367, 650, 649, 651, 652, 648, 1370, 1368, 1372, 1369, 1371, 653, 1373, 656, 658, 657, 655, 654, 1377, 1374, 1378, 1376, 1375, 659, 1379, 662, 660, 661, 664, 663, 1384, 1381, 1383, 1380, 1382, 665, 1385, 670, 667, 668, 666, 669, 1388, 1389, 1390, 1387, 1386, 671, 1391, 673, 674, 676, 675, 672, 1396, 1392, 1395, 1394, 1393, 677, 1397, 679, 682, 680, 678, 681, 1402, 1401, 1398, 1399, 1400, 683, 1403, 684, 686, 687, 685, 688, 1407, 1405, 1404, 1408, 1406, 689, 1409, 690, 691, 693, 692, 694, 1412, 1413, 1414, 1411, 1410, 695, 1415, 699, 698, 697, 696, 700, 1417, 1416, 1420, 1418, 1419, 701, 1421, 706, 704, 703, 702, 705, 1422, 1424, 1426, 1425, 1423, 707, 1427, 709, 710, 711, 712, 708, 1430, 1428, 1432, 1429, 1431, 713, 1433, 716, 717, 715, 714, 718, 1438, 1437, 1435, 1434, 1436, 719, 1439]\n",
      "len indices 1440\n",
      "tensor([[[0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 1]]])\n"
     ]
    }
   ],
   "source": [
    "n_way = 2\n",
    "n_shots = 5\n",
    "ds = torchvision.datasets.ImageFolder(os.path.join(FEW_SHOT_PATH, \"test\"), transform=transforms.ToTensor())\n",
    "print(len(ds))\n",
    "sampler = FewShotSampler(ds, n_shots)\n",
    "loader = DataLoader(ds, sampler=sampler, batch_size=(n_way * n_shots + n_way))\n",
    "\n",
    "for samples, labels in loader:\n",
    "    support = samples[:n_way * n_shots]\n",
    "    support_labels = labels[:n_way * n_shots]\n",
    "    support_labels = support_labels.view(1, n_way, n_shots)\n",
    "    print(support_labels)\n",
    "    break\n",
    "    # print(samples.shape, label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ship', 'truck']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[-0.0343, -0.0505, -0.0505,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.0181, -0.0505, -0.0505,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.0019, -0.0181, -0.0343,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.7472, -0.6824, -0.0505,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.6986, -0.6014, -0.6662,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.1246,  0.1081,  0.1081,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.1739,  0.1410,  0.1410,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.2233,  0.1904,  0.1739,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.7642, -0.6983, -0.0071,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.5996, -0.5008, -0.5831,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[-0.6785, -0.6937, -0.6937,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.6634, -0.6937, -0.7089,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.6634, -0.6937, -0.7089,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.7695, -0.6482, -0.0268,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.6331, -0.4967, -0.5270,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.1763,  0.1601,  0.1277,  ...,  0.1601,  0.1601,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0305, -0.0019,  0.1277,  ...,  0.0467,  0.0305,  0.0000],\n",
       "           [ 0.0143, -0.0343,  0.1439,  ...,  0.0629,  0.0305,  0.0000],\n",
       "           [-0.0019, -0.0505,  0.1763,  ...,  0.0467,  0.0305,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.0236, -0.0400, -0.0729,  ..., -0.0236, -0.0236,  0.0000],\n",
       "           ...,\n",
       "           [-0.0894, -0.1058, -0.0565,  ..., -0.0071, -0.0071,  0.0000],\n",
       "           [-0.1058, -0.1058, -0.0565,  ..., -0.0071, -0.0071,  0.0000],\n",
       "           [-0.1223, -0.1223, -0.0236,  ..., -0.0071, -0.0071,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.4512, -0.4664, -0.4967,  ..., -0.5270, -0.5270,  0.0000],\n",
       "           ...,\n",
       "           [-0.6179, -0.5118, -0.3148,  ..., -0.5573, -0.5573,  0.0000],\n",
       "           [-0.6331, -0.5270, -0.3148,  ..., -0.5421, -0.5573,  0.0000],\n",
       "           [-0.6482, -0.5421, -0.2845,  ..., -0.5421, -0.5573,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.8081,  0.7919,  0.8081,  ...,  0.0305,  0.0467,  0.1115],\n",
       "           [ 0.8405,  0.8243,  0.8567,  ...,  0.1115,  0.1601,  0.2411],\n",
       "           ...,\n",
       "           [-0.0667, -0.0667, -0.2611,  ...,  1.3104,  1.6182,  1.3752],\n",
       "           [-0.0505, -0.0505, -0.1477,  ...,  1.3266,  1.5048,  1.4562],\n",
       "           [-0.0343, -0.0019, -0.0343,  ...,  1.2942,  1.3428,  1.4238]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.4537,  0.4373,  0.4702,  ..., -0.2540, -0.2540, -0.1881],\n",
       "           [ 0.4866,  0.4702,  0.5031,  ..., -0.1881, -0.1388, -0.0729],\n",
       "           ...,\n",
       "           [-0.4350, -0.4185, -0.4844,  ...,  0.9968,  1.2766,  1.0297],\n",
       "           [-0.4021, -0.4350, -0.4185,  ...,  1.0297,  1.1943,  1.1450],\n",
       "           [-0.3692, -0.4185, -0.3856,  ...,  1.0462,  1.0791,  1.1450]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.1853,  0.1702,  0.2005,  ..., -0.4512, -0.4815, -0.4209],\n",
       "           [ 0.2157,  0.2005,  0.2308,  ..., -0.3906, -0.3754, -0.3148],\n",
       "           ...,\n",
       "           [-0.4209, -0.4057, -0.5118,  ...,  0.7764,  0.9583,  0.7158],\n",
       "           [-0.4057, -0.4057, -0.4360,  ...,  0.8522,  0.8825,  0.8370],\n",
       "           [-0.3754, -0.3906, -0.3754,  ...,  0.8674,  0.8067,  0.8674]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.1801, -0.2773, -0.6014,  ..., -0.8444, -1.3142,  0.0000],\n",
       "           [-0.1801, -0.6662, -0.9416,  ..., -0.5527, -1.2494,  0.0000],\n",
       "           [ 0.5327, -0.5689, -1.1198,  ..., -0.6662,  0.1115,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.2046, -0.3362, -0.6819,  ..., -0.9123, -1.3895,  0.0000],\n",
       "           [-0.1388, -0.7312, -1.0275,  ..., -0.7312, -1.1921,  0.0000],\n",
       "           [ 0.6018, -0.6160, -1.1427,  ..., -0.8300, -0.1717,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.1853, -0.0117, -0.3451,  ..., -0.6482, -1.1029,  0.0000],\n",
       "           [ 0.1702, -0.4209, -0.6785,  ..., -0.4664, -0.9059,  0.0000],\n",
       "           [ 0.7764, -0.2996, -0.8453,  ..., -0.5573,  0.0186,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.2773, -0.2935, -0.2125,  ..., -1.1360,  0.0000,  0.0000],\n",
       "           [-0.2935, -0.3907, -0.5689,  ..., -1.1522,  0.0000,  0.0000],\n",
       "           [-0.5689, -0.5041, -0.7148,  ..., -1.0874,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0258,  0.0094,  0.0587,  ..., -1.0275,  0.0000,  0.0000],\n",
       "           [-0.0236, -0.0894, -0.2704,  ..., -1.0439,  0.0000,  0.0000],\n",
       "           [-0.2704, -0.2210, -0.4021,  ..., -0.9781,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-0.4360, -0.5118, -0.3603,  ..., -0.8301,  0.0000,  0.0000],\n",
       "           [-0.4057, -0.5118, -0.6785,  ..., -0.8604,  0.0000,  0.0000],\n",
       "           [-0.6482, -0.5725, -0.7392,  ..., -0.8301,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-1.1846, -0.8282, -0.9740,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.6824, -0.3097, -1.4600,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-1.0226, -0.9578, -1.3142,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-1.3731, -1.0933, -1.1756,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.8958, -0.5337, -1.5541,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-1.3402, -1.2085, -1.5048,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [-1.2242, -1.0574, -1.0878,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-0.9059, -0.6179, -1.3454,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [-1.1938, -1.1181, -1.3151,  ...,  0.0000,  0.0000,  0.0000]]]]),\n",
       " tensor([0, 0, 0,  ..., 3, 4, 5])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(1)\n",
    "ds_train, ds_valid, ds_test = get_cinic_few(FEW_SHOT_PATH, 32, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "samples, labels = next(iter(ds_train))\n",
    "print(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpecialCrossEntropyLoss, self).__init__()\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: (b, query_size, n_classes, h, w)\n",
    "        # inputs: (b, query_size)\n",
    "        inputs = inputs.view(inputs.size(0)*inputs.size(1), inputs.size(2), -1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        log_probs = self.logsoftmax(inputs)\n",
    "        print(targets)\n",
    "        targets = torch.zeros(inputs.size(0), inputs.size(1)).scatter_(1, targets.unsqueeze(1).data.cpu(), 1)\n",
    "        print(targets)\n",
    "        targets = targets.unsqueeze(-1)\n",
    "        targets = targets.cuda()\n",
    "        print(\"log_probs:\", log_probs.shape, \"targets:\", targets.shape)\n",
    "        loss = (-targets * log_probs).mean(0).sum()\n",
    "        print(-targets*log_probs)\n",
    "        return loss / inputs.size(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training time\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_way = 6\n",
    "n_shots = 5\n",
    "c, h, w = 3, 32, 32\n",
    "\n",
    "set_seed(1)\n",
    "ds_train, ds_valid, ds_test = get_cinic_few(FEW_SHOT_PATH, 1, n_shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_support_query(\n",
    "        episode_samples: torch.Tensor,\n",
    "        n_way: int,\n",
    "        n_shots: int\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    episode_samples = episode_samples\n",
    "    query = episode_samples[n_way * n_shots:].unsqueeze(0)\n",
    "    support = episode_samples[:n_way * n_shots].unsqueeze(0)\n",
    "    return support, query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warm start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 3, 1, 5, 2])\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "log_probs: torch.Size([6, 6, 9]) targets: torch.Size([6, 6, 1])\n",
      "tensor([[[1.4144, 1.8716, 2.0096, 1.5667, 1.6312, 1.9034, 1.2339, 1.1492,\n",
      "          1.6190],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [2.0208, 2.1379, 1.9268, 1.7543, 1.9896, 1.2225, 1.8704, 2.3489,\n",
      "          1.4361],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [2.3549, 1.2669, 2.2866, 1.1537, 1.7869, 2.1681, 2.5091, 1.4095,\n",
      "          1.1770],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [3.0389, 3.4876, 2.9378, 2.4453, 6.6118, 2.2131, 2.7716, 1.7536,\n",
      "          2.7583],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [2.1720, 1.3424, 1.5926, 2.1217, 0.5356, 1.5930, 1.5713, 1.8093,\n",
      "          2.1290]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [1.8258, 1.3344, 1.4957, 0.7265, 0.9375, 2.3109, 1.9451, 1.5410,\n",
      "          1.7362],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0, 4, 3, 1, 5, 2])\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.]])\n",
      "log_probs: torch.Size([6, 6, 9]) targets: torch.Size([6, 6, 1])\n",
      "tensor([[[1.7832, 1.7882, 1.7697, 1.7863, 1.7913, 1.7794, 1.7803, 1.7897,\n",
      "          1.7860],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [1.7779, 1.7961, 1.7986, 1.7906, 1.7954, 1.8044, 1.7986, 1.7973,\n",
      "          1.8035],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [1.7962, 1.7907, 1.7946, 1.8081, 1.8011, 1.7923, 1.7795, 1.8012,\n",
      "          1.7803],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [1.8010, 1.7956, 1.7888, 1.7952, 1.7845, 1.7798, 1.8156, 1.8090,\n",
      "          1.7899],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [1.7955, 1.8006, 1.8087, 1.7838, 1.7942, 1.7939, 1.7952, 1.7809,\n",
      "          1.7898]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [1.8027, 1.8041, 1.7846, 1.8000, 1.7897, 1.7802, 1.8044, 1.7954,\n",
      "          1.7724],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000]]], device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120 [00:07<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 2.8213820457458496\n",
      "Losses [2.8213820457458496]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "n_way = 6\n",
    "n_shots = 5\n",
    "\n",
    "can = CrossAttentionNetwork().cuda()\n",
    "criterion = SpecialCrossEntropyLoss()\n",
    "optimizer_params = {\n",
    "    \"lr\": 0.1,\n",
    "}\n",
    "optimizer = SGD(can.parameters(), **optimizer_params, )\n",
    "epochs = 25\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for episode_samples, episode_labels in tqdm(ds_train):\n",
    "        optimizer.zero_grad()\n",
    "        # print(episode_samples.shape)\n",
    "        # print(episode_labels.shape)\n",
    "        support, query = split_support_query(episode_samples, n_way, n_shots)\n",
    "        support: torch.Tensor = support.cuda()\n",
    "        y_support = episode_labels[:n_way * n_shots].unsqueeze(0)\n",
    "        y_support_ohe: torch.Tensor = F.one_hot(y_support, n_way).float().cuda()\n",
    "\n",
    "        y_query = episode_labels[n_way * n_shots:].unsqueeze(0)\n",
    "        y_query_ohe: torch.Tensor =  F.one_hot(y_query,n_way).float().cuda()\n",
    "        query: torch.Tensor = query.cuda()\n",
    "\n",
    "        # print(\"SUP:\", support.shape)\n",
    "        # print(\"QUERY:\", query.shape)\n",
    "        # print(\"Y_SUP:\", y_support_ohe.shape)\n",
    "        # print(\"Y_QUERY:\", y_query_ohe.shape)\n",
    "\n",
    "        y_pred, cls_score = can(support, query, y_support_ohe, y_query_ohe)\n",
    "        # print(\"y_pred\", y_pred.shape, \"cls_score\", cls_score.shape)\n",
    "        loss1 = criterion(y_pred, y_query.view(-1))\n",
    "        loss2 = criterion(cls_score, y_query.view(-1))\n",
    "        # print(\"l1:\", loss1, \"l2: \", loss2.item)\n",
    "        loss = loss1 + 0.5 * loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        break\n",
    "    print(f\"Epoch {epoch+1}, loss: {torch.tensor(losses).mean()}\")\n",
    "    print(\"Losses\", losses)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 4.283754825592041\n",
      "Losses [2.8294456005096436, 9.136136054992676, 13.230528831481934, 13.052899360656738, 17.615345001220703, 8.604540824890137, 6.4407958984375, 13.702064514160156, 7.341562271118164, 28.1121826171875, 4.872560024261475, 3.0149474143981934, 13.354408264160156, 28.230745315551758, 8.545151710510254, 6.080874443054199, 11.377700805664062, 4.19000244140625, 3.9890012741088867, 4.6881513595581055, 4.505551338195801, 4.5318169593811035, 4.680295944213867, 4.42927360534668, 3.2379324436187744, 3.5702686309814453, 4.196882247924805, 3.2555174827575684, 5.165862083435059, 4.894817352294922, 3.3822593688964844, 3.249025583267212, 2.931387186050415, 3.2217676639556885, 2.6986289024353027, 3.2894160747528076, 3.72867751121521, 2.5520641803741455, 4.32594108581543, 4.045936107635498, 2.9464457035064697, 3.0410008430480957, 2.6001687049865723, 2.3025436401367188, 2.6005003452301025, 4.34052038192749, 2.542372465133667, 3.8469910621643066, 2.810100555419922, 2.608327627182007, 2.4987969398498535, 2.4928135871887207, 2.7775890827178955, 2.6688942909240723, 2.935443878173828, 2.7597758769989014, 2.8853254318237305, 3.070835590362549, 3.5982251167297363, 3.1949234008789062, 2.8692476749420166, 3.1979432106018066, 3.53515625, 3.5227856636047363, 2.993353843688965, 2.5789742469787598, 2.584714889526367, 2.6547253131866455, 2.443869113922119, 2.4824070930480957, 2.8472766876220703, 2.9884400367736816, 2.498798131942749, 2.829909324645996, 2.8255867958068848, 2.503754138946533, 2.8294003009796143, 3.1301097869873047, 3.2913150787353516, 2.6654698848724365, 2.8703927993774414, 3.17915415763855, 2.916422128677368, 2.843829870223999, 2.6215200424194336, 2.6393706798553467, 2.538208246231079, 2.7314186096191406, 2.837311267852783, 2.6797282695770264, 2.596271276473999, 2.771181583404541, 3.6238913536071777, 2.833888053894043, 2.964358329772949, 3.036860466003418, 2.8126301765441895, 2.6921586990356445, 2.5488855838775635, 2.747661590576172, 2.7165536880493164, 3.0780978202819824, 2.86814022064209, 2.466686725616455, 2.828636884689331, 2.72487211227417, 2.573158025741577, 2.7442941665649414, 3.2821927070617676, 2.8848342895507812, 2.976886034011841, 3.2199692726135254, 2.7142300605773926, 2.986722946166992, 3.178959846496582, 2.9155735969543457, 2.7401418685913086, 2.7829856872558594, 2.6996665000915527, 3.085660457611084]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss: 2.7312419414520264\n",
      "Losses [3.060857057571411, 2.69051456451416, 3.01593279838562, 2.680072784423828, 2.6981589794158936, 2.9113516807556152, 2.809696912765503, 2.7537803649902344, 2.794569253921509, 2.5362462997436523, 2.792789936065674, 2.7387959957122803, 2.6019511222839355, 2.6583056449890137, 2.936633825302124, 2.519298553466797, 2.584510326385498, 2.68925142288208, 2.604865789413452, 2.9561057090759277, 2.5127692222595215, 2.562983989715576, 2.6948673725128174, 2.9041690826416016, 2.5802602767944336, 2.4910404682159424, 2.9335343837738037, 3.267958164215088, 2.98996901512146, 2.750950336456299, 2.7400858402252197, 2.7195911407470703, 2.7591662406921387, 2.755347967147827, 2.903384208679199, 2.6517157554626465, 3.0106372833251953, 2.669735908508301, 2.675886631011963, 2.453761100769043, 2.6791367530822754, 2.6913013458251953, 2.790091037750244, 2.547377586364746, 2.6052567958831787, 2.6911168098449707, 2.4100379943847656, 3.142364740371704, 2.624706983566284, 2.7156217098236084, 2.5552000999450684, 2.366309881210327, 2.31845760345459, 2.6726226806640625, 2.8725500106811523, 2.4397647380828857, 3.187708854675293, 2.9112954139709473, 2.6677088737487793, 2.712571859359741, 2.442479133605957, 2.7746689319610596, 2.67718505859375, 2.7123117446899414, 2.845284938812256, 2.42253041267395, 2.818506956100464, 2.763948440551758, 2.6135153770446777, 2.411081552505493, 2.831481695175171, 3.1679701805114746, 2.5163826942443848, 2.576404571533203, 2.798614263534546, 2.5844886302948, 2.9009785652160645, 2.8981244564056396, 2.8604469299316406, 2.518866539001465, 2.6664302349090576, 2.9851837158203125, 2.7119979858398438, 2.7947371006011963, 2.645787477493286, 2.5163707733154297, 2.434671640396118, 2.7480626106262207, 2.754831552505493, 2.8942861557006836, 2.516155958175659, 2.668295383453369, 3.0766005516052246, 2.8499433994293213, 2.5789694786071777, 2.8636021614074707, 2.8792035579681396, 2.8055856227874756, 2.5358614921569824, 2.6003689765930176, 2.802889347076416, 3.149492025375366, 2.9540607929229736, 2.5062685012817383, 2.7444801330566406, 2.6926755905151367, 2.6062138080596924, 2.886317491531372, 2.8506181240081787, 2.692981004714966, 2.8408899307250977, 2.6206936836242676, 2.8011550903320312, 2.759962797164917, 2.851088047027588, 2.72001314163208, 2.7310056686401367, 2.6593518257141113, 2.8720784187316895, 2.6818623542785645]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss: 2.695554494857788\n",
      "Losses [2.9881105422973633, 2.810535192489624, 2.9024295806884766, 2.6380889415740967, 2.7233080863952637, 2.7990829944610596, 2.735384464263916, 2.55119252204895, 2.586552143096924, 2.615338087081909, 2.6094746589660645, 2.7745773792266846, 2.5540523529052734, 2.6304104328155518, 2.6915531158447266, 2.721095085144043, 2.739131450653076, 2.6717708110809326, 2.4914939403533936, 2.6893017292022705, 2.4245686531066895, 2.5942366123199463, 2.6116881370544434, 2.838710308074951, 2.5809316635131836, 2.3684396743774414, 2.746819496154785, 3.128225803375244, 2.834019184112549, 2.7087676525115967, 2.6811811923980713, 2.735593557357788, 2.6355204582214355, 2.889894485473633, 2.9102349281311035, 2.7435309886932373, 2.69708514213562, 2.7676353454589844, 2.6213083267211914, 2.422738552093506, 2.722111463546753, 2.6096482276916504, 2.589284896850586, 2.5624194145202637, 2.6054177284240723, 2.578826904296875, 2.450066328048706, 2.7383148670196533, 2.4761083126068115, 2.506648302078247, 2.5737290382385254, 2.3546578884124756, 2.2861862182617188, 2.6683950424194336, 2.673739433288574, 2.4952659606933594, 3.2265915870666504, 3.00565767288208, 2.768955945968628, 3.0307183265686035, 2.37738037109375, 2.8061957359313965, 2.8474087715148926, 2.733799695968628, 2.8067450523376465, 2.599266529083252, 2.83486270904541, 2.91087007522583, 2.582444190979004, 2.416194438934326, 2.490492820739746, 3.109433650970459, 2.441494941711426, 2.661771059036255, 2.8121285438537598, 2.6002068519592285, 2.7339859008789062, 2.740147829055786, 2.621187210083008, 2.55135440826416, 2.548129081726074, 2.774685859680176, 2.631138801574707, 2.9042158126831055, 2.5365610122680664, 2.5511624813079834, 2.3938045501708984, 2.7752861976623535, 2.7056615352630615, 2.8292746543884277, 2.371884346008301, 2.8133316040039062, 3.24149751663208, 2.672111749649048, 2.640777349472046, 2.87716007232666, 2.8234806060791016, 2.819938898086548, 2.700580596923828, 2.640249252319336, 2.7808642387390137, 3.135427474975586, 2.67691707611084, 2.6701762676239014, 2.7985951900482178, 2.621124029159546, 2.620326042175293, 2.725379467010498, 2.7223691940307617, 2.5978384017944336, 2.9248249530792236, 2.725520610809326, 2.74194073677063, 2.846879005432129, 2.7544748783111572, 2.816380500793457, 2.6186647415161133, 2.6438562870025635, 2.709721565246582, 2.620208501815796]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss: 2.660261869430542\n",
      "Losses [2.995896339416504, 2.592712879180908, 2.8803415298461914, 2.500459671020508, 2.7844998836517334, 2.9148824214935303, 2.7239749431610107, 2.606924057006836, 2.6953272819519043, 2.556788921356201, 2.545518159866333, 2.847712278366089, 2.4874556064605713, 2.633477210998535, 2.6612706184387207, 2.465423107147217, 2.77932071685791, 2.732682466506958, 2.6241211891174316, 2.458482265472412, 2.3024072647094727, 2.5106375217437744, 2.61417293548584, 2.634122371673584, 2.515411853790283, 2.4734039306640625, 2.9274516105651855, 2.907456398010254, 2.8689892292022705, 2.607189178466797, 2.774837017059326, 2.6497464179992676, 2.7368292808532715, 2.7690834999084473, 2.855652093887329, 2.9001896381378174, 2.9385128021240234, 2.650587558746338, 2.583840847015381, 2.367122173309326, 2.6300883293151855, 2.6031270027160645, 2.5817291736602783, 2.3849806785583496, 2.3454012870788574, 2.694528579711914, 2.0938827991485596, 2.7906737327575684, 2.100104808807373, 2.5657460689544678, 2.3132057189941406, 2.553865671157837, 2.1232573986053467, 2.5267677307128906, 2.5798463821411133, 2.646512985229492, 3.11494517326355, 2.857781410217285, 2.7626638412475586, 2.74157977104187, 2.334242343902588, 2.4780397415161133, 2.9148547649383545, 2.846479892730713, 2.7519350051879883, 2.6103034019470215, 2.837026357650757, 2.8417813777923584, 2.624122142791748, 2.551018714904785, 2.5781824588775635, 2.9300782680511475, 2.414577007293701, 2.6239120960235596, 2.8668324947357178, 2.603184461593628, 2.771019458770752, 2.671123504638672, 2.758950710296631, 2.4635932445526123, 2.5972540378570557, 2.7260022163391113, 2.6053049564361572, 2.6664884090423584, 2.5016818046569824, 2.5924570560455322, 2.5715248584747314, 2.680262327194214, 2.626875638961792, 2.8410253524780273, 2.5800063610076904, 2.8179521560668945, 2.9963929653167725, 2.6577656269073486, 2.4426374435424805, 2.758502244949341, 2.699277639389038, 2.6538729667663574, 2.6080143451690674, 2.578784704208374, 2.643921375274658, 3.052506923675537, 2.6333906650543213, 2.4954428672790527, 2.864502429962158, 2.566318988800049, 2.7000503540039062, 2.861279010772705, 2.744502067565918, 2.605536460876465, 2.905120849609375, 2.625561475753784, 2.910733699798584, 2.784792423248291, 2.671623468399048, 2.66329288482666, 2.671781063079834, 2.7380969524383545, 2.712947130203247, 2.621105432510376]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss: 2.644871473312378\n",
      "Losses [2.715864419937134, 2.446046829223633, 2.718503475189209, 2.5079174041748047, 2.6079843044281006, 2.852008581161499, 2.6082122325897217, 2.498654365539551, 2.5741381645202637, 2.382997512817383, 2.68794322013855, 2.9508676528930664, 2.3791017532348633, 2.752394437789917, 2.4401848316192627, 2.8840644359588623, 2.5968613624572754, 2.4563472270965576, 2.6126585006713867, 2.45238995552063, 2.4541845321655273, 2.606088876724243, 2.528163433074951, 2.7898061275482178, 2.7746782302856445, 2.4745044708251953, 2.535886764526367, 2.8351523876190186, 2.6301212310791016, 2.899625301361084, 2.5862808227539062, 2.862063407897949, 2.7823550701141357, 2.61970591545105, 2.905122756958008, 2.7759010791778564, 2.5495967864990234, 2.6596391201019287, 2.582819700241089, 2.5298261642456055, 2.623915195465088, 2.5845813751220703, 2.631376266479492, 2.4292123317718506, 2.5819778442382812, 2.4852333068847656, 2.2728567123413086, 2.9085185527801514, 2.284738063812256, 2.31219220161438, 2.490386486053467, 2.6701624393463135, 2.227212905883789, 2.587564706802368, 2.7374966144561768, 2.495499849319458, 2.6463284492492676, 2.732693672180176, 2.3431286811828613, 2.619609832763672, 2.5838499069213867, 2.7659101486206055, 2.922070026397705, 2.7951302528381348, 2.7827675342559814, 2.528043746948242, 2.7303309440612793, 3.010519027709961, 2.5483763217926025, 2.4984073638916016, 2.7000629901885986, 2.8711419105529785, 2.450791835784912, 2.6638050079345703, 2.8643887042999268, 2.530902147293091, 2.7950339317321777, 2.7744383811950684, 2.7360756397247314, 2.370051383972168, 2.559126853942871, 3.0064053535461426, 2.506960868835449, 2.8409628868103027, 2.5127909183502197, 2.4897398948669434, 2.4123992919921875, 2.6634254455566406, 2.78631854057312, 2.779792308807373, 2.4706220626831055, 2.621922731399536, 3.149614095687866, 2.6422460079193115, 2.4280014038085938, 2.8963072299957275, 2.6837916374206543, 2.7585349082946777, 2.50911021232605, 2.588759422302246, 2.634195327758789, 3.0963141918182373, 2.5305166244506836, 2.540940046310425, 2.596238374710083, 2.799384355545044, 2.517054796218872, 2.7040443420410156, 2.6877145767211914, 2.5494470596313477, 2.6157784461975098, 2.647425889968872, 2.6442723274230957, 3.0779590606689453, 2.820486307144165, 2.8105337619781494, 2.6949331760406494, 2.593125104904175, 2.7295947074890137, 2.6943421363830566]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss: 2.628109931945801\n",
      "Losses [2.6869125366210938, 2.4434359073638916, 2.8326566219329834, 2.5476512908935547, 2.6259355545043945, 2.757293462753296, 2.5945191383361816, 2.6144115924835205, 2.5748980045318604, 2.451211452484131, 2.475947856903076, 2.6126160621643066, 2.5092597007751465, 2.774779796600342, 2.543365478515625, 2.523944139480591, 2.588050127029419, 2.2640862464904785, 3.184805154800415, 2.461103677749634, 2.569711685180664, 2.8326878547668457, 2.618192195892334, 2.687025308609009, 2.604093074798584, 2.5881290435791016, 2.5217649936676025, 2.7322733402252197, 2.6760380268096924, 2.508326292037964, 2.543675184249878, 2.986765146255493, 2.593953847885132, 2.9146833419799805, 2.854214668273926, 2.6632235050201416, 2.7216808795928955, 2.6184678077697754, 2.594362258911133, 2.4139065742492676, 2.487051248550415, 2.752257823944092, 2.609609603881836, 2.2532401084899902, 2.522672653198242, 2.4539215564727783, 2.2229154109954834, 2.6833889484405518, 2.171003580093384, 2.4177839756011963, 2.398162364959717, 2.4374380111694336, 2.1060526371002197, 2.650247812271118, 2.383235454559326, 2.427219867706299, 2.7956881523132324, 2.949805736541748, 2.645461082458496, 2.8330788612365723, 2.3044793605804443, 2.6339569091796875, 3.232496976852417, 2.7388498783111572, 2.9195632934570312, 2.5042474269866943, 2.7269928455352783, 2.994677782058716, 2.6211142539978027, 2.522848129272461, 2.6398699283599854, 2.8886942863464355, 2.4114248752593994, 2.6657729148864746, 2.7271950244903564, 2.6233201026916504, 2.722055196762085, 2.59765887260437, 2.7698049545288086, 2.4230642318725586, 2.4663825035095215, 2.813736915588379, 2.457915782928467, 2.8138678073883057, 2.3813588619232178, 2.49595046043396, 2.3034863471984863, 2.756800889968872, 2.7195076942443848, 2.774280548095703, 2.624124765396118, 2.7130985260009766, 2.890246868133545, 2.5782017707824707, 2.367833137512207, 3.0061113834381104, 2.7813501358032227, 2.471550464630127, 2.5102791786193848, 2.597106456756592, 2.556349039077759, 3.0616044998168945, 2.743544340133667, 2.4781699180603027, 2.6717023849487305, 2.7518177032470703, 2.6857571601867676, 2.754002094268799, 2.737048387527466, 2.5142688751220703, 2.6477608680725098, 2.616910696029663, 2.5518712997436523, 2.8917622566223145, 2.68684983253479, 2.6480040550231934, 2.665449619293213, 2.718686103820801, 2.6060690879821777, 2.68400239944458]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss: 2.614802598953247\n",
      "Losses [2.764807939529419, 2.5266807079315186, 2.6219868659973145, 2.5885119438171387, 2.662684440612793, 2.784043788909912, 2.6994447708129883, 2.395488739013672, 2.546602725982666, 2.547227144241333, 2.5896289348602295, 2.8972764015197754, 2.403109073638916, 2.638826370239258, 2.4417545795440674, 2.647230625152588, 2.59023380279541, 2.391660213470459, 2.5512986183166504, 2.396986484527588, 2.3130457401275635, 2.4427173137664795, 2.689507007598877, 2.6897592544555664, 2.5542068481445312, 2.3892035484313965, 2.8203999996185303, 2.7411186695098877, 2.6117467880249023, 2.5621204376220703, 2.4849305152893066, 2.798586845397949, 2.798494577407837, 2.735610008239746, 2.8166744709014893, 2.8813085556030273, 2.9104695320129395, 2.55397629737854, 2.480375289916992, 2.227761745452881, 2.618595600128174, 2.569037437438965, 2.788572311401367, 2.2960872650146484, 2.5357556343078613, 2.5891003608703613, 2.4478061199188232, 2.6725006103515625, 2.6060352325439453, 2.5060036182403564, 2.243511199951172, 2.4590694904327393, 2.1136951446533203, 2.5386359691619873, 2.421823501586914, 2.2868294715881348, 3.0517148971557617, 2.6609714031219482, 2.8444979190826416, 2.7118771076202393, 2.1239075660705566, 2.637619972229004, 2.8674192428588867, 2.5575578212738037, 2.9874019622802734, 2.2698862552642822, 2.8203494548797607, 3.0217010974884033, 2.514314651489258, 2.3430821895599365, 2.507762908935547, 2.9638619422912598, 2.2761335372924805, 2.6043198108673096, 2.685466766357422, 2.7065930366516113, 2.8211419582366943, 2.724825143814087, 2.676313638687134, 2.3986172676086426, 2.5019516944885254, 2.9815540313720703, 2.457369804382324, 2.7339656352996826, 2.518665075302124, 2.503671646118164, 2.318418025970459, 2.831763744354248, 2.6267967224121094, 3.0278537273406982, 2.477973461151123, 2.737200975418091, 2.8324544429779053, 2.6142563819885254, 2.4064111709594727, 2.5886876583099365, 2.9592862129211426, 2.4262032508850098, 2.4662835597991943, 2.7034621238708496, 2.6003146171569824, 3.038482666015625, 2.6395773887634277, 2.3207364082336426, 2.839055061340332, 2.55926251411438, 2.53417706489563, 2.5548148155212402, 2.6558287143707275, 2.467367172241211, 3.0318310260772705, 2.747648239135742, 2.570960760116577, 2.7914583683013916, 2.733776569366455, 2.7105796337127686, 2.7758281230926514, 2.6873369216918945, 2.5549137592315674, 2.5902271270751953]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 2.6055593490600586\n",
      "Losses [2.6538336277008057, 2.4943222999572754, 2.6299986839294434, 2.4447576999664307, 2.665508270263672, 2.83394455909729, 2.630075216293335, 2.460599184036255, 2.689728260040283, 2.6663784980773926, 2.4773311614990234, 2.9674124717712402, 2.49946928024292, 2.6511309146881104, 2.3880343437194824, 2.4655442237854004, 2.5885472297668457, 2.4433085918426514, 2.3635833263397217, 2.396719217300415, 2.2879374027252197, 2.6253299713134766, 2.651721239089966, 2.7768449783325195, 2.5196704864501953, 2.161285638809204, 2.717418670654297, 2.716383218765259, 2.668393135070801, 2.8233790397644043, 2.519165515899658, 2.724757671356201, 2.7268638610839844, 2.859424114227295, 3.021409273147583, 2.947599411010742, 2.646371841430664, 2.669220447540283, 2.621185541152954, 2.2139930725097656, 2.496363401412964, 2.7018303871154785, 2.7856991291046143, 2.2898905277252197, 2.5480592250823975, 2.5529227256774902, 2.2917070388793945, 2.575984001159668, 2.687304735183716, 2.4284727573394775, 2.4140024185180664, 2.5366098880767822, 2.1845905780792236, 2.5295591354370117, 2.7552170753479004, 2.2250590324401855, 2.735811710357666, 2.6848249435424805, 2.536567211151123, 2.540188789367676, 2.4082531929016113, 2.593057870864868, 3.095526695251465, 2.5297467708587646, 2.598505973815918, 2.530649185180664, 2.7769503593444824, 2.937779426574707, 2.541475296020508, 2.4229979515075684, 2.5967421531677246, 2.6757256984710693, 2.24290132522583, 2.7001094818115234, 2.7298080921173096, 2.8392367362976074, 2.701765537261963, 2.6348061561584473, 2.7322795391082764, 2.369955539703369, 2.48227858543396, 2.564955234527588, 2.26554012298584, 2.701231002807617, 2.533677101135254, 2.50040340423584, 2.149033784866333, 2.7334048748016357, 2.636991024017334, 3.0237033367156982, 2.7168009281158447, 2.6783556938171387, 2.7789056301116943, 2.4915223121643066, 2.373983144760132, 2.8076248168945312, 2.7870588302612305, 2.54893159866333, 2.4004783630371094, 2.5877978801727295, 2.5479846000671387, 3.1089677810668945, 2.7713353633880615, 2.6711854934692383, 2.6993417739868164, 2.5575766563415527, 2.7671515941619873, 2.5322375297546387, 2.814943790435791, 2.629387855529785, 2.745967388153076, 2.558917999267578, 2.6101913452148438, 2.6684796810150146, 2.749661922454834, 2.6702327728271484, 2.617485761642456, 2.485767364501953, 2.6166882514953613, 2.6133852005004883]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, loss: 2.5887644290924072\n",
      "Losses [2.514286518096924, 2.342604160308838, 2.5894365310668945, 2.5348711013793945, 2.4508864879608154, 2.879136085510254, 2.552659511566162, 2.750056743621826, 2.6306777000427246, 2.4825968742370605, 2.3491287231445312, 2.8518242835998535, 2.3742432594299316, 2.6427412033081055, 2.48567271232605, 2.562464714050293, 2.479778528213501, 2.198087692260742, 2.6592211723327637, 2.2645480632781982, 2.3920531272888184, 2.696373224258423, 2.6706154346466064, 2.758934736251831, 2.5627663135528564, 2.3281593322753906, 2.5218799114227295, 2.659024715423584, 2.476799249649048, 2.687746286392212, 2.5640745162963867, 2.9356327056884766, 2.782414197921753, 3.028792381286621, 2.7728025913238525, 2.599249839782715, 2.789139747619629, 2.5535311698913574, 2.5164201259613037, 2.3095576763153076, 2.356282949447632, 2.721949338912964, 2.7296271324157715, 2.4004323482513428, 2.5634374618530273, 2.60402250289917, 2.262397050857544, 2.429792881011963, 2.515427827835083, 2.1389241218566895, 2.388550043106079, 2.2539775371551514, 2.2103898525238037, 2.403200626373291, 2.3342323303222656, 2.293193817138672, 2.8289878368377686, 2.53001070022583, 2.626436471939087, 2.425426959991455, 2.8027806282043457, 2.5082757472991943, 2.994140386581421, 2.5683352947235107, 2.854401111602783, 2.608938694000244, 2.6529924869537354, 2.741234302520752, 2.6685218811035156, 2.4900453090667725, 2.485116958618164, 2.7966365814208984, 2.3136281967163086, 2.7229504585266113, 2.8346707820892334, 2.767378807067871, 2.699094295501709, 2.721925735473633, 2.7072699069976807, 2.316504716873169, 2.523925304412842, 2.593848705291748, 2.403143882751465, 2.7114508152008057, 2.3969383239746094, 2.3478736877441406, 2.2849230766296387, 2.575507164001465, 2.949206590652466, 3.0306460857391357, 2.575223684310913, 2.465522050857544, 2.780533790588379, 2.512359619140625, 2.3245081901550293, 2.598180055618286, 3.065880537033081, 2.423654556274414, 2.540031909942627, 2.6779773235321045, 2.8210806846618652, 2.8986897468566895, 2.610536813735962, 2.499305486679077, 2.8039939403533936, 2.6824185848236084, 2.639902353286743, 2.45796537399292, 2.5716307163238525, 2.5135140419006348, 2.864420175552368, 2.5328826904296875, 2.5521929264068604, 2.8443360328674316, 2.697890281677246, 2.771841526031494, 2.5695579051971436, 2.6763157844543457, 2.7514545917510986, 2.638042449951172]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss: 2.5750062465667725\n",
      "Losses [2.783693552017212, 2.5530755519866943, 2.7445218563079834, 2.5290653705596924, 2.550168752670288, 2.6789989471435547, 3.0021462440490723, 2.255223035812378, 2.460578680038452, 2.6500144004821777, 2.250804901123047, 2.8057498931884766, 2.402285575866699, 2.5225138664245605, 2.3658511638641357, 2.6098203659057617, 2.5574913024902344, 2.1092369556427, 2.3771746158599854, 2.405704975128174, 2.419577121734619, 2.695812225341797, 2.6141130924224854, 2.7985095977783203, 2.4562950134277344, 2.2715847492218018, 2.3045287132263184, 2.565887928009033, 2.453476905822754, 2.541428804397583, 2.6063640117645264, 2.826597213745117, 2.420703649520874, 2.7921109199523926, 3.052978038787842, 2.4574990272521973, 2.6371426582336426, 2.6189799308776855, 2.589548110961914, 2.466750144958496, 2.3541758060455322, 2.6582698822021484, 2.723414421081543, 2.217844009399414, 2.372398853302002, 2.569601058959961, 2.461646795272827, 2.3343167304992676, 2.295454740524292, 2.1895995140075684, 2.465421676635742, 2.5430922508239746, 2.304110527038574, 2.4523239135742188, 2.3657867908477783, 2.3477025032043457, 3.1561527252197266, 2.7413902282714844, 2.933053970336914, 2.54581356048584, 2.1356289386749268, 2.7757046222686768, 2.7869067192077637, 2.6944074630737305, 2.8396382331848145, 2.337222099304199, 2.7109594345092773, 2.99241304397583, 2.487325668334961, 2.4466135501861572, 2.740237236022949, 2.7744383811950684, 2.214454174041748, 2.623599052429199, 2.444951295852661, 3.0087292194366455, 2.691983699798584, 2.5984272956848145, 2.8365700244903564, 2.4005329608917236, 2.5453193187713623, 2.6908373832702637, 2.4561767578125, 2.8097026348114014, 2.3888325691223145, 2.3804209232330322, 2.269442081451416, 2.68023943901062, 2.8168225288391113, 2.7968385219573975, 2.3311281204223633, 2.6641428470611572, 3.1271185874938965, 2.5328831672668457, 2.2445454597473145, 2.595628261566162, 2.8004987239837646, 2.5719099044799805, 2.355348825454712, 2.5740771293640137, 2.529236078262329, 2.936673879623413, 2.600950241088867, 2.583564281463623, 2.5892601013183594, 2.5757639408111572, 2.5329291820526123, 2.486875534057617, 2.6437289714813232, 2.554185152053833, 2.9387834072113037, 2.3471596240997314, 2.4499645233154297, 2.754277229309082, 2.7923901081085205, 2.652907609939575, 2.8366076946258545, 2.523502826690674, 2.5486087799072266, 2.7151668071746826]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 2.559849977493286\n",
      "Losses [2.6476874351501465, 2.407184600830078, 2.7556095123291016, 2.6301817893981934, 2.4587788581848145, 2.631277084350586, 2.5666966438293457, 2.3395142555236816, 2.4983408451080322, 2.329860210418701, 2.5991320610046387, 2.9918599128723145, 2.3582091331481934, 2.7802114486694336, 2.41625714302063, 2.5631864070892334, 2.472480058670044, 2.222628116607666, 2.521245002746582, 2.149017095565796, 2.3267788887023926, 2.426279067993164, 2.584888458251953, 2.6625821590423584, 2.634591579437256, 2.274618625640869, 2.2023091316223145, 2.3333113193511963, 2.664564609527588, 2.465121030807495, 2.5786685943603516, 2.8579869270324707, 2.4063234329223633, 2.820115327835083, 2.8235585689544678, 2.6699154376983643, 2.7528626918792725, 2.822218656539917, 2.442397117614746, 2.5383763313293457, 2.3987340927124023, 2.6333045959472656, 2.4922690391540527, 2.2573511600494385, 2.3584201335906982, 2.7626101970672607, 2.2695415019989014, 2.5905842781066895, 2.29055118560791, 2.260392904281616, 2.3837592601776123, 2.3136255741119385, 2.231901168823242, 2.465819835662842, 2.1568243503570557, 2.114847183227539, 2.946180820465088, 2.784262180328369, 2.615643262863159, 2.432659149169922, 2.4885668754577637, 2.5378522872924805, 3.556445837020874, 2.693702220916748, 2.7225005626678467, 2.455019474029541, 2.7268712520599365, 2.7884998321533203, 2.6464688777923584, 2.346627950668335, 2.4828941822052, 2.7662017345428467, 2.231200695037842, 3.0029375553131104, 2.6648569107055664, 2.453963279724121, 2.714674234390259, 2.818692207336426, 2.78009295463562, 2.3617167472839355, 2.3025941848754883, 2.6172778606414795, 2.435309410095215, 2.5766313076019287, 2.330063581466675, 2.45064377784729, 2.086578130722046, 2.668219566345215, 2.5687637329101562, 2.904409885406494, 2.4476633071899414, 2.275782823562622, 3.0508134365081787, 2.601893186569214, 2.202944755554199, 2.9025769233703613, 2.974060535430908, 2.578334331512451, 2.386357307434082, 2.4823639392852783, 2.761392593383789, 2.9201760292053223, 2.6366560459136963, 2.4991376399993896, 2.7269952297210693, 2.349583864212036, 2.5540218353271484, 2.6378626823425293, 2.6813464164733887, 2.4390575885772705, 3.082631826400757, 2.553792715072632, 2.58955717086792, 2.78969669342041, 2.5856199264526367, 2.673971176147461, 2.5363359451293945, 2.557649850845337, 2.7296035289764404, 2.4077541828155518]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, loss: 2.5538277626037598\n",
      "Losses [2.732764482498169, 2.3388917446136475, 2.640352964401245, 2.4802024364471436, 2.5641088485717773, 2.8163437843322754, 2.493596076965332, 2.2130393981933594, 2.5645713806152344, 2.2496657371520996, 2.523510456085205, 2.8043041229248047, 2.3314085006713867, 2.564570426940918, 2.5408992767333984, 2.491302013397217, 2.5468456745147705, 2.227689266204834, 2.512734889984131, 2.4169058799743652, 2.224144697189331, 2.3005075454711914, 2.571183919906616, 2.757951498031616, 2.364854574203491, 2.28761625289917, 2.3702573776245117, 2.5974016189575195, 2.7886242866516113, 2.614063024520874, 2.643561840057373, 2.88344144821167, 2.436933994293213, 3.139958381652832, 2.608304738998413, 2.7438395023345947, 2.742310047149658, 2.40922212600708, 2.5186710357666016, 2.159257650375366, 2.4368176460266113, 2.5708327293395996, 2.8424201011657715, 2.3697397708892822, 2.175736427307129, 2.642117738723755, 2.4988739490509033, 2.5483899116516113, 2.4878547191619873, 2.075303316116333, 2.2754323482513428, 2.419243812561035, 1.9162267446517944, 2.68974232673645, 2.6848843097686768, 2.3291287422180176, 2.873359203338623, 2.8160653114318848, 2.2132959365844727, 2.6078526973724365, 2.258779764175415, 2.5055127143859863, 3.062408924102783, 2.874582290649414, 2.6827638149261475, 2.471850872039795, 2.513714075088501, 2.911954164505005, 2.588809013366699, 2.2350704669952393, 2.3414149284362793, 3.11627459526062, 2.1764345169067383, 2.8482251167297363, 2.414637804031372, 2.5863451957702637, 2.573803424835205, 2.664290428161621, 2.811619281768799, 2.4632792472839355, 2.4739058017730713, 2.6026172637939453, 2.558633327484131, 2.692491054534912, 2.3289835453033447, 2.50400972366333, 2.254976511001587, 2.4443702697753906, 2.6253955364227295, 2.9587526321411133, 2.4928152561187744, 2.177560329437256, 2.97721529006958, 2.6365106105804443, 2.459916830062866, 2.559025764465332, 2.965514659881592, 2.6015028953552246, 2.293691396713257, 2.5016214847564697, 2.6577951908111572, 2.89412522315979, 2.5859627723693848, 2.553013801574707, 2.497082471847534, 2.433999538421631, 2.4916656017303467, 3.009094476699829, 2.553602457046509, 2.316991090774536, 2.697106122970581, 2.3720691204071045, 2.537421941757202, 2.804224967956543, 2.7722177505493164, 2.4613230228424072, 2.966823101043701, 2.523669719696045, 2.612497329711914, 2.8182260990142822]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:10<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, loss: 2.5346083641052246\n",
      "Losses [2.5883376598358154, 2.4749176502227783, 2.525010824203491, 2.410388231277466, 2.4467811584472656, 2.6670382022857666, 2.9393553733825684, 2.4752228260040283, 2.4115333557128906, 2.151427984237671, 2.439624547958374, 2.654843807220459, 2.2202208042144775, 2.323281764984131, 2.4139833450317383, 2.503342390060425, 2.586469888687134, 2.131030559539795, 2.2850148677825928, 2.3672966957092285, 2.3648312091827393, 2.4924826622009277, 2.7014477252960205, 2.8613779544830322, 2.229851245880127, 2.587125301361084, 2.4966049194335938, 2.468468427658081, 2.9641027450561523, 2.563852071762085, 2.7910518646240234, 2.4332034587860107, 2.6248316764831543, 2.7741971015930176, 2.7097537517547607, 2.687730312347412, 2.6319527626037598, 2.5143187046051025, 2.4804141521453857, 2.1278252601623535, 2.3970930576324463, 2.7222180366516113, 2.4228100776672363, 2.2345004081726074, 2.4715960025787354, 2.7223360538482666, 2.564624786376953, 2.4772531986236572, 2.145864486694336, 2.13271427154541, 2.640256881713867, 2.4103052616119385, 2.2195003032684326, 2.6412107944488525, 2.4873428344726562, 2.309603691101074, 2.8698418140411377, 2.8192996978759766, 2.6221799850463867, 2.3410587310791016, 2.9459073543548584, 2.6233363151550293, 2.869539260864258, 2.6156535148620605, 2.483607530593872, 2.147770643234253, 2.8127553462982178, 2.9546003341674805, 2.5068564414978027, 2.496126651763916, 2.2946128845214844, 2.7354540824890137, 2.1493892669677734, 2.5546810626983643, 2.623933792114258, 2.5847532749176025, 2.6606900691986084, 2.5783896446228027, 2.5920310020446777, 2.394702196121216, 2.6827902793884277, 3.063683271408081, 2.202449321746826, 2.8377652168273926, 2.4248569011688232, 2.479668140411377, 2.1552445888519287, 2.5593414306640625, 2.5438547134399414, 2.6787068843841553, 2.248868465423584, 2.6461563110351562, 2.8287854194641113, 2.3481545448303223, 2.1628763675689697, 2.486006259918213, 2.69977068901062, 2.510582447052002, 2.382418632507324, 2.5815229415893555, 2.4288153648376465, 3.1108169555664062, 2.526839256286621, 2.5933945178985596, 2.6322546005249023, 2.59051513671875, 2.465022563934326, 2.4212865829467773, 2.5335071086883545, 2.4987564086914062, 3.179985523223877, 2.1600840091705322, 2.3862783908843994, 2.5127205848693848, 2.556070327758789, 3.062798261642456, 2.6815600395202637, 2.3885133266448975, 2.3868367671966553, 2.7165253162384033]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, loss: 2.5172054767608643\n",
      "Losses [2.649017810821533, 2.40610933303833, 2.958345413208008, 2.4575672149658203, 2.483642101287842, 2.4349617958068848, 2.593290328979492, 2.1786680221557617, 2.5805554389953613, 1.9856452941894531, 2.6384212970733643, 2.8153953552246094, 2.2629811763763428, 2.4187827110290527, 2.285438060760498, 2.5313704013824463, 2.8448896408081055, 2.181893825531006, 2.2966392040252686, 2.395402193069458, 2.054990291595459, 2.1033847332000732, 2.3847367763519287, 3.023655891418457, 2.708109140396118, 2.1038265228271484, 2.188149929046631, 2.09616756439209, 2.59128475189209, 2.2837016582489014, 2.964162588119507, 2.606764793395996, 2.5687222480773926, 2.5310845375061035, 2.7108168601989746, 2.3946828842163086, 3.228548526763916, 2.5349626541137695, 2.5806286334991455, 2.4759886264801025, 2.612321376800537, 2.3945844173431396, 2.434431552886963, 2.2688815593719482, 2.3063206672668457, 2.66890811920166, 2.688891887664795, 2.3681259155273438, 2.5625240802764893, 2.310473918914795, 2.3785252571105957, 2.449807643890381, 2.009638547897339, 2.547891616821289, 2.171701431274414, 2.066735029220581, 2.5533077716827393, 2.5262091159820557, 2.6935276985168457, 2.265955924987793, 2.4660751819610596, 2.899867057800293, 2.76100754737854, 2.5956780910491943, 2.200486183166504, 2.689462900161743, 2.968997001647949, 2.856311798095703, 2.682634115219116, 2.37443470954895, 2.2963290214538574, 2.748403787612915, 2.205920457839966, 2.7175331115722656, 2.4917943477630615, 2.598601818084717, 2.6119608879089355, 2.8324778079986572, 3.040071964263916, 2.3731274604797363, 2.3723316192626953, 2.535278797149658, 2.260242462158203, 2.7171213626861572, 2.3657984733581543, 2.377904176712036, 2.1905288696289062, 2.7269158363342285, 2.730649471282959, 2.7910850048065186, 2.3372488021850586, 2.484313488006592, 2.9680938720703125, 2.7534070014953613, 2.3305342197418213, 2.6662118434906006, 2.6659350395202637, 2.4376673698425293, 2.4346799850463867, 2.5771665573120117, 2.554309844970703, 2.7863383293151855, 2.8615896701812744, 2.4316534996032715, 2.6970059871673584, 2.3839111328125, 2.1984145641326904, 2.5881550312042236, 2.4365367889404297, 2.3670949935913086, 2.704648733139038, 2.3274855613708496, 2.4942731857299805, 2.9616003036499023, 2.5403940677642822, 2.8751442432403564, 2.3925836086273193, 2.2908573150634766, 2.6466362476348877, 2.5795886516571045]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, loss: 2.477229595184326\n",
      "Losses [2.7855052947998047, 2.371513605117798, 2.4260759353637695, 2.2942824363708496, 2.2319350242614746, 2.501021146774292, 2.4860575199127197, 2.605243682861328, 2.4216320514678955, 2.3592240810394287, 2.0556681156158447, 2.918083429336548, 2.3327841758728027, 2.70353364944458, 2.432361602783203, 2.3133840560913086, 2.545891761779785, 1.9198367595672607, 2.7289321422576904, 2.3734636306762695, 2.225738286972046, 2.379091501235962, 2.4818334579467773, 3.0642471313476562, 2.5073049068450928, 2.5481929779052734, 2.1568102836608887, 2.3709986209869385, 2.4839017391204834, 2.1277174949645996, 2.753335475921631, 2.926626443862915, 2.303392171859741, 2.638794422149658, 2.854881525039673, 2.5995097160339355, 2.5200083255767822, 2.478512763977051, 2.3202319145202637, 2.186305046081543, 2.4039876461029053, 2.4562182426452637, 2.5304605960845947, 2.257284641265869, 2.225947618484497, 2.6056036949157715, 2.2520532608032227, 2.3814666271209717, 2.4618964195251465, 2.2707600593566895, 2.29961895942688, 2.154571056365967, 2.0736868381500244, 2.301244020462036, 2.3000075817108154, 2.1969051361083984, 2.710811138153076, 2.618992805480957, 2.402419328689575, 2.3195345401763916, 1.978204607963562, 2.36214280128479, 3.3931469917297363, 2.5946435928344727, 2.0693931579589844, 2.2390530109405518, 2.938971757888794, 2.638500690460205, 2.7613956928253174, 2.746342897415161, 2.254734516143799, 2.943018913269043, 2.007612466812134, 2.6410040855407715, 2.233421802520752, 2.450455904006958, 2.546645402908325, 2.592829704284668, 2.8377463817596436, 2.329392433166504, 2.6226720809936523, 2.6901519298553467, 2.3535170555114746, 2.7225940227508545, 2.250828742980957, 2.143085479736328, 2.0947866439819336, 2.558433771133423, 2.502941608428955, 3.3321449756622314, 2.1933672428131104, 1.919072151184082, 2.9213123321533203, 2.3812713623046875, 2.263072967529297, 3.0410990715026855, 2.580526351928711, 2.3335814476013184, 2.2743921279907227, 2.3878467082977295, 2.8250834941864014, 2.6578454971313477, 2.361100673675537, 2.41849946975708, 2.352086305618286, 2.4540135860443115, 2.8811655044555664, 2.597470760345459, 2.290449857711792, 2.1908857822418213, 3.2500672340393066, 2.501174211502075, 2.25480318069458, 2.572348117828369, 2.6131319999694824, 2.680333375930786, 2.6621479988098145, 2.727250814437866, 2.4345288276672363, 2.4604549407958984]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, loss: 2.49756121635437\n",
      "Losses [2.7260212898254395, 2.1985416412353516, 2.4240481853485107, 2.452897310256958, 2.5426530838012695, 2.5813193321228027, 2.648458957672119, 2.379106283187866, 2.380958080291748, 2.3079028129577637, 2.118537664413452, 2.7460427284240723, 2.2571451663970947, 2.801093578338623, 2.3734679222106934, 2.418710708618164, 2.5044467449188232, 2.2668416500091553, 2.7063536643981934, 2.2910337448120117, 2.335296630859375, 2.4868249893188477, 2.498459577560425, 2.7109336853027344, 2.466167688369751, 2.3999483585357666, 2.166904926300049, 2.1405234336853027, 2.45021653175354, 2.487978458404541, 2.9549808502197266, 3.1018104553222656, 2.53924822807312, 2.7565526962280273, 2.4993319511413574, 2.392704963684082, 2.2469139099121094, 2.334315776824951, 2.2722361087799072, 1.9808053970336914, 2.598773241043091, 2.4295711517333984, 2.471177339553833, 2.411205768585205, 2.526516914367676, 2.5151095390319824, 2.265958547592163, 2.3574953079223633, 2.606248378753662, 2.367975950241089, 2.562814474105835, 2.383350133895874, 2.146839141845703, 2.2258377075195312, 2.348902940750122, 2.2403156757354736, 3.306501626968384, 2.43924617767334, 2.429262638092041, 2.5179948806762695, 2.4815797805786133, 2.4302051067352295, 3.317462682723999, 2.43027925491333, 2.374993324279785, 2.5764665603637695, 2.4702301025390625, 2.6235060691833496, 2.477703094482422, 2.573042392730713, 2.3329763412475586, 2.6077163219451904, 2.321697950363159, 2.903010845184326, 2.319943904876709, 2.3259434700012207, 2.577319622039795, 2.7996556758880615, 2.9599761962890625, 2.0834877490997314, 2.538026809692383, 2.4919986724853516, 2.2650306224823, 3.041912794113159, 2.2358906269073486, 2.1636104583740234, 1.9920039176940918, 2.864438056945801, 2.409782886505127, 3.019721031188965, 2.2066593170166016, 2.1947214603424072, 3.0560572147369385, 2.411432981491089, 2.385713815689087, 2.8190715312957764, 2.6896352767944336, 2.4047722816467285, 2.288220167160034, 2.387744903564453, 2.8746726512908936, 2.7983031272888184, 2.358067035675049, 2.601816177368164, 2.60513973236084, 2.34364652633667, 2.327530860900879, 2.8241961002349854, 2.1859853267669678, 2.2799742221832275, 2.7431254386901855, 2.6008832454681396, 2.128995895385742, 2.7029812335968018, 3.039243221282959, 2.5477185249328613, 2.7338013648986816, 2.7125048637390137, 2.3852710723876953, 2.56099796295166]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, loss: 2.452986240386963\n",
      "Losses [2.5574886798858643, 2.5504817962646484, 2.466244697570801, 2.361182928085327, 2.2464511394500732, 2.4595136642456055, 2.3607406616210938, 2.117069721221924, 2.425652503967285, 2.152669668197632, 1.982121467590332, 2.9789390563964844, 2.124408006668091, 2.7468669414520264, 2.133892059326172, 2.3064370155334473, 2.3283960819244385, 2.185594081878662, 2.694215774536133, 2.3875226974487305, 2.128687620162964, 2.399564027786255, 2.210613250732422, 2.430924892425537, 2.689302682876587, 2.1302003860473633, 2.201016426086426, 2.2105584144592285, 2.890221118927002, 2.6019954681396484, 2.869192600250244, 2.5090396404266357, 2.5791540145874023, 2.4278621673583984, 2.6346867084503174, 2.5193593502044678, 2.735973596572876, 2.279233694076538, 2.1792306900024414, 2.013214588165283, 2.424215316772461, 2.7915797233581543, 2.326896905899048, 2.407046318054199, 2.059946298599243, 2.3637447357177734, 2.1024763584136963, 2.4533607959747314, 2.263824939727783, 2.1758484840393066, 2.231891632080078, 2.13794207572937, 2.515970468521118, 2.5017473697662354, 2.4227216243743896, 2.2163524627685547, 2.7232768535614014, 2.481950283050537, 2.54246187210083, 2.4385571479797363, 2.3852698802948, 2.4140217304229736, 3.1423287391662598, 2.3333470821380615, 2.20596981048584, 2.755500316619873, 2.722764492034912, 2.6071910858154297, 2.656233310699463, 2.6109302043914795, 2.1990528106689453, 2.559929847717285, 2.310429096221924, 2.717708110809326, 2.2153306007385254, 2.309522867202759, 2.2355446815490723, 2.6345653533935547, 2.6217150688171387, 2.1977951526641846, 2.4248099327087402, 2.568084955215454, 2.5299415588378906, 2.8085920810699463, 2.210055351257324, 2.1394190788269043, 2.1926281452178955, 2.5570945739746094, 2.265470027923584, 2.792206287384033, 2.789114236831665, 2.17067813873291, 2.5322844982147217, 2.445805072784424, 2.1236252784729004, 2.7558276653289795, 2.771376132965088, 2.1657471656799316, 2.1213600635528564, 2.295656442642212, 2.768880844116211, 2.720311403274536, 2.5463383197784424, 2.461327075958252, 2.5825610160827637, 2.441535234451294, 2.5629382133483887, 2.665786027908325, 2.4388177394866943, 2.075742483139038, 3.0214178562164307, 2.165198802947998, 2.3005285263061523, 2.5690293312072754, 3.1049423217773438, 2.675027847290039, 2.64652943611145, 2.598060369491577, 2.9553537368774414, 2.775329113006592]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, loss: 2.4376211166381836\n",
      "Losses [2.62164306640625, 2.175971508026123, 2.5083138942718506, 2.3420891761779785, 2.4489827156066895, 2.3712985515594482, 2.394505023956299, 2.205413818359375, 2.5762503147125244, 2.4545233249664307, 2.193166732788086, 2.3580968379974365, 2.2020745277404785, 2.6235408782958984, 2.3721823692321777, 2.1947686672210693, 2.2691867351531982, 2.2019450664520264, 2.598097324371338, 2.332505702972412, 2.066635847091675, 2.1859421730041504, 2.5151236057281494, 2.8647594451904297, 2.568601131439209, 2.3224668502807617, 2.3083882331848145, 2.348057270050049, 2.910511016845703, 2.5910239219665527, 2.633537769317627, 2.621532917022705, 2.5728678703308105, 2.65988826751709, 2.3549606800079346, 2.4954802989959717, 2.454957962036133, 2.2209911346435547, 2.4663150310516357, 2.0379719734191895, 2.278048515319824, 2.4872541427612305, 2.209066867828369, 2.1147923469543457, 2.2295854091644287, 2.387836456298828, 2.581613540649414, 2.328462839126587, 2.593538999557495, 2.1764516830444336, 2.4700450897216797, 1.756931185722351, 2.3536219596862793, 2.363223075866699, 2.247840642929077, 2.018188238143921, 2.737778425216675, 2.651499032974243, 2.2650983333587646, 2.3362553119659424, 2.3027987480163574, 2.503786087036133, 2.922511100769043, 2.4472334384918213, 2.382378578186035, 2.412736415863037, 2.5906660556793213, 2.8318071365356445, 2.3578073978424072, 2.7295358180999756, 2.2623701095581055, 2.9011733531951904, 2.2897768020629883, 2.23193097114563, 2.123145341873169, 2.504415512084961, 2.448732614517212, 2.6615259647369385, 2.931340217590332, 2.158926486968994, 2.7308859825134277, 2.32201886177063, 2.2199230194091797, 2.6195833683013916, 2.1513257026672363, 2.109483480453491, 1.8566209077835083, 2.7695581912994385, 2.2970170974731445, 2.5400805473327637, 2.3395352363586426, 2.5599887371063232, 2.718837022781372, 2.318535566329956, 2.162212371826172, 2.623943567276001, 3.0592281818389893, 2.1245498657226562, 2.268286943435669, 2.80682373046875, 2.8681299686431885, 2.58040714263916, 2.8398470878601074, 2.4946069717407227, 2.318856716156006, 2.2056193351745605, 2.154808759689331, 2.4515256881713867, 2.2290425300598145, 2.4430370330810547, 2.71085524559021, 2.096998929977417, 2.1159844398498535, 2.5331294536590576, 2.7252495288848877, 3.034586191177368, 3.047645092010498, 2.571913480758667, 2.5626306533813477, 2.7048983573913574]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, loss: 2.4120588302612305\n",
      "Losses [2.686161518096924, 2.1665444374084473, 2.068446636199951, 2.447434425354004, 2.1164822578430176, 2.326284885406494, 2.8608078956604004, 2.1560678482055664, 2.314302921295166, 2.1290907859802246, 2.013777017593384, 2.8033761978149414, 2.1143527030944824, 2.456226348876953, 2.2522735595703125, 2.1851654052734375, 2.2799320220947266, 1.8955893516540527, 2.661043167114258, 2.2329823970794678, 2.3329875469207764, 2.0442495346069336, 2.496410369873047, 2.4254143238067627, 2.428884744644165, 2.3836684226989746, 1.9129948616027832, 2.1159825325012207, 2.6291937828063965, 2.160266637802124, 2.5817437171936035, 2.6806955337524414, 2.316883087158203, 2.2753231525421143, 2.211623191833496, 2.328022003173828, 2.5484113693237305, 2.24843692779541, 2.2246813774108887, 2.304356098175049, 2.1868820190429688, 2.4529688358306885, 2.457186460494995, 2.5024890899658203, 2.1648037433624268, 2.9826836585998535, 2.101393461227417, 2.1587135791778564, 2.3846869468688965, 1.9866015911102295, 2.265437602996826, 1.8815510272979736, 1.8821125030517578, 2.608104705810547, 2.0572943687438965, 2.671757698059082, 2.5579183101654053, 2.7191548347473145, 2.779693126678467, 2.161719799041748, 2.7594761848449707, 2.6014671325683594, 2.9881889820098877, 2.5050241947174072, 2.0857739448547363, 2.505518913269043, 2.3148670196533203, 2.5377981662750244, 2.506218910217285, 2.641411304473877, 2.3517165184020996, 2.7837939262390137, 2.2019500732421875, 2.306243419647217, 2.181917667388916, 2.0793964862823486, 2.377366304397583, 2.508044481277466, 2.7435355186462402, 1.9763424396514893, 2.5516154766082764, 2.77988338470459, 2.0154964923858643, 2.9928243160247803, 2.48237943649292, 2.0842275619506836, 2.0776467323303223, 2.7691166400909424, 2.2506020069122314, 2.4453301429748535, 2.375775098800659, 2.1052443981170654, 2.667978286743164, 2.584084987640381, 2.13330078125, 2.8152658939361572, 2.878568410873413, 2.5174739360809326, 2.628600597381592, 2.34233021736145, 2.5141115188598633, 2.7053685188293457, 2.714529037475586, 2.754992961883545, 2.457563638687134, 2.1402618885040283, 2.3967959880828857, 2.5408613681793213, 2.279600143432617, 2.0311343669891357, 2.482736349105835, 2.3733513355255127, 2.235764265060425, 2.880152702331543, 2.986163854598999, 2.7847161293029785, 2.7376327514648438, 2.510063648223877, 2.8493528366088867, 2.8384251594543457]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, loss: 2.390683174133301\n",
      "Losses [2.5678749084472656, 2.299727201461792, 2.5864059925079346, 2.269726037979126, 2.2986268997192383, 2.3534512519836426, 2.55181884765625, 2.2304091453552246, 2.736872911453247, 2.335381269454956, 2.00746750831604, 2.7696080207824707, 2.215702772140503, 2.6773955821990967, 2.2790746688842773, 2.0038022994995117, 2.55303955078125, 1.9495949745178223, 2.2477402687072754, 2.2744901180267334, 2.225564956665039, 2.122783899307251, 2.1847918033599854, 2.729092597961426, 2.655118703842163, 1.928307056427002, 2.0151329040527344, 2.2670986652374268, 2.4126367568969727, 2.272772789001465, 2.6654958724975586, 2.4440481662750244, 2.3966593742370605, 2.507870674133301, 2.345689296722412, 2.3810932636260986, 2.4581432342529297, 1.8365793228149414, 2.171679973602295, 1.7940199375152588, 2.619983434677124, 2.23736310005188, 2.0466606616973877, 2.1575021743774414, 2.6373085975646973, 2.469805955886841, 2.27943754196167, 2.1779298782348633, 2.1766364574432373, 2.093677043914795, 2.4351089000701904, 2.62168550491333, 2.5897369384765625, 2.4880099296569824, 2.1415443420410156, 1.8516907691955566, 2.4604029655456543, 2.15705943107605, 2.841269016265869, 2.4050581455230713, 2.6118905544281006, 2.602283477783203, 2.7553906440734863, 2.276453733444214, 2.1810367107391357, 2.667466402053833, 2.242309331893921, 2.7741475105285645, 2.779325008392334, 2.5789361000061035, 1.978623867034912, 3.060884952545166, 2.0038139820098877, 2.7455546855926514, 2.2399845123291016, 2.466423511505127, 2.2519619464874268, 2.934786319732666, 2.7327957153320312, 2.095860481262207, 2.7006430625915527, 2.5419998168945312, 2.2356584072113037, 2.676459312438965, 2.37052583694458, 2.1686811447143555, 1.9672383069992065, 2.5266947746276855, 2.2215170860290527, 2.870469808578491, 2.4020442962646484, 2.075087070465088, 2.6247873306274414, 2.1645822525024414, 1.8594679832458496, 2.410365104675293, 2.865264654159546, 2.5673487186431885, 2.446622371673584, 2.150007724761963, 2.655395030975342, 2.620803117752075, 2.6467344760894775, 2.3735125064849854, 2.4801392555236816, 2.3455963134765625, 2.500694513320923, 2.625589370727539, 2.210522174835205, 2.1404943466186523, 3.0031659603118896, 2.1469595432281494, 1.940176248550415, 2.6551902294158936, 2.8977904319763184, 2.2802038192749023, 2.3103387355804443, 2.582139015197754, 2.2843079566955566, 2.4461758136749268]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, loss: 2.3848233222961426\n",
      "Losses [3.2977700233459473, 2.297795295715332, 2.1286001205444336, 2.402513265609741, 2.160956859588623, 2.4216647148132324, 2.3201162815093994, 1.9444366693496704, 2.273001194000244, 2.1288528442382812, 2.2794766426086426, 2.570544481277466, 2.441664457321167, 2.59842586517334, 2.132251024246216, 2.176056385040283, 2.178302049636841, 1.7892982959747314, 2.733877658843994, 2.139626979827881, 2.855013847351074, 2.2213642597198486, 1.9864076375961304, 2.7252490520477295, 2.524768829345703, 1.8636043071746826, 1.881680965423584, 2.04061222076416, 2.4629266262054443, 2.2873244285583496, 2.3604276180267334, 2.4651877880096436, 2.7257487773895264, 2.2877297401428223, 2.404865264892578, 2.397928237915039, 2.4754996299743652, 2.3550283908843994, 2.2649765014648438, 1.8979331254959106, 2.2829227447509766, 2.331632375717163, 2.187875747680664, 2.2971200942993164, 2.303493022918701, 2.2241036891937256, 2.045424461364746, 2.1621551513671875, 2.3740415573120117, 2.161794424057007, 2.4204254150390625, 1.9538464546203613, 2.6347157955169678, 2.337479591369629, 2.050278663635254, 2.359861135482788, 2.9393935203552246, 2.2442429065704346, 2.249659299850464, 2.182849168777466, 2.2991766929626465, 2.8346447944641113, 2.806519031524658, 2.3380799293518066, 2.6083145141601562, 2.8687961101531982, 2.464714765548706, 2.5209341049194336, 2.46132230758667, 2.706859827041626, 2.165597915649414, 2.9183719158172607, 2.0412020683288574, 2.4660825729370117, 2.363502025604248, 2.2716832160949707, 2.0828099250793457, 2.7363040447235107, 2.7146189212799072, 1.948082685470581, 2.723071575164795, 2.2953884601593018, 1.9513341188430786, 3.1186740398406982, 2.156632900238037, 2.1053967475891113, 1.8526220321655273, 2.8990328311920166, 2.866255283355713, 2.7309374809265137, 2.331606388092041, 2.076834201812744, 2.8591036796569824, 2.5667521953582764, 2.1695683002471924, 2.489473342895508, 2.532465934753418, 2.296875476837158, 2.0773696899414062, 2.4618306159973145, 2.2749428749084473, 2.459719181060791, 2.6245527267456055, 2.5490753650665283, 2.4124393463134766, 2.2173023223876953, 2.095094680786133, 2.6037540435791016, 2.1172266006469727, 2.130795955657959, 2.4400579929351807, 1.912543773651123, 2.454054117202759, 2.6125926971435547, 3.227532386779785, 2.9957735538482666, 2.538651704788208, 3.1020357608795166, 2.394202709197998, 2.194796562194824]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, loss: 2.367060899734497\n",
      "Losses [2.751924991607666, 2.1015238761901855, 2.264651298522949, 2.3895819187164307, 2.245710611343384, 2.4808952808380127, 2.560558319091797, 2.3132665157318115, 2.3128576278686523, 1.9330031871795654, 1.9731144905090332, 2.676266670227051, 2.1724390983581543, 2.8388571739196777, 2.2365505695343018, 2.037337303161621, 2.3868417739868164, 1.8411107063293457, 2.3994548320770264, 2.3488636016845703, 1.8018202781677246, 2.293588638305664, 1.960949182510376, 2.6688599586486816, 2.3485450744628906, 1.9855833053588867, 2.228804111480713, 2.1068596839904785, 2.469770669937134, 2.510061740875244, 2.665889024734497, 2.7531304359436035, 2.3719053268432617, 2.0927016735076904, 2.3112380504608154, 1.8918476104736328, 2.5567941665649414, 1.8628451824188232, 1.9342975616455078, 1.932403564453125, 2.302607536315918, 2.5851387977600098, 1.9867055416107178, 2.233185291290283, 2.05003023147583, 2.3336005210876465, 2.229238748550415, 2.2378950119018555, 2.0752787590026855, 2.137723684310913, 2.612743854522705, 2.0526390075683594, 1.9110420942306519, 2.3806686401367188, 2.377110004425049, 1.8436336517333984, 2.6278254985809326, 2.4057137966156006, 2.6257643699645996, 2.220492362976074, 2.6431565284729004, 2.7707457542419434, 2.8818018436431885, 2.5899875164031982, 2.1583409309387207, 2.32313871383667, 2.7537105083465576, 2.467703342437744, 2.341477394104004, 2.3567821979522705, 2.2036631107330322, 2.6725902557373047, 2.3386104106903076, 2.9154646396636963, 2.177712917327881, 2.345883846282959, 2.220698833465576, 2.515310287475586, 2.92539644241333, 2.074674367904663, 2.6020374298095703, 2.6150851249694824, 2.1974823474884033, 2.7095603942871094, 2.2304372787475586, 2.0086047649383545, 2.2782578468322754, 2.682295322418213, 2.209754705429077, 2.701359510421753, 2.507558584213257, 2.2764761447906494, 2.868527889251709, 2.374208450317383, 2.2253148555755615, 2.429636001586914, 2.51241135597229, 2.174345016479492, 2.1683998107910156, 2.172442674636841, 2.7164671421051025, 2.5753672122955322, 2.461310863494873, 2.5037999153137207, 2.3542120456695557, 2.228645086288452, 2.0606963634490967, 2.6674623489379883, 2.4777939319610596, 2.005084991455078, 3.1738810539245605, 2.0004425048828125, 2.117629289627075, 2.5568528175354004, 2.7850332260131836, 2.5334320068359375, 2.941092014312744, 2.6175742149353027, 2.9057602882385254, 2.501967191696167]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:10<00:00, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, loss: 2.3314285278320312\n",
      "Losses [2.757357358932495, 2.3198869228363037, 2.3301868438720703, 2.105756998062134, 2.1424505710601807, 2.479609966278076, 2.5505518913269043, 2.0671072006225586, 2.059544324874878, 1.826061725616455, 2.0257980823516846, 3.1022558212280273, 2.0048749446868896, 2.6278316974639893, 2.038682460784912, 2.3142154216766357, 2.4011483192443848, 2.1719818115234375, 2.5211877822875977, 2.1791791915893555, 1.9793322086334229, 2.078101873397827, 2.112499713897705, 2.270103931427002, 2.4766812324523926, 2.275151491165161, 2.0060205459594727, 2.5007858276367188, 2.4270355701446533, 2.1952567100524902, 2.640820026397705, 2.702566146850586, 2.4646894931793213, 2.0377578735351562, 2.437744379043579, 2.361577272415161, 2.3505077362060547, 2.023777961730957, 2.03017520904541, 1.878673791885376, 2.092095375061035, 3.316777229309082, 2.374136447906494, 2.255547285079956, 1.9288136959075928, 2.296382188796997, 2.574056625366211, 2.246173858642578, 2.282146692276001, 2.1532294750213623, 2.1337239742279053, 1.9403111934661865, 2.109027862548828, 2.128706932067871, 2.2493364810943604, 2.0738887786865234, 2.2603018283843994, 2.4451942443847656, 2.3653974533081055, 2.298933744430542, 2.778904914855957, 2.943258047103882, 2.7325692176818848, 2.09224796295166, 2.5640532970428467, 2.1063120365142822, 2.554184675216675, 2.4197826385498047, 2.4208645820617676, 2.4837191104888916, 2.066837787628174, 2.9206862449645996, 1.8940808773040771, 2.3426265716552734, 2.190114974975586, 2.1161327362060547, 2.1018714904785156, 2.3238165378570557, 2.7400121688842773, 1.822432518005371, 2.57342791557312, 2.4025821685791016, 2.298772096633911, 2.598700523376465, 2.327333927154541, 1.9047755002975464, 2.0479111671447754, 2.500507116317749, 2.2136809825897217, 2.5073022842407227, 2.7541980743408203, 2.072784423828125, 2.580962896347046, 2.261446952819824, 2.0578365325927734, 2.6246917247772217, 2.4026527404785156, 2.2592856884002686, 2.169989585876465, 2.2987477779388428, 2.489003896713257, 2.577439785003662, 2.316795587539673, 2.142465353012085, 2.3376247882843018, 2.2571215629577637, 2.366889715194702, 2.4703316688537598, 2.3331596851348877, 2.0508131980895996, 2.6826720237731934, 2.133732318878174, 1.9447524547576904, 2.5131709575653076, 3.1611156463623047, 2.415647029876709, 3.014289379119873, 2.306364059448242, 2.598004102706909, 2.3888158798217773]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:10<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, loss: 2.3047127723693848\n",
      "Losses [2.715297222137451, 2.1832921504974365, 2.230015993118286, 2.1808207035064697, 2.0945348739624023, 2.6949288845062256, 3.098728656768799, 2.1109657287597656, 2.1861839294433594, 2.042839527130127, 1.841050148010254, 2.6770193576812744, 2.0923755168914795, 2.7517402172088623, 2.2770121097564697, 2.014504909515381, 2.2955403327941895, 1.6851797103881836, 2.4439663887023926, 2.030555486679077, 2.0820066928863525, 1.837900161743164, 2.139676094055176, 2.7609212398529053, 2.3092403411865234, 1.9794446229934692, 1.9717025756835938, 1.823206901550293, 2.398892879486084, 2.143543243408203, 2.5644655227661133, 2.5306334495544434, 2.3532931804656982, 2.2206430435180664, 2.2850863933563232, 2.11296010017395, 2.26796817779541, 1.8263893127441406, 1.9930129051208496, 1.6643390655517578, 1.9138896465301514, 2.4252612590789795, 2.314427614212036, 2.2357635498046875, 1.7105040550231934, 2.0828516483306885, 1.9041626453399658, 2.211092472076416, 1.9311240911483765, 1.8512849807739258, 2.39801025390625, 2.010892391204834, 2.796658992767334, 2.0567824840545654, 1.7594565153121948, 2.2835564613342285, 2.831904411315918, 2.413661241531372, 2.336674690246582, 2.2508909702301025, 1.876693844795227, 2.834286689758301, 3.4159598350524902, 2.2780423164367676, 1.8659543991088867, 2.350205421447754, 2.434983491897583, 2.5318150520324707, 2.2209272384643555, 2.646533966064453, 2.053457736968994, 2.8509345054626465, 2.160229206085205, 2.4343225955963135, 2.6099185943603516, 2.365121603012085, 2.289905548095703, 2.7074339389801025, 2.4924733638763428, 2.088749647140503, 2.2722866535186768, 2.9864351749420166, 2.0678114891052246, 2.6867053508758545, 2.093073844909668, 1.771127462387085, 1.7240971326828003, 2.7106821537017822, 1.9905757904052734, 2.66247296333313, 2.4766290187835693, 2.0463898181915283, 2.7995309829711914, 2.177947521209717, 2.349255084991455, 2.332365036010742, 2.720761775970459, 2.1922426223754883, 2.0764245986938477, 2.3462066650390625, 2.4652864933013916, 2.564291477203369, 2.4640867710113525, 2.4373199939727783, 2.191718101501465, 2.0837204456329346, 2.1655478477478027, 2.7835631370544434, 1.931567907333374, 1.9663398265838623, 3.562368631362915, 1.9889936447143555, 2.0358266830444336, 3.145096778869629, 3.0910120010375977, 2.2869789600372314, 2.550291061401367, 2.2905101776123047, 2.5173521041870117, 2.421933174133301]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:09<00:00, 12.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, loss: 2.300511598587036\n",
      "Losses [2.7883827686309814, 2.1250288486480713, 2.5206480026245117, 2.463980197906494, 1.959113597869873, 2.3444247245788574, 2.328101873397827, 2.2106728553771973, 2.450039863586426, 2.0417299270629883, 1.8133907318115234, 2.6542890071868896, 1.9374346733093262, 2.774535655975342, 1.9397151470184326, 2.0586814880371094, 2.5984549522399902, 2.064669609069824, 2.660753011703491, 2.323810577392578, 1.8745503425598145, 1.8692831993103027, 2.052159309387207, 2.958265542984009, 2.2840216159820557, 1.8214857578277588, 1.7888915538787842, 1.8428361415863037, 2.049453020095825, 2.3722219467163086, 3.485842704772949, 2.8806281089782715, 2.3901381492614746, 2.6370623111724854, 2.2754034996032715, 2.0976076126098633, 1.9731249809265137, 1.8105762004852295, 2.1374480724334717, 1.7571771144866943, 2.4155192375183105, 2.736812114715576, 2.5160789489746094, 2.0285134315490723, 1.9638195037841797, 2.3086719512939453, 1.9733318090438843, 2.0892961025238037, 2.0067105293273926, 2.077788829803467, 2.2409157752990723, 2.060589075088501, 2.421623468399048, 2.17462420463562, 2.494535446166992, 2.2232918739318848, 2.387789249420166, 2.61906361579895, 2.5095314979553223, 2.303196907043457, 2.709320545196533, 2.7198500633239746, 3.1043882369995117, 2.5275230407714844, 2.092776298522949, 2.3597488403320312, 2.420644760131836, 2.3836913108825684, 2.1543145179748535, 2.429553508758545, 2.2749929428100586, 2.968268632888794, 1.891554832458496, 2.514157772064209, 2.0384936332702637, 2.078817844390869, 2.4989945888519287, 2.4588701725006104, 2.644838571548462, 1.7662863731384277, 2.4573910236358643, 2.474586009979248, 1.945472240447998, 2.9347591400146484, 2.292027235031128, 1.9420289993286133, 1.7408702373504639, 2.4096150398254395, 2.079117774963379, 2.4213168621063232, 2.3601481914520264, 1.8600349426269531, 2.3774120807647705, 2.0049405097961426, 1.883484959602356, 2.5239977836608887, 2.3253183364868164, 1.8039765357971191, 2.043731927871704, 1.958409070968628, 2.4284372329711914, 2.2926909923553467, 2.340049982070923, 2.1610991954803467, 2.0126757621765137, 2.3801751136779785, 2.391850471496582, 2.479032516479492, 2.0148420333862305, 2.494929790496826, 2.687047004699707, 1.8535442352294922, 2.218380928039551, 2.4379687309265137, 2.806488037109375, 3.215618133544922, 2.4615049362182617, 2.583956241607666, 2.2110202312469482, 2.7466001510620117]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "can = CrossAttentionNetwork().cuda()\n",
    "criterion = SpecialCrossEntropyLoss()\n",
    "optimizer_params = {\n",
    "    \"lr\": 0.1,\n",
    "}\n",
    "optimizer = SGD(can.parameters(), **optimizer_params, )\n",
    "epochs = 25\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for episode_samples, episode_labels in tqdm(ds_train):\n",
    "        optimizer.zero_grad()\n",
    "        # print(episode_samples.shape)\n",
    "        # print(episode_labels.shape)\n",
    "        support, query = split_support_query(episode_samples, n_way, n_shots)\n",
    "        support: torch.Tensor = support.cuda()\n",
    "        y_support = episode_labels[:n_way * n_shots].unsqueeze(0)\n",
    "        y_support_ohe: torch.Tensor = F.one_hot(y_support, n_way).float().cuda()\n",
    "\n",
    "        y_query = episode_labels[n_way * n_shots:].unsqueeze(0)\n",
    "        y_query_ohe: torch.Tensor =  F.one_hot(y_query,n_way).float().cuda()\n",
    "        query: torch.Tensor = query.cuda()\n",
    "\n",
    "        # print(\"SUP:\", support.shape)\n",
    "        # print(\"QUERY:\", query.shape)\n",
    "        # print(\"Y_SUP:\", y_support_ohe.shape)\n",
    "        # print(\"Y_QUERY:\", y_query_ohe.shape)\n",
    "\n",
    "        y_pred, cls_score = can(support, query, y_support_ohe, y_query_ohe)\n",
    "        # print(\"y_pred\", y_pred.shape, \"cls_score\", cls_score.shape)\n",
    "        loss1 = criterion(y_pred, y_query.view(-1))\n",
    "        loss2 = criterion(cls_score, y_query.view(-1))\n",
    "        # print(\"l1:\", loss1, \"l2: \", loss2.item)\n",
    "        loss = loss1 + 0.5 * loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch+1}, loss: {torch.tensor(losses).mean()}\")\n",
    "    print(\"Losses\", losses)\n",
    "        # break\n",
    "\n",
    "#         loss = criterion(y_pred, y_query_ohe)\n",
    "\n",
    "# can_warmed_up = CrossAttentionNetwork()\n",
    "# can_warmed_up.load_state_dict(can.state_dict())\n",
    "# criterion_warmed_up = CANLoss(can_warmed_up, n_classes=n_way)\n",
    "# criterion_warmed_up.load_state_dict(criterion.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6, 3, 3]) torch.Size([6, 6, 3, 3])\n",
      "tensor([[[0.6723, 0.6892, 0.6877],\n",
      "         [0.6556, 0.6629, 0.6731],\n",
      "         [0.6663, 0.6526, 0.6691]],\n",
      "\n",
      "        [[0.6274, 0.6571, 0.6813],\n",
      "         [0.6301, 0.6362, 0.6410],\n",
      "         [0.6410, 0.6144, 0.6337]],\n",
      "\n",
      "        [[0.6694, 0.6798, 0.7027],\n",
      "         [0.6486, 0.6566, 0.6589],\n",
      "         [0.6495, 0.6457, 0.6690]],\n",
      "\n",
      "        [[0.5005, 0.5307, 0.5705],\n",
      "         [0.5281, 0.5810, 0.5521],\n",
      "         [0.5444, 0.5394, 0.5345]],\n",
      "\n",
      "        [[0.6406, 0.6689, 0.6833],\n",
      "         [0.6351, 0.6397, 0.6448],\n",
      "         [0.6366, 0.6207, 0.6429]],\n",
      "\n",
      "        [[0.5691, 0.6060, 0.6315],\n",
      "         [0.5913, 0.6170, 0.6109],\n",
      "         [0.6117, 0.5997, 0.5970]]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape, cls_score.shape)\n",
    "print(cls_score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:14<00:00,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 198.50894165039062\n",
      "Losses [192.42591857910156, 237.9314422607422, 237.76531982421875, 245.19598388671875, 221.25787353515625, 236.29574584960938, 239.2960968017578, 289.90252685546875, 234.09927368164062, 219.93170166015625, 228.55364990234375, 350.619873046875, 185.3072967529297, 324.6642761230469, 256.7618408203125, 210.46917724609375, 268.48870849609375, 251.00247192382812, 301.4913330078125, 205.4044952392578, 210.89247131347656, 223.46502685546875, 184.7008514404297, 223.15823364257812, 219.7668914794922, 250.71609497070312, 214.39227294921875, 212.18272399902344, 245.47213745117188, 215.8321533203125, 192.01109313964844, 196.920166015625, 247.02711486816406, 205.81088256835938, 184.94223022460938, 197.227783203125, 194.1817626953125, 183.76893615722656, 195.86712646484375, 233.53994750976562, 205.96414184570312, 185.79127502441406, 193.08319091796875, 187.62911987304688, 189.32125854492188, 181.9144287109375, 183.29896545410156, 191.9380340576172, 182.1277313232422, 181.13812255859375, 179.456298828125, 180.37554931640625, 180.21878051757812, 183.67501831054688, 192.23779296875, 181.87063598632812, 188.20672607421875, 194.891845703125, 180.18368530273438, 184.64503479003906, 182.23243713378906, 195.01377868652344, 179.2401580810547, 179.32530212402344, 180.80491638183594, 179.26980590820312, 181.70068359375, 194.9708709716797, 179.15390014648438, 179.01905822753906, 185.7069091796875, 180.58436584472656, 179.88612365722656, 178.3203887939453, 183.3856201171875, 178.76919555664062, 180.73202514648438, 181.17300415039062, 181.90634155273438, 180.0982666015625, 179.73281860351562, 179.22100830078125, 180.59637451171875, 179.39181518554688, 178.92787170410156, 189.4513702392578, 185.60360717773438, 192.0589141845703, 181.42140197753906, 189.5312957763672, 180.1661834716797, 179.51976013183594, 180.97451782226562, 180.15699768066406, 191.73178100585938, 183.77427673339844, 184.63629150390625, 179.69955444335938, 180.82127380371094, 181.88230895996094, 180.9064483642578, 178.44874572753906, 178.83758544921875, 180.28851318359375, 179.62156677246094, 185.70602416992188, 181.10397338867188, 179.04806518554688, 180.65020751953125, 180.2208251953125, 179.90306091308594, 178.77273559570312, 180.65310668945312, 196.202880859375, 181.51852416992188, 185.6227264404297, 179.83493041992188, 178.52108764648438, 178.32913208007812, 179.58148193359375]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:15<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss: 178.96128845214844\n",
      "Losses [179.9770050048828, 178.4154052734375, 178.51812744140625, 179.78636169433594, 179.15972900390625, 179.83270263671875, 178.6851806640625, 178.74208068847656, 178.71200561523438, 178.9686279296875, 180.3282012939453, 178.48582458496094, 178.55201721191406, 179.2447509765625, 179.7429962158203, 179.1844940185547, 178.46884155273438, 178.6881103515625, 178.9486083984375, 178.95611572265625, 178.5220184326172, 178.22509765625, 178.27886962890625, 179.30364990234375, 178.673095703125, 181.0144805908203, 179.02105712890625, 178.48541259765625, 178.2066650390625, 178.20907592773438, 180.08395385742188, 184.88461303710938, 178.8894500732422, 178.96788024902344, 198.44859313964844, 178.3725128173828, 181.89874267578125, 178.21633911132812, 178.33876037597656, 178.2165069580078, 181.2938995361328, 178.37440490722656, 179.03564453125, 178.23912048339844, 180.42269897460938, 178.78286743164062, 178.5854949951172, 178.0987548828125, 178.1309051513672, 178.30360412597656, 178.54624938964844, 179.04124450683594, 178.37957763671875, 178.61453247070312, 180.04776000976562, 178.733642578125, 178.22837829589844, 178.61048889160156, 178.3326416015625, 178.18539428710938, 178.39132690429688, 179.2935791015625, 179.59423828125, 178.50245666503906, 178.2156982421875, 178.25418090820312, 178.3082733154297, 179.9906463623047, 178.22174072265625, 178.3811492919922, 178.1256866455078, 178.23301696777344, 178.2792205810547, 178.24050903320312, 178.82772827148438, 178.39556884765625, 178.35696411132812, 178.52890014648438, 178.1020050048828, 179.17233276367188, 178.4506378173828, 178.44924926757812, 181.754150390625, 179.38546752929688, 180.55755615234375, 178.95640563964844, 178.05194091796875, 178.90135192871094, 178.08287048339844, 178.21713256835938, 178.24549865722656, 178.8462677001953, 178.6871337890625, 178.8936767578125, 178.58148193359375, 179.01495361328125, 178.40948486328125, 178.24896240234375, 178.34393310546875, 178.24945068359375, 178.97854614257812, 178.3135223388672, 178.1591796875, 178.18487548828125, 178.2848358154297, 178.12405395507812, 178.2755126953125, 178.66348266601562, 178.2867431640625, 178.24261474609375, 178.52108764648438, 178.227294921875, 178.06982421875, 178.1605987548828, 178.078369140625, 178.1973114013672, 178.11618041992188, 178.25643920898438, 178.65476989746094, 178.17581176757812]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:14<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss: 178.22218322753906\n",
      "Losses [178.44183349609375, 178.35025024414062, 178.119873046875, 178.07376098632812, 178.24285888671875, 178.13284301757812, 178.43304443359375, 178.27415466308594, 178.18087768554688, 178.1523895263672, 178.25372314453125, 178.42291259765625, 178.26458740234375, 178.30197143554688, 178.06936645507812, 178.13427734375, 178.21983337402344, 180.2061767578125, 178.59146118164062, 178.23013305664062, 178.17788696289062, 178.19503784179688, 178.11444091796875, 178.81100463867188, 178.6336669921875, 178.43423461914062, 178.28765869140625, 178.2894287109375, 178.15841674804688, 178.54345703125, 178.18060302734375, 178.09329223632812, 178.24998474121094, 178.4127197265625, 178.19825744628906, 178.2690887451172, 178.50747680664062, 178.16970825195312, 178.25875854492188, 178.5123291015625, 178.17185974121094, 178.14859008789062, 178.07310485839844, 178.15000915527344, 178.26168823242188, 178.13995361328125, 178.3119354248047, 178.17269897460938, 178.10939025878906, 178.17095947265625, 178.1505584716797, 178.14083862304688, 178.0926513671875, 178.24063110351562, 178.1951904296875, 178.03317260742188, 178.08041381835938, 178.12831115722656, 178.06658935546875, 178.07196044921875, 178.06544494628906, 178.1390380859375, 178.2811279296875, 178.24191284179688, 178.21682739257812, 178.15658569335938, 178.08409118652344, 178.52297973632812, 178.10911560058594, 178.13963317871094, 178.0806884765625, 178.269775390625, 178.13717651367188, 178.20358276367188, 178.23190307617188, 178.6715087890625, 178.12637329101562, 178.09872436523438, 178.12411499023438, 178.08541870117188, 178.17916870117188, 178.0561065673828, 178.0909423828125, 178.14120483398438, 178.07644653320312, 178.1263427734375, 178.0500030517578, 178.23733520507812, 178.08682250976562, 178.1259765625, 178.07504272460938, 178.20541381835938, 178.17103576660156, 178.07057189941406, 178.24180603027344, 178.08534240722656, 178.1927947998047, 178.17666625976562, 178.153564453125, 178.0430450439453, 178.14242553710938, 178.09841918945312, 178.059814453125, 178.14404296875, 178.1865234375, 178.03726196289062, 178.17819213867188, 178.25181579589844, 178.021484375, 178.0731201171875, 178.06591796875, 178.18740844726562, 178.06985473632812, 178.32330322265625, 178.1103515625, 178.28192138671875, 178.22222900390625, 178.09349060058594, 178.27963256835938, 178.75875854492188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:15<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss: 178.12826538085938\n",
      "Losses [178.28614807128906, 178.24887084960938, 178.21029663085938, 178.38241577148438, 178.19439697265625, 178.160888671875, 178.0693817138672, 178.229248046875, 178.079345703125, 178.07008361816406, 178.122314453125, 178.1374969482422, 178.17190551757812, 178.09617614746094, 178.05642700195312, 178.10360717773438, 178.08572387695312, 178.1634979248047, 178.05758666992188, 178.2420196533203, 178.1058807373047, 178.00912475585938, 178.10423278808594, 178.08236694335938, 178.07772827148438, 178.43399047851562, 178.1163330078125, 178.06285095214844, 178.07208251953125, 178.16546630859375, 178.08241271972656, 178.06143188476562, 178.1833953857422, 178.13658142089844, 178.11056518554688, 178.14163208007812, 178.0637969970703, 178.01687622070312, 178.1009521484375, 178.12664794921875, 178.23680114746094, 178.13040161132812, 178.06930541992188, 178.06741333007812, 178.17758178710938, 178.17440795898438, 178.12779235839844, 178.04026794433594, 178.15652465820312, 178.10833740234375, 178.08004760742188, 178.03842163085938, 178.1186981201172, 178.1110076904297, 178.18743896484375, 178.20733642578125, 178.09780883789062, 178.03445434570312, 178.0738525390625, 178.15234375, 178.05679321289062, 178.36761474609375, 178.06773376464844, 178.1963653564453, 178.1304931640625, 178.0574188232422, 178.1136016845703, 178.06622314453125, 178.1083221435547, 178.0740966796875, 178.11473083496094, 178.11184692382812, 178.10064697265625, 178.1311798095703, 178.1520233154297, 178.1130828857422, 178.07424926757812, 178.14981079101562, 178.0161590576172, 178.07508850097656, 178.16015625, 178.07334899902344, 178.09219360351562, 178.1416778564453, 178.07479858398438, 178.0871124267578, 178.06201171875, 178.1411590576172, 178.1647186279297, 178.03131103515625, 178.07614135742188, 178.21859741210938, 178.12161254882812, 178.0291748046875, 178.08685302734375, 178.40740966796875, 178.27337646484375, 178.18052673339844, 178.13949584960938, 178.01025390625, 178.10488891601562, 178.08782958984375, 178.08114624023438, 178.06765747070312, 178.10423278808594, 178.03343200683594, 178.08993530273438, 178.08975219726562, 178.07867431640625, 178.13504028320312, 178.10430908203125, 178.09027099609375, 178.0348663330078, 178.2879638671875, 178.1195831298828, 178.4647674560547, 178.23623657226562, 178.11068725585938, 178.24021911621094, 178.16937255859375]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:15<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss: 178.2189483642578\n",
      "Losses [178.1849365234375, 178.146728515625, 178.4124755859375, 178.08981323242188, 178.15037536621094, 178.5340576171875, 178.4835205078125, 178.28326416015625, 178.05093383789062, 178.14266967773438, 178.09918212890625, 178.0738525390625, 178.10702514648438, 178.02452087402344, 178.03013610839844, 178.0419921875, 178.18804931640625, 178.73690795898438, 178.0683135986328, 178.17666625976562, 178.0782928466797, 178.12489318847656, 178.0885467529297, 178.32574462890625, 182.28189086914062, 178.07423400878906, 178.2489471435547, 178.11976623535156, 180.00213623046875, 178.04736328125, 178.15045166015625, 178.14170837402344, 181.87420654296875, 178.13809204101562, 178.0547332763672, 178.260498046875, 178.73336791992188, 178.1239471435547, 178.0979461669922, 178.15142822265625, 178.42800903320312, 178.651611328125, 178.14544677734375, 178.12960815429688, 178.13845825195312, 178.10079956054688, 178.0794677734375, 178.25469970703125, 178.0694580078125, 178.10142517089844, 178.0511474609375, 178.1342315673828, 178.04147338867188, 178.14361572265625, 178.20431518554688, 178.0146484375, 178.07901000976562, 178.05294799804688, 178.07887268066406, 178.0384979248047, 178.03672790527344, 178.38739013671875, 178.12786865234375, 178.104736328125, 178.03761291503906, 178.06796264648438, 178.01934814453125, 178.17318725585938, 178.02345275878906, 178.31202697753906, 178.11184692382812, 178.05905151367188, 178.032470703125, 178.1837615966797, 178.18121337890625, 178.11935424804688, 178.1162109375, 178.05316162109375, 178.2254638671875, 178.07382202148438, 178.15354919433594, 178.05209350585938, 178.04690551757812, 178.11383056640625, 178.0350799560547, 178.099609375, 178.0563507080078, 178.15228271484375, 178.08767700195312, 178.10348510742188, 178.05328369140625, 178.06727600097656, 178.1145477294922, 178.0235137939453, 178.1998291015625, 178.07046508789062, 178.13888549804688, 178.07852172851562, 178.14036560058594, 178.01553344726562, 178.0504913330078, 178.16827392578125, 178.09254455566406, 178.0904541015625, 178.11219787597656, 178.0474395751953, 178.29934692382812, 178.04415893554688, 178.0231170654297, 178.0888671875, 178.20408630371094, 178.14321899414062, 178.0459442138672, 178.10140991210938, 178.06556701660156, 178.12648010253906, 178.13198852539062, 178.07577514648438, 177.9978485107422, 178.0327911376953]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:15<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss: 178.08486938476562\n",
      "Losses [178.06622314453125, 178.36410522460938, 178.03855895996094, 178.02328491210938, 178.13650512695312, 178.02191162109375, 178.06134033203125, 178.57675170898438, 178.02066040039062, 178.0683135986328, 178.04769897460938, 178.09536743164062, 178.09713745117188, 178.0325164794922, 178.02362060546875, 178.0115203857422, 178.08526611328125, 178.02764892578125, 178.06126403808594, 178.20162963867188, 178.01541137695312, 178.02110290527344, 178.01771545410156, 178.11619567871094, 178.06082153320312, 178.09234619140625, 178.03701782226562, 178.07545471191406, 178.0655975341797, 178.02365112304688, 178.02456665039062, 178.0282440185547, 178.167236328125, 178.01654052734375, 178.14459228515625, 178.07798767089844, 178.28176879882812, 178.04025268554688, 178.03575134277344, 178.12579345703125, 178.064208984375, 178.5859375, 178.02877807617188, 178.0803680419922, 178.17039489746094, 178.03765869140625, 178.1770782470703, 178.36717224121094, 178.04147338867188, 178.05616760253906, 178.0660400390625, 178.05543518066406, 178.0879364013672, 178.0711669921875, 178.14910888671875, 178.0113525390625, 178.0635528564453, 178.11312866210938, 178.0509490966797, 178.0270233154297, 178.04263305664062, 178.14358520507812, 178.0619354248047, 178.1559295654297, 178.10406494140625, 178.01812744140625, 178.2488250732422, 178.04635620117188, 178.00506591796875, 178.13272094726562, 178.012939453125, 178.05613708496094, 178.04556274414062, 178.0899200439453, 178.16549682617188, 178.03688049316406, 178.0957794189453, 178.05368041992188, 178.0355224609375, 178.03903198242188, 178.08712768554688, 178.0584259033203, 178.03778076171875, 178.04666137695312, 178.040771484375, 178.02920532226562, 177.9976043701172, 178.18190002441406, 178.03271484375, 178.032470703125, 178.0933837890625, 178.08502197265625, 178.08316040039062, 178.00733947753906, 178.0637664794922, 178.02642822265625, 178.05484008789062, 178.1151580810547, 178.03131103515625, 178.03309631347656, 178.12887573242188, 178.0596466064453, 178.06671142578125, 178.04299926757812, 178.1384735107422, 178.027587890625, 178.0697479248047, 178.09567260742188, 178.17897033691406, 178.13674926757812, 178.02957153320312, 178.10719299316406, 178.00686645507812, 178.05384826660156, 178.05410766601562, 178.0422821044922, 178.12669372558594, 178.12539672851562, 178.01296997070312, 178.11605834960938]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:14<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss: 178.0553741455078\n",
      "Losses [178.01602172851562, 178.0426483154297, 178.0181884765625, 178.0244140625, 178.17259216308594, 178.01890563964844, 178.07223510742188, 178.05618286132812, 178.02392578125, 178.04611206054688, 178.0930938720703, 178.05905151367188, 178.06546020507812, 178.00119018554688, 178.008056640625, 178.05926513671875, 178.10516357421875, 178.05599975585938, 178.05276489257812, 178.14199829101562, 178.05291748046875, 178.0447235107422, 178.00531005859375, 178.07980346679688, 178.0530242919922, 178.1043243408203, 178.09873962402344, 178.10182189941406, 178.02943420410156, 178.05014038085938, 178.08108520507812, 178.04452514648438, 178.06100463867188, 178.07272338867188, 178.0284881591797, 178.090576171875, 178.03323364257812, 178.11328125, 178.0310516357422, 178.04515075683594, 178.04299926757812, 178.0757293701172, 178.0349884033203, 178.0308380126953, 178.0506591796875, 178.02655029296875, 178.12307739257812, 178.03260803222656, 178.09786987304688, 178.0279998779297, 178.02540588378906, 178.05731201171875, 178.0495147705078, 178.169921875, 178.12130737304688, 177.9937286376953, 178.043212890625, 178.0379638671875, 178.0263671875, 178.00804138183594, 178.03414916992188, 178.12583923339844, 178.10386657714844, 178.1570281982422, 178.0460968017578, 178.05157470703125, 178.0598907470703, 178.0562744140625, 178.04000854492188, 178.02662658691406, 178.0147247314453, 178.0331268310547, 178.08670043945312, 178.06248474121094, 178.13075256347656, 178.05572509765625, 178.05963134765625, 178.06069946289062, 178.0252685546875, 178.02932739257812, 178.03048706054688, 178.04678344726562, 178.02926635742188, 178.09738159179688, 178.09298706054688, 178.0504150390625, 178.02047729492188, 178.0390625, 178.01864624023438, 178.05136108398438, 178.11618041992188, 178.03916931152344, 178.04209899902344, 178.0240478515625, 178.03536987304688, 178.082763671875, 178.01052856445312, 178.001220703125, 178.0724334716797, 178.01341247558594, 178.08938598632812, 178.0785369873047, 178.02481079101562, 178.031005859375, 178.0959014892578, 178.06146240234375, 178.02381896972656, 178.03399658203125, 178.01171875, 178.039306640625, 178.00836181640625, 178.0867919921875, 178.0129852294922, 178.04507446289062, 178.02578735351562, 178.0819091796875, 178.1174774169922, 178.0369873046875, 178.0615234375, 178.07427978515625]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:15<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 178.06346130371094\n",
      "Losses [178.03433227539062, 178.09771728515625, 178.00180053710938, 178.0249786376953, 178.095947265625, 178.1306915283203, 178.0175323486328, 178.045654296875, 178.03109741210938, 178.0653839111328, 178.04714965820312, 178.0985107421875, 178.00100708007812, 178.0218048095703, 177.9994659423828, 178.03109741210938, 178.01319885253906, 178.03353881835938, 178.2552490234375, 178.25344848632812, 178.04896545410156, 177.99526977539062, 178.0371551513672, 178.23907470703125, 178.02967834472656, 178.0779571533203, 178.04722595214844, 178.064208984375, 178.06130981445312, 178.0089111328125, 178.00421142578125, 178.05343627929688, 178.08084106445312, 178.03268432617188, 178.0548858642578, 178.1356201171875, 178.06460571289062, 178.05772399902344, 178.05001831054688, 178.04190063476562, 178.051513671875, 178.0622100830078, 178.02886962890625, 178.02880859375, 178.06785583496094, 178.06170654296875, 178.07408142089844, 178.0369110107422, 178.05572509765625, 178.02561950683594, 178.02590942382812, 178.0381622314453, 178.024658203125, 178.10206604003906, 178.0566864013672, 178.01258850097656, 178.0629119873047, 178.03103637695312, 178.01168823242188, 178.0255889892578, 178.0326690673828, 178.20343017578125, 178.19424438476562, 178.12249755859375, 178.03195190429688, 178.06842041015625, 178.05027770996094, 178.0497283935547, 178.00425720214844, 178.029052734375, 178.10572814941406, 178.07315063476562, 178.03355407714844, 178.1401824951172, 178.0784149169922, 178.10409545898438, 178.05438232421875, 178.03054809570312, 178.01483154296875, 178.02700805664062, 178.13870239257812, 178.0402069091797, 178.047607421875, 178.0484619140625, 178.1641387939453, 178.06298828125, 178.03440856933594, 178.08201599121094, 178.00440979003906, 178.03590393066406, 178.34536743164062, 178.0644073486328, 178.08163452148438, 178.01219177246094, 178.0963134765625, 178.01393127441406, 178.0242919921875, 178.02969360351562, 178.0829315185547, 177.9990234375, 178.0258026123047, 178.0518035888672, 178.08404541015625, 178.0164794921875, 178.134765625, 178.0443878173828, 178.051513671875, 178.1485595703125, 178.01263427734375, 178.0196533203125, 178.05052185058594, 178.0877685546875, 178.0162353515625, 178.0462646484375, 178.0363006591797, 178.0797119140625, 178.14727783203125, 178.19061279296875, 178.07754516601562, 178.06845092773438]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:15<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, loss: 178.0452117919922\n",
      "Losses [178.01480102539062, 178.04432678222656, 178.02935791015625, 177.99264526367188, 178.07205200195312, 178.05343627929688, 178.0538787841797, 178.07432556152344, 178.04615783691406, 178.07736206054688, 178.05311584472656, 178.04226684570312, 178.03492736816406, 177.99124145507812, 178.01351928710938, 178.03269958496094, 178.03759765625, 178.0446319580078, 178.02395629882812, 178.00473022460938, 178.02102661132812, 178.0048828125, 178.1102294921875, 178.03880310058594, 178.0186004638672, 178.0362548828125, 178.043212890625, 178.022705078125, 178.00222778320312, 178.06289672851562, 178.01412963867188, 178.024169921875, 178.16273498535156, 178.02081298828125, 178.01226806640625, 178.0391387939453, 178.02029418945312, 178.0433807373047, 178.02053833007812, 178.0191192626953, 177.997314453125, 178.04519653320312, 178.0301971435547, 178.0805206298828, 178.04190063476562, 178.02972412109375, 178.08657836914062, 178.00050354003906, 178.0264892578125, 178.05776977539062, 178.06781005859375, 178.0132293701172, 178.0669708251953, 178.0576171875, 177.997802734375, 178.03871154785156, 178.02354431152344, 178.04522705078125, 178.0686492919922, 178.01141357421875, 178.03004455566406, 178.06417846679688, 178.07044982910156, 178.0771942138672, 178.00625610351562, 178.04286193847656, 178.03585815429688, 178.02078247070312, 178.01551818847656, 178.01043701171875, 178.02943420410156, 178.04917907714844, 178.01393127441406, 178.05751037597656, 178.03207397460938, 178.02047729492188, 178.01925659179688, 178.06895446777344, 178.0029296875, 178.02816772460938, 178.02310180664062, 178.048828125, 178.03262329101562, 178.04185485839844, 178.12452697753906, 178.08786010742188, 178.0009002685547, 178.0360107421875, 178.00656127929688, 178.01556396484375, 178.02227783203125, 178.05276489257812, 178.06849670410156, 178.01065063476562, 178.01219177246094, 178.06842041015625, 178.01907348632812, 178.0884246826172, 178.10006713867188, 178.02340698242188, 178.0341033935547, 178.12588500976562, 178.02259826660156, 178.39227294921875, 178.224853515625, 178.01246643066406, 178.01483154296875, 178.10882568359375, 177.99932861328125, 178.0484161376953, 178.09315490722656, 178.12081909179688, 178.02139282226562, 178.00790405273438, 178.05722045898438, 178.03219604492188, 178.17330932617188, 178.05189514160156, 178.003662109375, 178.04104614257812]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:15<00:00,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss: 178.05142211914062\n",
      "Losses [178.04055786132812, 178.05270385742188, 177.99264526367188, 177.99847412109375, 178.34024047851562, 178.0435791015625, 178.0194854736328, 178.1002960205078, 177.99256896972656, 178.07032775878906, 178.13406372070312, 178.11837768554688, 178.02764892578125, 178.10238647460938, 178.00669860839844, 178.041748046875, 178.07769775390625, 178.11422729492188, 178.06370544433594, 178.05690002441406, 178.10110473632812, 178.0052490234375, 178.0106201171875, 178.05084228515625, 178.00030517578125, 178.07171630859375, 178.0263214111328, 178.04588317871094, 178.00881958007812, 178.01597595214844, 178.0190887451172, 178.01168823242188, 178.119873046875, 178.03311157226562, 178.04054260253906, 178.1046142578125, 178.0120849609375, 178.01341247558594, 178.0101776123047, 178.07696533203125, 178.03794860839844, 178.0901336669922, 178.0034637451172, 178.0672607421875, 178.11033630371094, 178.00221252441406, 178.06492614746094, 178.0002899169922, 178.06298828125, 178.037353515625, 178.00762939453125, 178.0159149169922, 178.06214904785156, 178.11215209960938, 178.04566955566406, 178.00537109375, 178.03152465820312, 178.02732849121094, 178.01116943359375, 177.9999542236328, 178.02955627441406, 178.04530334472656, 178.15281677246094, 178.10641479492188, 178.0419158935547, 178.03475952148438, 178.0135955810547, 178.05079650878906, 178.01904296875, 178.10140991210938, 178.07525634765625, 178.02133178710938, 178.00436401367188, 178.0748291015625, 178.09933471679688, 178.01173400878906, 178.01144409179688, 178.02764892578125, 178.00262451171875, 178.01568603515625, 178.04855346679688, 178.05604553222656, 177.99720764160156, 178.04830932617188, 178.06381225585938, 178.00099182128906, 177.99142456054688, 178.0762939453125, 177.99063110351562, 178.0151824951172, 178.01968383789062, 178.05648803710938, 178.0524139404297, 178.00863647460938, 178.03079223632812, 178.01205444335938, 178.00613403320312, 178.0168914794922, 178.05615234375, 177.99559020996094, 178.0426788330078, 178.05885314941406, 178.011962890625, 178.09512329101562, 178.19882202148438, 178.01254272460938, 178.02198791503906, 178.2084197998047, 178.01583862304688, 178.03973388671875, 178.03768920898438, 178.03468322753906, 178.2294921875, 178.1561737060547, 178.06138610839844, 178.07191467285156, 178.09014892578125, 178.05758666992188, 178.08554077148438, 178.1868896484375]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can = CrossAttentionNetwork().cuda()\n",
    "criterion = CANLoss(can, n_classes=n_way).cuda()\n",
    "optimizer_params = {\n",
    "    \"lr\": 0.001,\n",
    "}\n",
    "optimizer = SGD(criterion.parameters(), **optimizer_params, )\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for episode_samples, episode_labels in tqdm(ds_train):\n",
    "        optimizer.zero_grad()\n",
    "        # print(episode_samples.shape)\n",
    "        # print(episode_labels.shape)\n",
    "        y_true: torch.Tensor = episode_labels[n_way * n_shots:].unsqueeze(-1).cuda()\n",
    "        support, query = split_support_query(episode_samples, n_way, n_shots)\n",
    "        support: torch.Tensor = support.cuda()\n",
    "        query: torch.Tensor = query.cuda()\n",
    "\n",
    "        # print(support.shape)\n",
    "        # print(query.shape)\n",
    "        # print(y_true.shape)\n",
    "\n",
    "        loss = criterion(support, query, y_true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch+1}, loss: {torch.tensor(losses).mean()}\")\n",
    "    print(\"Losses\", losses)\n",
    "\n",
    "can_warmed_up = CrossAttentionNetwork()\n",
    "can_warmed_up.load_state_dict(can.state_dict())\n",
    "criterion_warmed_up = CANLoss(can_warmed_up, n_classes=n_way)\n",
    "criterion_warmed_up.load_state_dict(criterion.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "can.load_state_dict(can_warmed_up.state_dict())\n",
    "criterion.load_state_dict(criterion_warmed_up.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:15<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 178.0390625\n",
      "Losses [178.0507049560547, 178.0585174560547, 178.0208740234375, 178.004150390625, 178.07373046875, 178.0234832763672, 178.04244995117188, 178.07427978515625, 178.02801513671875, 178.05892944335938, 178.058837890625, 178.01821899414062, 178.01966857910156, 177.99734497070312, 177.99365234375, 178.03753662109375, 178.0151824951172, 178.03021240234375, 177.99514770507812, 178.02157592773438, 178.07846069335938, 177.9918212890625, 178.06390380859375, 178.0248260498047, 178.0069580078125, 178.07432556152344, 178.0235595703125, 178.06692504882812, 178.04476928710938, 178.00929260253906, 178.039794921875, 178.04180908203125, 178.15969848632812, 178.040771484375, 178.07904052734375, 178.13258361816406, 178.0528564453125, 178.0921630859375, 178.01751708984375, 178.02383422851562, 178.05450439453125, 178.04824829101562, 178.04150390625, 178.0391845703125, 178.11485290527344, 178.02041625976562, 178.0462646484375, 177.999755859375, 178.0139617919922, 178.04159545898438, 178.03018188476562, 178.0113983154297, 177.99620056152344, 178.06991577148438, 178.0135040283203, 177.9942626953125, 178.0449981689453, 178.04107666015625, 178.0509490966797, 178.0023956298828, 178.06967163085938, 178.0541534423828, 178.08468627929688, 178.0745086669922, 178.01730346679688, 178.0115509033203, 178.02174377441406, 178.0243377685547, 178.03997802734375, 178.0196990966797, 177.99868774414062, 178.03921508789062, 178.041259765625, 178.0791015625, 178.04776000976562, 178.01255798339844, 178.0177001953125, 178.0343017578125, 177.99256896972656, 178.0133056640625, 178.0404052734375, 178.06930541992188, 178.04583740234375, 178.03396606445312, 178.06443786621094, 178.0869140625, 177.99057006835938, 178.02764892578125, 177.9923095703125, 178.03253173828125, 178.00180053710938, 178.0787353515625, 178.0634765625, 178.0010528564453, 178.0546875, 178.03453063964844, 177.9994354248047, 178.02005004882812, 178.0192108154297, 177.99639892578125, 178.02586364746094, 178.027587890625, 178.0142822265625, 178.05381774902344, 178.09364318847656, 178.014892578125, 178.01353454589844, 178.1193084716797, 177.99755859375, 178.00921630859375, 178.04324340820312, 178.04759216308594, 178.00733947753906, 178.0333251953125, 178.10531616210938, 178.03753662109375, 178.13966369628906, 178.07701110839844, 178.05352783203125, 178.06317138671875]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:15<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss: 178.03428649902344\n",
      "Losses [177.99905395507812, 178.03048706054688, 177.99755859375, 178.0045928955078, 178.0302734375, 178.01617431640625, 178.0039520263672, 178.032470703125, 178.02511596679688, 178.03909301757812, 178.0143585205078, 178.08926391601562, 178.0113525390625, 177.986572265625, 178.0303955078125, 178.05819702148438, 178.00157165527344, 178.0264434814453, 178.0155792236328, 178.00390625, 178.060791015625, 177.99758911132812, 178.01333618164062, 178.025634765625, 178.03009033203125, 178.04598999023438, 178.01791381835938, 178.00894165039062, 178.00205993652344, 178.0091552734375, 178.01889038085938, 178.0022430419922, 178.09632873535156, 178.000244140625, 178.03701782226562, 178.06353759765625, 178.0583953857422, 178.061767578125, 178.02517700195312, 177.99606323242188, 178.08358764648438, 178.02828979492188, 178.00985717773438, 178.03439331054688, 178.05552673339844, 178.01019287109375, 178.0233917236328, 177.99880981445312, 178.0674591064453, 178.06016540527344, 178.01087951660156, 178.02435302734375, 178.01026916503906, 178.0889892578125, 178.0054168701172, 177.9916534423828, 178.02627563476562, 178.0252227783203, 178.01681518554688, 178.001220703125, 178.03900146484375, 178.02056884765625, 178.10153198242188, 178.043212890625, 178.0164794921875, 178.0010986328125, 178.0050048828125, 178.01473999023438, 177.990966796875, 178.01766967773438, 178.00201416015625, 178.0225830078125, 178.00714111328125, 178.01373291015625, 178.01187133789062, 178.03018188476562, 178.01351928710938, 178.05105590820312, 177.99325561523438, 178.0308837890625, 178.06320190429688, 178.00799560546875, 178.03085327148438, 178.0422821044922, 178.0148468017578, 178.01449584960938, 177.9967041015625, 178.01910400390625, 177.98971557617188, 178.03440856933594, 178.05380249023438, 178.3677215576172, 178.059814453125, 178.0090789794922, 178.2732391357422, 178.0196533203125, 178.0215606689453, 177.99269104003906, 178.0828094482422, 178.00238037109375, 178.03839111328125, 178.04702758789062, 178.0004119873047, 178.13742065429688, 178.2105712890625, 178.0097198486328, 178.01870727539062, 178.1400146484375, 177.99362182617188, 178.03189086914062, 178.10145568847656, 178.07827758789062, 177.99449157714844, 178.00132751464844, 178.04052734375, 178.01187133789062, 178.05694580078125, 178.026611328125, 178.01150512695312, 178.08001708984375]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:15<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss: 178.032958984375\n",
      "Losses [178.01939392089844, 178.0391387939453, 178.0081024169922, 177.98912048339844, 178.06321716308594, 178.0206756591797, 178.07723999023438, 178.05242919921875, 177.99896240234375, 178.0080108642578, 178.007080078125, 178.02183532714844, 177.9876708984375, 177.9996795654297, 178.01683044433594, 178.01173400878906, 178.00526428222656, 178.06887817382812, 178.0416259765625, 178.03289794921875, 178.014892578125, 177.99920654296875, 178.03236389160156, 178.0393524169922, 178.01548767089844, 178.0250244140625, 178.02011108398438, 178.02838134765625, 178.00375366210938, 177.9990234375, 178.0015411376953, 178.02505493164062, 178.05372619628906, 178.01791381835938, 178.01771545410156, 178.0804443359375, 178.02932739257812, 178.10008239746094, 178.00634765625, 178.05892944335938, 178.04116821289062, 178.00689697265625, 177.9995880126953, 178.10060119628906, 178.01513671875, 178.0260772705078, 178.0406951904297, 177.99420166015625, 178.03497314453125, 178.010986328125, 178.0311279296875, 178.01971435546875, 178.03175354003906, 178.05377197265625, 178.10939025878906, 178.01571655273438, 178.03367614746094, 178.04409790039062, 178.0128173828125, 178.03866577148438, 177.99468994140625, 178.0374298095703, 178.02589416503906, 178.05795288085938, 178.02049255371094, 178.07334899902344, 178.018798828125, 178.0094757080078, 178.00685119628906, 178.08734130859375, 178.03948974609375, 178.0465087890625, 178.0263214111328, 178.01087951660156, 178.02975463867188, 178.02088928222656, 178.0159149169922, 178.0606689453125, 177.9913330078125, 178.06216430664062, 178.052001953125, 178.03997802734375, 178.02737426757812, 178.10462951660156, 178.00836181640625, 178.04367065429688, 178.00643920898438, 178.0516815185547, 177.98660278320312, 177.9951629638672, 178.02359008789062, 178.0649871826172, 178.0413360595703, 178.02099609375, 178.06256103515625, 178.01614379882812, 178.0208282470703, 177.9945068359375, 178.0255126953125, 177.9981689453125, 178.0823516845703, 178.03485107421875, 178.0382843017578, 178.0047607421875, 178.18495178222656, 178.02252197265625, 178.00042724609375, 178.04823303222656, 177.98953247070312, 177.99615478515625, 178.05343627929688, 178.07130432128906, 177.98876953125, 178.01402282714844, 178.0004425048828, 178.0103759765625, 178.1246337890625, 178.07374572753906, 178.12380981445312, 178.173095703125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:15<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss: 178.03604125976562\n",
      "Losses [178.01934814453125, 178.09927368164062, 178.02386474609375, 178.01766967773438, 178.09771728515625, 178.03875732421875, 178.01162719726562, 178.01547241210938, 178.27243041992188, 178.20339965820312, 178.06414794921875, 178.05276489257812, 177.99771118164062, 178.01939392089844, 178.002685546875, 178.02053833007812, 178.0089874267578, 178.108642578125, 178.05978393554688, 178.072509765625, 178.04019165039062, 177.99012756347656, 178.0199737548828, 178.01742553710938, 178.0102081298828, 178.050537109375, 178.02297973632812, 178.02301025390625, 178.02316284179688, 178.01611328125, 178.00082397460938, 178.0130615234375, 178.13841247558594, 178.0157470703125, 178.02218627929688, 178.04873657226562, 178.04183959960938, 178.10888671875, 178.02593994140625, 178.00680541992188, 178.03671264648438, 178.00698852539062, 178.01513671875, 178.0296173095703, 178.01339721679688, 177.99191284179688, 178.08612060546875, 177.98971557617188, 178.0002899169922, 178.03866577148438, 178.02638244628906, 178.02865600585938, 178.00387573242188, 178.0552215576172, 178.02182006835938, 178.01205444335938, 178.0191650390625, 178.03314208984375, 178.05477905273438, 178.00279235839844, 178.00453186035156, 178.05264282226562, 178.0946502685547, 178.04769897460938, 178.00570678710938, 178.02284240722656, 178.05712890625, 178.02699279785156, 178.0057830810547, 178.17276000976562, 178.0084686279297, 178.04136657714844, 178.09336853027344, 178.05026245117188, 178.04994201660156, 178.1928253173828, 177.999755859375, 178.021484375, 177.9899139404297, 178.03851318359375, 178.12742614746094, 178.03489685058594, 178.00869750976562, 178.0236053466797, 178.00698852539062, 178.04861450195312, 177.99191284179688, 178.00592041015625, 177.99949645996094, 178.01608276367188, 178.029296875, 178.06600952148438, 178.04470825195312, 178.00599670410156, 178.05807495117188, 177.99908447265625, 178.0148162841797, 177.996826171875, 178.03529357910156, 178.0157470703125, 178.07086181640625, 178.00827026367188, 178.02520751953125, 178.02410888671875, 178.02581787109375, 178.00186157226562, 178.05165100097656, 178.06942749023438, 177.99209594726562, 178.01065063476562, 178.03414916992188, 178.0416717529297, 177.9897918701172, 178.00875854492188, 178.0274658203125, 178.00363159179688, 178.06362915039062, 178.0056915283203, 178.01641845703125, 178.03634643554688]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:15<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss: 178.02891540527344\n",
      "Losses [178.02244567871094, 178.04339599609375, 177.9982452392578, 178.00428771972656, 178.10362243652344, 178.00045776367188, 178.0092315673828, 178.01449584960938, 178.0089111328125, 178.05386352539062, 178.1006317138672, 178.033203125, 178.00628662109375, 178.0438690185547, 178.005859375, 178.0050048828125, 178.01333618164062, 178.09552001953125, 178.0707550048828, 178.00494384765625, 178.09432983398438, 177.99240112304688, 178.02536010742188, 178.0741729736328, 178.00010681152344, 178.0247344970703, 178.01055908203125, 178.00555419921875, 178.01121520996094, 178.0437774658203, 178.00674438476562, 177.99868774414062, 178.1166229248047, 178.01077270507812, 178.00669860839844, 178.0201416015625, 178.07403564453125, 178.01133728027344, 178.0244140625, 177.99472045898438, 178.12522888183594, 178.0327911376953, 178.00076293945312, 178.0194091796875, 178.01089477539062, 178.0269317626953, 177.9929962158203, 177.9996337890625, 177.9952392578125, 177.99351501464844, 177.99853515625, 177.99423217773438, 177.98794555664062, 178.06439208984375, 178.025146484375, 177.99063110351562, 178.01449584960938, 178.02366638183594, 178.003173828125, 178.0008087158203, 178.029052734375, 178.04547119140625, 178.04086303710938, 178.04446411132812, 178.0056610107422, 178.0045166015625, 178.02783203125, 178.04931640625, 177.98971557617188, 178.03402709960938, 178.00660705566406, 178.4185333251953, 178.04405212402344, 178.07449340820312, 178.05026245117188, 178.0103302001953, 178.0062713623047, 178.01461791992188, 177.9900665283203, 178.01556396484375, 178.23309326171875, 178.04183959960938, 178.04983520507812, 178.02069091796875, 178.002197265625, 178.01834106445312, 177.99903869628906, 178.0388641357422, 177.9947509765625, 178.01995849609375, 177.9913787841797, 178.0786895751953, 178.03738403320312, 178.001953125, 178.01507568359375, 178.006103515625, 177.99208068847656, 177.99627685546875, 178.01199340820312, 177.99459838867188, 178.0223388671875, 178.02732849121094, 178.0064697265625, 178.02310180664062, 178.06919860839844, 177.99871826171875, 177.99346923828125, 178.04983520507812, 177.9903564453125, 178.03695678710938, 178.0443115234375, 178.08285522460938, 178.00985717773438, 178.0457763671875, 178.0313262939453, 178.03451538085938, 178.03187561035156, 178.00222778320312, 177.9888916015625, 178.05056762695312]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:16<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss: 178.02854919433594\n",
      "Losses [178.0059814453125, 178.0265655517578, 177.99801635742188, 177.99319458007812, 178.05038452148438, 178.00070190429688, 177.99301147460938, 178.05169677734375, 178.00172424316406, 178.06591796875, 178.03155517578125, 178.0418701171875, 178.02365112304688, 177.98690795898438, 177.99447631835938, 177.9990234375, 178.00588989257812, 178.05677795410156, 178.01278686523438, 178.01461791992188, 178.00637817382812, 177.9945831298828, 178.05227661132812, 177.99691772460938, 177.9981231689453, 178.06834411621094, 178.02020263671875, 178.03543090820312, 178.0032501220703, 177.9992218017578, 178.03648376464844, 177.9952392578125, 178.03683471679688, 178.03213500976562, 177.99998474121094, 178.01329040527344, 178.0772705078125, 177.9962921142578, 178.0293426513672, 178.01242065429688, 178.09275817871094, 178.03585815429688, 178.01211547851562, 178.06626892089844, 178.05667114257812, 178.0072021484375, 178.03042602539062, 177.9891815185547, 178.02230834960938, 178.01905822753906, 178.00775146484375, 177.99920654296875, 178.03318786621094, 178.02847290039062, 178.09019470214844, 177.99913024902344, 177.9998321533203, 178.02662658691406, 178.00941467285156, 177.99880981445312, 178.03347778320312, 178.0084686279297, 178.0679473876953, 178.04180908203125, 178.00933837890625, 178.0234375, 178.0225067138672, 178.025146484375, 178.00753784179688, 177.9927978515625, 177.98435974121094, 178.00778198242188, 178.00936889648438, 178.00767517089844, 178.0728759765625, 178.03515625, 178.00634765625, 178.03579711914062, 177.9854278564453, 178.01821899414062, 178.04302978515625, 178.01931762695312, 178.02389526367188, 178.04144287109375, 178.0072021484375, 178.06491088867188, 178.0201416015625, 177.99676513671875, 177.98880004882812, 178.04327392578125, 177.9985809326172, 178.037841796875, 178.04931640625, 178.0021514892578, 178.00405883789062, 178.01332092285156, 178.01174926757812, 178.00091552734375, 178.03968811035156, 178.00848388671875, 177.99798583984375, 178.04519653320312, 178.06454467773438, 177.99981689453125, 178.11167907714844, 178.0010528564453, 178.15151977539062, 178.09249877929688, 177.9966583251953, 178.0291748046875, 178.0365753173828, 178.08172607421875, 177.98834228515625, 178.05908203125, 178.0395050048828, 178.51394653320312, 178.10400390625, 177.99041748046875, 178.00277709960938, 178.02484130859375]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:16<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss: 178.03826904296875\n",
      "Losses [178.07382202148438, 178.0862274169922, 178.0128631591797, 177.9967041015625, 178.03253173828125, 178.0360565185547, 178.03952026367188, 178.052734375, 177.993408203125, 178.00262451171875, 178.06198120117188, 178.0276641845703, 178.0399169921875, 177.99169921875, 178.25473022460938, 178.0681915283203, 178.00653076171875, 178.0222930908203, 178.02114868164062, 178.0251922607422, 178.0535430908203, 177.98745727539062, 178.03045654296875, 178.01718139648438, 178.01939392089844, 178.04354858398438, 178.04087829589844, 178.05422973632812, 178.01467895507812, 177.99876403808594, 177.99493408203125, 177.99609375, 178.09716796875, 178.00634765625, 178.01510620117188, 178.07916259765625, 178.00645446777344, 178.05419921875, 178.01783752441406, 178.11215209960938, 178.07577514648438, 178.0399932861328, 178.03445434570312, 178.06549072265625, 178.09295654296875, 178.04498291015625, 178.05465698242188, 177.9898681640625, 178.0020294189453, 178.03958129882812, 178.00135803222656, 178.01296997070312, 178.00076293945312, 178.0731201171875, 178.01519775390625, 177.989013671875, 178.02764892578125, 178.0189666748047, 178.06008911132812, 177.99844360351562, 178.0054473876953, 178.13720703125, 178.02783203125, 178.03407287597656, 177.99978637695312, 178.02740478515625, 177.99276733398438, 178.04977416992188, 177.98892211914062, 178.04153442382812, 177.99588012695312, 178.0531005859375, 178.04969787597656, 178.02879333496094, 178.05747985839844, 178.03125, 178.0039520263672, 178.05679321289062, 177.99705505371094, 178.04249572753906, 178.09988403320312, 178.00912475585938, 178.03341674804688, 178.04345703125, 178.0509033203125, 178.01419067382812, 178.01214599609375, 178.0012664794922, 177.9876251220703, 178.01693725585938, 177.989990234375, 178.0269012451172, 178.09054565429688, 177.9911651611328, 178.21597290039062, 177.99745178222656, 178.01736450195312, 178.11380004882812, 178.03160095214844, 178.00201416015625, 178.01185607910156, 178.12454223632812, 178.08978271484375, 178.33682250976562, 178.05581665039062, 177.99880981445312, 178.00257873535156, 178.0274658203125, 177.9888916015625, 178.01983642578125, 178.1749267578125, 178.01747131347656, 177.99462890625, 178.01739501953125, 178.1262969970703, 178.04855346679688, 178.01507568359375, 178.0351104736328, 178.015625, 178.0008544921875]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:17<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 178.03121948242188\n",
      "Losses [178.01185607910156, 178.04522705078125, 178.0711669921875, 177.9980010986328, 178.08372497558594, 178.0096435546875, 178.04241943359375, 178.0178985595703, 178.01260375976562, 177.99740600585938, 178.06973266601562, 178.12057495117188, 178.0122528076172, 177.9859619140625, 178.014404296875, 178.0413055419922, 178.00221252441406, 178.02523803710938, 178.04388427734375, 177.99166870117188, 178.0423126220703, 177.9960479736328, 177.9939422607422, 178.00816345214844, 177.99468994140625, 178.05691528320312, 178.0072021484375, 178.06423950195312, 178.0068359375, 177.99371337890625, 178.0346221923828, 178.02120971679688, 178.07481384277344, 178.0118408203125, 178.0138397216797, 178.056640625, 178.0389862060547, 178.01336669921875, 178.03872680664062, 178.0150146484375, 178.08921813964844, 178.043212890625, 178.02508544921875, 178.02047729492188, 178.03164672851562, 178.00733947753906, 178.01055908203125, 177.99057006835938, 178.02093505859375, 178.0045928955078, 178.02215576171875, 178.00509643554688, 178.02110290527344, 178.007080078125, 178.06109619140625, 177.9881591796875, 178.02842712402344, 178.0292510986328, 177.9970703125, 178.00296020507812, 178.01934814453125, 178.11087036132812, 178.07003784179688, 178.05538940429688, 178.04190063476562, 178.00540161132812, 178.02529907226562, 177.99612426757812, 178.03689575195312, 178.03915405273438, 178.01953125, 178.42312622070312, 178.012939453125, 178.05538940429688, 178.03738403320312, 178.00930786132812, 177.9995880126953, 178.0010223388672, 177.9835968017578, 178.0000457763672, 178.20411682128906, 178.00474548339844, 178.04470825195312, 178.01092529296875, 178.00155639648438, 178.04278564453125, 177.99635314941406, 178.0077362060547, 177.9917755126953, 178.02093505859375, 177.995849609375, 178.0816650390625, 178.0313262939453, 178.0082550048828, 178.14398193359375, 178.0110321044922, 178.0341796875, 177.9894256591797, 178.06613159179688, 177.99180603027344, 178.03213500976562, 178.0236358642578, 177.9921417236328, 178.01571655273438, 178.09048461914062, 178.02774047851562, 178.00802612304688, 178.10153198242188, 177.98924255371094, 178.03648376464844, 178.08975219726562, 178.00018310546875, 177.98947143554688, 178.0120086669922, 178.05372619628906, 178.01516723632812, 178.04205322265625, 178.0302276611328, 178.02734375, 178.0595703125]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:15<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, loss: 178.1092071533203\n",
      "Losses [177.99659729003906, 178.02719116210938, 178.01803588867188, 177.99168395996094, 178.03231811523438, 178.01815795898438, 178.027099609375, 178.04031372070312, 178.01560974121094, 178.01925659179688, 178.0301513671875, 178.03646850585938, 178.00753784179688, 177.99142456054688, 177.99256896972656, 178.0326690673828, 178.00360107421875, 178.04623413085938, 178.01654052734375, 177.9921112060547, 178.03250122070312, 177.99644470214844, 177.99220275878906, 177.99891662597656, 178.00567626953125, 178.03475952148438, 178.0072479248047, 178.01287841796875, 178.00698852539062, 178.0072784423828, 178.02267456054688, 177.9951171875, 178.0298614501953, 178.00181579589844, 178.01760864257812, 178.0435791015625, 178.0468292236328, 177.99229431152344, 178.0223388671875, 177.99932861328125, 178.0791015625, 178.02761840820312, 178.00466918945312, 178.02659606933594, 178.0367431640625, 178.01199340820312, 178.01841735839844, 177.98748779296875, 178.00242614746094, 177.99514770507812, 178.01255798339844, 178.01974487304688, 178.00234985351562, 178.04498291015625, 178.04254150390625, 177.98733520507812, 178.02015686035156, 178.0052490234375, 177.99172973632812, 178.01927185058594, 177.99989318847656, 179.382568359375, 178.0823516845703, 178.19259643554688, 178.05264282226562, 178.02537536621094, 178.0388946533203, 177.99667358398438, 178.0012664794922, 178.01214599609375, 178.00172424316406, 181.54638671875, 178.0597381591797, 178.05581665039062, 178.0316619873047, 177.99595642089844, 178.0118865966797, 178.00894165039062, 177.9931640625, 178.0885009765625, 178.91114807128906, 178.00698852539062, 178.1187744140625, 181.32916259765625, 178.00418090820312, 178.01690673828125, 177.9927215576172, 178.0137939453125, 177.98980712890625, 178.0121307373047, 178.00894165039062, 178.027587890625, 178.6075439453125, 177.9918670654297, 178.01278686523438, 177.99595642089844, 178.01206970214844, 178.65069580078125, 178.03407287597656, 178.00393676757812, 178.00927734375, 178.06008911132812, 178.0140380859375, 178.24465942382812, 178.0469512939453, 177.99801635742188, 178.00216674804688, 178.0291748046875, 177.9891357421875, 178.04168701171875, 178.04531860351562, 178.04798889160156, 177.9862060546875, 178.00656127929688, 178.07403564453125, 178.0154266357422, 178.08139038085938, 178.05581665039062, 178.032958984375, 178.0671844482422]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:16<00:00,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss: 178.02137756347656\n",
      "Losses [178.00250244140625, 178.05242919921875, 178.00369262695312, 177.98593139648438, 178.05307006835938, 178.003662109375, 177.99461364746094, 178.04580688476562, 177.991943359375, 178.01319885253906, 178.053955078125, 178.08901977539062, 177.9940185546875, 177.99400329589844, 177.99195861816406, 178.00567626953125, 177.99795532226562, 178.02911376953125, 178.0139617919922, 178.01580810546875, 178.02813720703125, 177.99203491210938, 177.99359130859375, 178.00045776367188, 177.9938507080078, 177.9940185546875, 177.9979248046875, 178.0817413330078, 177.99224853515625, 177.99130249023438, 178.00216674804688, 178.00607299804688, 178.096435546875, 178.2023468017578, 178.0006866455078, 178.22238159179688, 178.0074005126953, 178.00064086914062, 178.0033721923828, 178.0355224609375, 178.0518035888672, 178.01351928710938, 177.99185180664062, 178.04815673828125, 178.0233154296875, 177.9921112060547, 178.00303649902344, 177.9933624267578, 177.99871826171875, 178.01629638671875, 178.01785278320312, 177.99502563476562, 177.98690795898438, 178.090576171875, 178.01507568359375, 177.99105834960938, 178.0042266845703, 178.01339721679688, 177.99119567871094, 178.0006103515625, 178.00146484375, 178.11917114257812, 178.09408569335938, 178.02650451660156, 178.0019073486328, 178.02020263671875, 177.994140625, 178.01943969726562, 178.00344848632812, 178.06903076171875, 178.01544189453125, 178.18556213378906, 178.00784301757812, 178.0153350830078, 178.03384399414062, 178.00698852539062, 178.00994873046875, 177.999755859375, 177.98342895507812, 178.0185546875, 178.01852416992188, 178.0163116455078, 178.02670288085938, 178.04171752929688, 178.03082275390625, 178.02716064453125, 177.9884490966797, 178.00210571289062, 177.9849090576172, 177.9951934814453, 178.00100708007812, 178.04844665527344, 178.0484619140625, 177.98960876464844, 177.99806213378906, 177.9969940185547, 178.0207977294922, 178.00387573242188, 178.03607177734375, 177.99362182617188, 178.0768585205078, 178.01239013671875, 178.01876831054688, 178.0127410888672, 178.04428100585938, 178.0242156982422, 178.0152587890625, 178.1195068359375, 177.98463439941406, 178.01376342773438, 178.0138397216797, 178.0402374267578, 177.98880004882812, 177.9935302734375, 178.01797485351562, 178.01071166992188, 178.04071044921875, 178.02313232421875, 177.99005126953125, 178.0167694091797]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer_params = {\n",
    "    \"lr\": 0.001,\n",
    "}\n",
    "optimizer = SGD(criterion.parameters(), **optimizer_params, )\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for episode_samples, episode_labels in tqdm(ds_train):\n",
    "        optimizer.zero_grad()\n",
    "        # print(episode_samples.shape)\n",
    "        # print(episode_labels.shape)\n",
    "        y_true: torch.Tensor = episode_labels[n_way * n_shots:].unsqueeze(-1).cuda()\n",
    "        support, query = split_support_query(episode_samples, n_way, n_shots)\n",
    "        support: torch.Tensor = support.cuda()\n",
    "        query: torch.Tensor = query.cuda()\n",
    "\n",
    "        # print(support.shape)\n",
    "        # print(query.shape)\n",
    "        # print(y_true.shape)\n",
    "\n",
    "        loss = criterion(support, query, y_true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch+1}, loss: {torch.tensor(losses).mean()}\")\n",
    "    print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
